{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ScanNet\n",
    "\n",
    "This notebook lets you instantiate the **[ScanNet](http://www.scan-net.org/)** dataset from scratch and visualize **3D+2D room samples**.\n",
    "\n",
    "Note that you will need **at least 1.2T** available for the SanNet raw dataset and **at least 64G** for the processed files at **5cm voxel resolution** and **320x240 image resolution**. \n",
    "\n",
    "The ScanNet dataset is composed of **rooms** of video acquisitions of indoor scenes. Thes video streams were used to produce a point cloud and images.\n",
    "\n",
    "Each room is small enough to be loaded at once into a **64G RAM** memory. The `ScannetDatasetMM` class from `torch_points3d.datasets.segmentation.multimodal.scannet` deals with loading the room and part of the images of the associated video stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select you GPU\n",
    "I_GPU = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMData debug() function changed, please uncomment the 3rd assert line when doing inference without M2F features!\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to use autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from omegaconf import OmegaConf\n",
    "start = time()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# torch.cuda.set_device(I_GPU)\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "from torch_geometric.data import Data\n",
    "from torch_points3d.core.multimodal.data import MMData, MMBatch\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
    "from torch_points3d.core.multimodal.image import SameSettingImageData, ImageData\n",
    "from torch_points3d.datasets.segmentation.multimodal.scannet import ScannetDatasetMM\n",
    "from torch_points3d.datasets.segmentation.scannet import CLASS_COLORS, CLASS_NAMES, CLASS_LABELS\n",
    "from torch_points3d.metrics.segmentation_tracker import SegmentationTracker\n",
    "from torch_points3d.datasets.segmentation import IGNORE_LABEL\n",
    "\n",
    "from pykeops.torch import LazyTensor\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_COLORS[0] = (174.0, 199.0, 232.0)\n",
    "CLASS_COLORS[-1] = (0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `visualize_mm_data` does not throw any error but the visualization does not appear, you may need to change your plotly renderer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "#pio.renderers.default = 'jupyterlab'        # for local notebook\n",
    "pio.renderers.default = 'iframe_connected'  # for remote notebook. Other working (but seemingly slower) options are: 'sphinx_gallery' and 'iframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Load predicted 2D semantic segmentation labels from directory  m2f_masks\n",
      "initialize train dataset\n",
      "initialize val dataset\n",
      "Time = 7.5 sec.\n"
     ]
    }
   ],
   "source": [
    "# Set your dataset root directory, where the data was/will be downloaded\n",
    "DATA_ROOT = '/scratch-shared/fsun/dvata'\n",
    "\n",
    "dataset_config = 'segmentation/multimodal/Feng/scannet-neucon-smallres-m2f'   \n",
    "models_config = 'segmentation/multimodal/Feng/mvfusion'    # model family\n",
    "model_name = 'MVFusion_3D_small_6views'                       # specific model\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    f'data={dataset_config}',\n",
    "    f'models={models_config}',\n",
    "    f'model_name={model_name}',\n",
    "    f'data.dataroot={DATA_ROOT}',\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n",
    "cfg.data.load_m2f_masks = True   # load Mask2Former predicted masks\n",
    "cfg.data.m2f_preds_dirname = 'm2f_masks'\n",
    "cfg.data.n_views = cfg.models[model_name].backbone.transformer.n_views\n",
    "print(cfg.data.n_views)\n",
    "\n",
    "# Dataset instantiation\n",
    "start = time()\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)|\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: MVFusion_3D_small_6views\n",
      "task:  segmentation.multimodal\n",
      "tested_model_name:  MVFusion_3D_small_6views\n",
      "class_name:  MVFusionAPIModel\n",
      "model_module:  torch_points3d.models.segmentation.multimodal.Feng.mvfusion_3d\n",
      "name, cls of chosen model_cls:  MVFusionAPIModel <class 'torch_points3d.models.segmentation.multimodal.Feng.mvfusion_3d.MVFusionAPIModel'>\n",
      "x feature dim:  {'FEAT': 3}\n",
      "nc_in:  67\n",
      "nc_in:  64\n",
      "nc_in:  32\n",
      "nc_in:  64\n",
      "nc_in:  128\n",
      "nc_in:  256\n",
      "nc_in:  128\n",
      "nc_in:  128\n",
      "nc_in:  96\n",
      "nc_in:  96\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from torch_points3d.models.model_factory import instantiate_model\n",
    "\n",
    "# ViT_masks 3rd run\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/ViT_masks_3rd_run' # 3rd run\n",
    "checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/MVFusion_3D_6_views_m2f_masks'\n",
    "\n",
    "# Create the model\n",
    "print(f\"Creating model: {cfg.model_name}\")\n",
    "model = instantiate_model(cfg, dataset)\n",
    "# print(model)\n",
    "\n",
    "# Load the checkpoint and recover the 'best_miou' model weights\n",
    "checkpoint = torch.load(f'{checkpoint_dir}/{model_name}.pt', map_location='cpu')\n",
    "model.load_state_dict_with_same_shape(checkpoint['models']['latest'], strict=False)\n",
    "\n",
    "# Prepare the model for training\n",
    "model = model.cuda()\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seen_points(mm_data):\n",
    "    ### Select seen points\n",
    "    csr_idx = mm_data.modalities['image'][0].view_csr_indexing\n",
    "    dense_idx_list = torch.arange(mm_data.modalities['image'][0].num_points).repeat_interleave(csr_idx[1:] - csr_idx[:-1])\n",
    "    # take subset of only seen points without re-indexing the same point\n",
    "    mm_data = mm_data[dense_idx_list.unique()]\n",
    "    return mm_data\n",
    "\n",
    "def get_mode_pred(data):\n",
    "    pixel_validity = data.data.mvfusion_input[:, :, 0].bool()\n",
    "    mv_preds = data.data.mvfusion_input[:, :, -1].long()\n",
    "            \n",
    "    valid_m2f_feats = []\n",
    "    for i in range(len(mv_preds)):\n",
    "        valid_m2f_feats.append(mv_preds[i][pixel_validity[i]])\n",
    "\n",
    "    mode_preds = []\n",
    "    for m2feats_of_seen_point in valid_m2f_feats:\n",
    "        mode_preds.append(torch.mode(m2feats_of_seen_point.squeeze(), dim=0)[0])\n",
    "    mode_preds = torch.stack(mode_preds, dim=0)\n",
    "        \n",
    "    return mode_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.metrics.scannet_segmentation_tracker import ScannetSegmentationTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_tracker = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "mvfusion_tracker = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'89.67 & 98.16 & 72.49 & 85.95 & 90.83 & 81.63 & 82.44 & 70.54 & 66.33 & 79.68 & 44.70 & 69.93 & 73.52 & 81.89 & 73.40 & 78.48 & 94.22 & 71.27 & 88.69 & 67.45'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {0: '89.67', 1: '98.16', 2: '72.49', 3: '85.95', 4: '90.83', 5: '81.63', 6: '82.44', 7: '70.54', 8: '66.33', 9: '79.68', 10: '44.70', 11: '69.93', 12: '73.52', 13: '81.89', 14: '73.40', 15: '78.48', 16: '94.22', 17: '71.27', 18: '88.69', 19: '67.45'}\n",
    "\" & \".join(list(a.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "3D validation IoU of seen points\n",
      "baseline  {'train_acc': 89.08670027260406, 'train_macc': 83.86834171426358, 'train_miou': 58.07665523950559}\n",
      "80.45 & 90.96 & 67.72 & 0.00 & 74.94 & 0.00 & 76.04 & 79.09 & 60.49 & 0.00 & 0.00 & 73.31 & 0.00 & 0.00 & 63.95 & 0.00 & 0.00 & 64.10 & 0.00 & 82.00\n",
      "mvfusion  {'train_acc': 93.6874647822975, 'train_macc': 88.87996157274955, 'train_miou': 68.72089955410117}\n",
      "81.79 & 99.23 & 68.40 & 0.00 & 93.64 & 0.00 & 90.17 & 88.63 & 67.45 & 0.00 & 0.00 & 76.53 & 0.00 & 0.00 & 63.96 & 0.00 & 0.00 & 70.63 & 0.00 & 92.93\n"
     ]
    }
   ],
   "source": [
    "baseline_tracker.reset(stage='train')\n",
    "mvfusion_tracker.reset(stage='train')\n",
    "\n",
    "\n",
    "for mm_data in dataset.val_dataset:\n",
    "    print(mm_data.id_scan)\n",
    "\n",
    "    # Create a MMBatch and run inference\n",
    "    batch = MMBatch.from_mm_data_list([mm_data])\n",
    "\n",
    "    model.set_input(batch, model.device)\n",
    "    model(batch)\n",
    "\n",
    "    # Recover the predicted labels for visualization\n",
    "    mm_data.data.pred = model.output.detach().cpu().argmax(1)\n",
    "        \n",
    "    mm_data = get_seen_points(mm_data)\n",
    "    baseline_preds = get_mode_pred(mm_data)\n",
    "    \n",
    "    mvfusion_tracker.track(pred_labels=mm_data.data.pred, gt_labels=mm_data.data.y, model=None)       \n",
    "    baseline_tracker.track(pred_labels=baseline_preds, gt_labels=mm_data.data.y, model=None)       \n",
    "\n",
    "    break\n",
    "    \n",
    "print(\"3D validation IoU of seen points\")\n",
    "print(\"baseline \", baseline_tracker.get_metrics())\n",
    "print(\" & \".join(list(baseline_tracker._miou_per_class.values())))\n",
    "\n",
    "print(\"mvfusion \", mvfusion_tracker.get_metrics())\n",
    "print(\" & \".join(list(mvfusion_tracker._miou_per_class.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mode_preds.shape)\n",
    "print(mm_data.data.pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2f_mm_data = mm_data.clone()\n",
    "m2f_mm_data.data.x = None\n",
    "m2f_mm_data.data.pred = mode_preds\n",
    "# m2f_mm_data.data.pred = m2f_mm_data.data.pred[m2f_mm_data.data.y != -1]\n",
    "m2f_mm_data = m2f_mm_data[m2f_mm_data.data.y != -1]\n",
    "\n",
    "visualize_mm_data(m2f_mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_mask_pred', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data.data.x = None\n",
    "mm_data.data.pred = mm_data.data.pred[mm_data.data.y != -1]\n",
    "mm_data = mm_data[mm_data.data.y != -1]\n",
    "\n",
    "\n",
    "print(mm_data.data.pred.unique())\n",
    "mm_data.data.y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_pred_mask', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
