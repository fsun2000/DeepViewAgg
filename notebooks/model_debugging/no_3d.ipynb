{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ScanNet\n",
    "\n",
    "This notebook lets you instantiate the **[ScanNet](http://www.scan-net.org/)** dataset from scratch and visualize **3D+2D room samples**.\n",
    "\n",
    "Note that you will need **at least 1.2T** available for the SanNet raw dataset and **at least 64G** for the processed files at **5cm voxel resolution** and **320x240 image resolution**. \n",
    "\n",
    "The ScanNet dataset is composed of **rooms** of video acquisitions of indoor scenes. Thes video streams were used to produce a point cloud and images.\n",
    "\n",
    "Each room is small enough to be loaded at once into a **64G RAM** memory. The `ScannetDatasetMM` class from `torch_points3d.datasets.segmentation.multimodal.scannet` deals with loading the room and part of the images of the associated video stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select you GPU\n",
    "I_GPU = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMData debug() function changed, please uncomment the 3rd assert line when doing inference without M2F features!\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to use autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from omegaconf import OmegaConf\n",
    "start = time()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# torch.cuda.set_device(I_GPU)\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "from torch_geometric.data import Data\n",
    "from torch_points3d.core.multimodal.data import MMData, MMBatch\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
    "from torch_points3d.core.multimodal.image import SameSettingImageData, ImageData\n",
    "from torch_points3d.datasets.segmentation.multimodal.scannet import ScannetDatasetMM\n",
    "from torch_points3d.datasets.segmentation.scannet import CLASS_COLORS, CLASS_NAMES, CLASS_LABELS\n",
    "from torch_points3d.metrics.segmentation_tracker import SegmentationTracker\n",
    "from torch_points3d.datasets.segmentation import IGNORE_LABEL\n",
    "\n",
    "from pykeops.torch import LazyTensor\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_COLORS[0] = (174.0, 199.0, 232.0)\n",
    "CLASS_COLORS[-1] = (0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `visualize_mm_data` does not throw any error but the visualization does not appear, you may need to change your plotly renderer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "#pio.renderers.default = 'jupyterlab'        # for local notebook\n",
    "pio.renderers.default = 'iframe_connected'  # for remote notebook. Other working (but seemingly slower) options are: 'sphinx_gallery' and 'iframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Load predicted 2D semantic segmentation labels from directory  ViT_masks\n",
      "initialize train dataset\n",
      "initialize val dataset\n",
      "Time = 7.3 sec.\n"
     ]
    }
   ],
   "source": [
    "# Set your dataset root directory, where the data was/will be downloaded\n",
    "DATA_ROOT = '/scratch-shared/fsun/dvata'\n",
    "\n",
    "dataset_config = 'segmentation/multimodal/Feng/scannet-neucon-smallres-m2f'   \n",
    "models_config = 'segmentation/multimodal/Feng/mvfusion'    # model family\n",
    "model_name = 'MVFusion_3D_small_6views'                       # specific model\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    f'data={dataset_config}',\n",
    "    f'models={models_config}',\n",
    "    f'model_name={model_name}',\n",
    "    f'data.dataroot={DATA_ROOT}',\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n",
    "cfg.data.load_m2f_masks = True   # load Mask2Former predicted masks\n",
    "cfg.data.m2f_preds_dirname = 'ViT_masks'\n",
    "cfg.data.n_views = cfg.models[model_name].backbone.transformer.n_views\n",
    "print(cfg.data.n_views)\n",
    "\n",
    "# Dataset instantiation\n",
    "start = time()\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)|\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: MVFusion_3D_small_6views\n",
      "task:  segmentation.multimodal\n",
      "tested_model_name:  MVFusion_3D_small_6views\n",
      "class_name:  MVFusionAPIModel\n",
      "model_module:  torch_points3d.models.segmentation.multimodal.Feng.mvfusion_3d\n",
      "name, cls of chosen model_cls:  MVFusionAPIModel <class 'torch_points3d.models.segmentation.multimodal.Feng.mvfusion_3d.MVFusionAPIModel'>\n",
      "x feature dim:  {'FEAT': 3}\n",
      "nc_in:  67\n",
      "nc_in:  64\n",
      "nc_in:  32\n",
      "nc_in:  64\n",
      "nc_in:  128\n",
      "nc_in:  256\n",
      "nc_in:  128\n",
      "nc_in:  128\n",
      "nc_in:  96\n",
      "nc_in:  96\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from torch_points3d.models.model_factory import instantiate_model\n",
    "\n",
    "# ViT_masks 3rd run\n",
    "checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/ViT_masks_3rd_run' # 3rd run\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/MVFusion_3D_6_views_m2f_masks'\n",
    "\n",
    "# Create the model\n",
    "print(f\"Creating model: {cfg.model_name}\")\n",
    "model = instantiate_model(cfg, dataset)\n",
    "# print(model)\n",
    "\n",
    "# Load the checkpoint and recover the 'best_miou' model weights\n",
    "checkpoint = torch.load(f'{checkpoint_dir}/{model_name}.pt', map_location='cpu')\n",
    "model.load_state_dict_with_same_shape(checkpoint['models']['latest'], strict=False)\n",
    "\n",
    "# Prepare the model for training\n",
    "model = model.cuda()\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data = dataset.val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 1, 1,  ..., 0, 2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_seen_points(mm_data):\n",
    "    ### Select seen points\n",
    "    csr_idx = mm_data.modalities['image'][0].view_csr_indexing\n",
    "    dense_idx_list = torch.arange(mm_data.modalities['image'][0].num_points).repeat_interleave(csr_idx[1:] - csr_idx[:-1])\n",
    "    # take subset of only seen points without re-indexing the same point\n",
    "    mm_data = mm_data[dense_idx_list.unique()]\n",
    "    return mm_data\n",
    "\n",
    "def get_mode_pred(data):\n",
    "    pixel_validity = data.data.mvfusion_input[:, :, 0].bool()\n",
    "    mv_preds = data.data.mvfusion_input[:, :, -1].long()\n",
    "            \n",
    "    valid_m2f_feats = []\n",
    "    for i in range(len(mv_preds)):\n",
    "        valid_m2f_feats.append(mv_preds[i][pixel_validity[i]])\n",
    "\n",
    "    mode_preds = []\n",
    "    for m2feats_of_seen_point in valid_m2f_feats:\n",
    "        mode_preds.append(torch.mode(m2feats_of_seen_point.squeeze(), dim=0)[0])\n",
    "    mode_preds = torch.stack(mode_preds, dim=0)\n",
    "        \n",
    "    return mode_preds\n",
    "\n",
    "def get_random_view_pred(data):\n",
    "    pixel_validity = data.data.mvfusion_input[:, :, 0].bool()\n",
    "    mv_preds = data.data.mvfusion_input[:, :, -1].long()\n",
    "            \n",
    "    valid_m2f_feats = []\n",
    "    for i in range(len(mv_preds)):\n",
    "        valid_m2f_feats.append(mv_preds[i][pixel_validity[i]])\n",
    "\n",
    "    selected_view_preds = []\n",
    "    for m2feats_of_seen_point in valid_m2f_feats:\n",
    "#         print(m2feats_of_seen_point)\n",
    "        selected_idx = torch.randint(low=0, high=m2feats_of_seen_point.shape[0], size=(1,))\n",
    "        selected_pred = m2feats_of_seen_point[selected_idx].squeeze(0)\n",
    "        \n",
    "        \n",
    "#         print(m2feats_of_seen_point.shape[0])\n",
    "#         print(selected_idx)\n",
    "#         print(selected_pred)\n",
    "        selected_view_preds.append(selected_pred)\n",
    "    selected_view_preds = torch.stack(selected_view_preds, dim=0)\n",
    "        \n",
    "    return selected_view_preds\n",
    "\n",
    "get_random_view_pred(mm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.metrics.scannet_segmentation_tracker import ScannetSegmentationTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_tracker = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "mvfusion_tracker = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'89.67 & 98.16 & 72.49 & 85.95 & 90.83 & 81.63 & 82.44 & 70.54 & 66.33 & 79.68 & 44.70 & 69.93 & 73.52 & 81.89 & 73.40 & 78.48 & 94.22 & 71.27 & 88.69 & 67.45'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {0: '89.67', 1: '98.16', 2: '72.49', 3: '85.95', 4: '90.83', 5: '81.63', 6: '82.44', 7: '70.54', 8: '66.33', 9: '79.68', 10: '44.70', 11: '69.93', 12: '73.52', 13: '81.89', 14: '73.40', 15: '78.48', 16: '94.22', 17: '71.27', 18: '88.69', 19: '67.45'}\n",
    "\" & \".join(list(a.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([1, 4, 1,  ..., 2, 0, 0])\n",
      "tensor([1, 4, 1,  ..., 2, 0, 0])\n",
      "3D validation IoU of seen points\n",
      "baseline  {'train_acc': 88.69502831585535, 'train_macc': 85.95748835062953, 'train_miou': 61.41866356445035}\n",
      "81.45 & 90.51 & 74.15 & 0.00 & 71.23 & 0.00 & 69.20 & 79.96 & 63.60 & 0.00 & 0.00 & 68.48 & 0.00 & 0.00 & 75.98 & 0.00 & 0.00 & 55.03 & 0.00 & 68.84\n",
      "mvfusion  {'train_acc': 95.88466059135106, 'train_macc': 93.89504929024365, 'train_miou': 79.23063734957574}\n",
      "87.83 & 99.13 & 83.22 & 0.00 & 93.52 & 0.00 & 91.54 & 96.12 & 71.81 & 0.00 & 0.00 & 81.49 & 0.00 & 0.00 & 83.65 & 0.00 & 0.00 & 68.84 & 0.00 & 93.62\n"
     ]
    }
   ],
   "source": [
    "baseline_tracker.reset(stage='train')\n",
    "mvfusion_tracker.reset(stage='train')\n",
    "\n",
    "\n",
    "for mm_data in dataset.val_dataset:\n",
    "    print(mm_data.id_scan)\n",
    "\n",
    "    # Create a MMBatch and run inference\n",
    "    batch = MMBatch.from_mm_data_list([mm_data])\n",
    "\n",
    "    model.set_input(batch, model.device)\n",
    "    model(batch)\n",
    "\n",
    "    # Recover the predicted labels for visualization\n",
    "    mm_data.data.pred = model.output.detach().cpu().argmax(1)\n",
    "        \n",
    "    mm_data = get_seen_points(mm_data)\n",
    "    baseline_preds = get_mode_pred(mm_data)\n",
    "    \n",
    "    mvfusion_tracker.track(pred_labels=mm_data.data.pred, gt_labels=mm_data.data.y, model=None)       \n",
    "    baseline_tracker.track(pred_labels=baseline_preds, gt_labels=mm_data.data.y, model=None)       \n",
    "\n",
    "    break\n",
    "    \n",
    "print(\"3D validation IoU of seen points\")\n",
    "print(\"baseline \", baseline_tracker.get_metrics())\n",
    "print(\" & \".join(list(baseline_tracker._miou_per_class.values())))\n",
    "\n",
    "print(\"mvfusion \", mvfusion_tracker.get_metrics())\n",
    "print(\" & \".join(list(mvfusion_tracker._miou_per_class.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mode_preds.shape)\n",
    "print(mm_data.data.pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2f_mm_data = mm_data.clone()\n",
    "m2f_mm_data.data.x = None\n",
    "m2f_mm_data.data.pred = mode_preds\n",
    "# m2f_mm_data.data.pred = m2f_mm_data.data.pred[m2f_mm_data.data.y != -1]\n",
    "m2f_mm_data = m2f_mm_data[m2f_mm_data.data.y != -1]\n",
    "\n",
    "visualize_mm_data(m2f_mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_mask_pred', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data.data.x = None\n",
    "mm_data.data.pred = mm_data.data.pred[mm_data.data.y != -1]\n",
    "mm_data = mm_data[mm_data.data.y != -1]\n",
    "\n",
    "\n",
    "print(mm_data.data.pred.unique())\n",
    "mm_data.data.y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_pred_mask', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Selection comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.metrics.scannet_segmentation_tracker import ScannetSegmentationTracker\n",
    "\n",
    "def get_seen_points(mm_data):\n",
    "    ### Select seen points\n",
    "    csr_idx = mm_data.modalities['image'][0].view_csr_indexing\n",
    "    dense_idx_list = torch.arange(mm_data.modalities['image'][0].num_points).repeat_interleave(csr_idx[1:] - csr_idx[:-1])\n",
    "    # take subset of only seen points without re-indexing the same point\n",
    "    mm_data = mm_data[dense_idx_list.unique()]\n",
    "    return mm_data\n",
    "\n",
    "def get_mode_pred(data):\n",
    "    pixel_validity = data.data.mvfusion_input[:, :, 0].bool()\n",
    "    mv_preds = data.data.mvfusion_input[:, :, -1].long()\n",
    "            \n",
    "    valid_m2f_feats = []\n",
    "    for i in range(len(mv_preds)):\n",
    "        valid_m2f_feats.append(mv_preds[i][pixel_validity[i]])\n",
    "\n",
    "    mode_preds = []\n",
    "    for m2feats_of_seen_point in valid_m2f_feats:\n",
    "        mode_preds.append(torch.mode(m2feats_of_seen_point.squeeze(), dim=0)[0])\n",
    "    mode_preds = torch.stack(mode_preds, dim=0)\n",
    "        \n",
    "    return mode_preds\n",
    "\n",
    "def get_random_view_pred(data):\n",
    "    pixel_validity = data.data.mvfusion_input[:, :, 0].bool()\n",
    "    mv_preds = data.data.mvfusion_input[:, :, -1].long()\n",
    "            \n",
    "    valid_m2f_feats = []\n",
    "    for i in range(len(mv_preds)):\n",
    "        valid_m2f_feats.append(mv_preds[i][pixel_validity[i]])\n",
    "\n",
    "    selected_view_preds = []\n",
    "    for m2feats_of_seen_point in valid_m2f_feats:\n",
    "        selected_idx = torch.randint(low=0, high=m2feats_of_seen_point.shape[0], size=(1,))\n",
    "        selected_pred = m2feats_of_seen_point[selected_idx].squeeze(0)\n",
    "        selected_view_preds.append(selected_pred)\n",
    "    selected_view_preds = torch.stack(selected_view_preds, dim=0)\n",
    "        \n",
    "    return selected_view_preds\n",
    "\n",
    "get_random_view_pred(mm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_selection_tracker = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "average_fusion_tracker = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "mvfusion_tracker = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "3D validation IoU of seen points\n",
      "random_selection_tracker  {'train_acc': 87.91855478847967, 'train_macc': 85.03762311878698, 'train_miou': 55.869509912625446}\n",
      "81.45 & 90.51 & 74.15 & 0.00 & 71.23 & 0.00 & 69.20 & 79.96 & 63.60 & 0.00 & 0.00 & 68.48 & 0.00 & 0.00 & 75.98 & 0.00 & 0.00 & 55.03 & 0.00 & 68.84\n",
      "average_fusion_tracker  {'train_acc': 91.44490739332745, 'train_macc': 88.73124250433506, 'train_miou': 62.13647460620422}\n",
      "81.45 & 90.51 & 74.15 & 0.00 & 71.23 & 0.00 & 69.20 & 79.96 & 63.60 & 0.00 & 0.00 & 68.48 & 0.00 & 0.00 & 75.98 & 0.00 & 0.00 & 55.03 & 0.00 & 68.84\n",
      "mvfusion_tracker  {'train_acc': 95.82585687783218, 'train_macc': 94.01124190410147, 'train_miou': 73.3844437401989}\n",
      "88.67 & 99.25 & 82.61 & 0.00 & 93.29 & 0.00 & 90.32 & 94.76 & 71.95 & 0.00 & 0.00 & 82.14 & 0.00 & 0.00 & 82.26 & 0.00 & 0.00 & 74.63 & 0.00 & 94.13\n"
     ]
    }
   ],
   "source": [
    "random_selection_tracker.reset(stage='train')\n",
    "average_fusion_tracker.reset(stage='train')\n",
    "mvfusion_tracker.reset(stage='train')\n",
    "\n",
    "\n",
    "for mm_data in dataset.val_dataset:\n",
    "    print(mm_data.id_scan)\n",
    "\n",
    "    # Create a MMBatch and run inference\n",
    "    batch = MMBatch.from_mm_data_list([mm_data])\n",
    "\n",
    "    model.set_input(batch, model.device)\n",
    "    model(batch)\n",
    "\n",
    "    # Recover the predicted labels for visualization\n",
    "    mm_data.data.pred = model.output.detach().cpu().argmax(1)\n",
    "        \n",
    "    mm_data = get_seen_points(mm_data)\n",
    "    random_selection_pred = get_random_view_pred(mm_data)\n",
    "    average_fusion_pred = get_mode_pred(mm_data)\n",
    "    \n",
    "    random_selection_tracker.track(pred_labels=random_selection_pred, gt_labels=mm_data.data.y, model=None)  \n",
    "    average_fusion_tracker.track(pred_labels=average_fusion_pred, gt_labels=mm_data.data.y, model=None)       \n",
    "    mvfusion_tracker.track(pred_labels=mm_data.data.pred, gt_labels=mm_data.data.y, model=None)       \n",
    "\n",
    "\n",
    "    break\n",
    "    \n",
    "print(\"3D validation IoU of seen points\")\n",
    "print(\"random_selection_tracker \", random_selection_tracker.get_metrics())\n",
    "print(\" & \".join(list(baseline_tracker._miou_per_class.values())))\n",
    "\n",
    "print(\"average_fusion_tracker \", average_fusion_tracker.get_metrics())\n",
    "print(\" & \".join(list(baseline_tracker._miou_per_class.values())))\n",
    "\n",
    "print(\"mvfusion_tracker \", mvfusion_tracker.get_metrics())\n",
    "print(\" & \".join(list(mvfusion_tracker._miou_per_class.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigKeyError",
     "evalue": "Missing key Average_Fusion\n    full_key: models.Average_Fusion\n    object_type=dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigKeyError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/scratch-local/fsun.2082029/ipykernel_1882269/1769721391.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_m2f_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m   \u001b[0;31m# load Mask2Former predicted masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm2f_preds_dirname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ViT_masks'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_views\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_views\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_views\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch3d/lib/python3.7/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_and_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch3d/lib/python3.7/site-packages/omegaconf/base.py\u001b[0m in \u001b[0;36m_format_and_raise\u001b[0;34m(self, key, value, cause, type_override)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mtype_override\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_override\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch3d/lib/python3.7/site-packages/omegaconf/_utils.py\u001b[0m in \u001b[0;36mformat_and_raise\u001b[0;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_type_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mref_type_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m     \u001b[0m_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch3d/lib/python3.7/site-packages/omegaconf/_utils.py\u001b[0m in \u001b[0;36m_raise\u001b[0;34m(ex, cause)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# set end OC_CAUSE=1 for full backtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch3d/lib/python3.7/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_DEFAULT_MARKER_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             self._format_and_raise(\n",
      "\u001b[0;32m~/.conda/envs/pytorch3d/lib/python3.7/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m_get_impl\u001b[0;34m(self, key, default_value)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDictKeyType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthrow_on_missing_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mConfigAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigKeyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdefault_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_DEFAULT_MARKER_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch3d/lib/python3.7/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m_get_node\u001b[0;34m(self, key, validate_access, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mthrow_on_missing_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mConfigKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Missing key {key}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mthrow_on_missing_value\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingMandatoryValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing mandatory value: $KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConfigKeyError\u001b[0m: Missing key Average_Fusion\n    full_key: models.Average_Fusion\n    object_type=dict"
     ]
    }
   ],
   "source": [
    "# Set your dataset root directory, where the data was/will be downloaded\n",
    "DATA_ROOT = '/scratch-shared/fsun/dvata'\n",
    "\n",
    "dataset_config = 'segmentation/multimodal/Feng/scannet-neucon-smallres-m2f'   \n",
    "models_config = 'segmentation/multimodal/Feng/view_selection_experiment'    # model family\n",
    "model_name = 'Average_Fusion'                       # specific model\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    f'data={dataset_config}',\n",
    "    f'models={models_config}',\n",
    "    f'model_name={model_name}',\n",
    "    f'data.dataroot={DATA_ROOT}',\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n",
    "cfg.data.load_m2f_masks = True   # load Mask2Former predicted masks\n",
    "cfg.data.m2f_preds_dirname = 'ViT_masks'\n",
    "cfg.data.n_views = cfg.models[model_name].backbone.transformer.n_views\n",
    "print(cfg.data.n_views)\n",
    "\n",
    "# Dataset instantiation\n",
    "start = time()\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)|\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: Average_Fusion\n",
      "task:  segmentation.multimodal\n",
      "tested_model_name:  Average_Fusion\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The model_name Average_Fusion isn t within ['MVFusion_small_6views', 'DeepSetAttention', 'MVFusion']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/scratch-local/fsun.2082029/ipykernel_1882269/662217531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating model: {cfg.model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstantiate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/home3/fsun/DeepViewAgg/notebooks/../torch_points3d/models/model_factory.py\u001b[0m in \u001b[0;36minstantiate_model\u001b[0;34m(config, dataset)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmodels_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The model_name {} isn t within {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtested_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mresolve_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The model_name Average_Fusion isn t within ['MVFusion_small_6views', 'DeepSetAttention', 'MVFusion']"
     ]
    }
   ],
   "source": [
    "from torch_points3d.models.model_factory import instantiate_model\n",
    "\n",
    "# ViT_masks 3rd run\n",
    "checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/ViT_masks_3rd_run' # 3rd run\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/MVFusion_3D_6_views_m2f_masks'\n",
    "\n",
    "# Create the model\n",
    "print(f\"Creating model: {cfg.model_name}\")\n",
    "model = instantiate_model(cfg, dataset)\n",
    "print(model)\n",
    "\n",
    "# # Load the checkpoint and recover the 'best_miou' model weights\n",
    "# checkpoint = torch.load(f'{checkpoint_dir}/{model_name}.pt', map_location='cpu')\n",
    "# model.load_state_dict_with_same_shape(checkpoint['models']['latest'], strict=False)\n",
    "\n",
    "# # Prepare the model for training\n",
    "model = model.cuda()\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMData(\n",
       "    data = Data(coords=[96882, 3], grid_size=[1], id_scan=[1], mapping_index=[96882], mvfusion_input=[70473, 6, 10], origin_id=[96882], pos=[96882, 3], rgb=[96882, 3], x=[96882, 3], y=[96882])\n",
       "    image = ImageData(num_settings=1, num_views=100, num_points=96882, device=cpu)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_data = dataset.val_dataset[1]\n",
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1,  ..., 0, 2, 0], device='cuda:0') torch.Size([70473])\n",
      "tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0]], device='cuda:0') torch.Size([70473, 20])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0') torch.Size([96882, 20])\n"
     ]
    }
   ],
   "source": [
    "# Create a MMBatch and run inference\n",
    "batch = MMBatch.from_mm_data_list([mm_data])\n",
    "\n",
    "model.set_input(batch, model.device)\n",
    "model(batch)\n",
    "\n",
    "# Recover the predicted labels for visualization\n",
    "mm_data.data.pred = model.output.detach().cpu().argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15, 16, 18, 19]),\n",
       " tensor([ 4502,  4263, 17765, 16889,  1407,  2851, 29363,    11,  2472,   682,\n",
       "           462,  2723,  1017,  9667,   127,  2332,     8,   341]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_data.data.pred.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_acc': 0.0015302921327681454,\n",
       " 'train_macc': 0.0032055391716886783,\n",
       " 'train_miou': 0.002188944426204394}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_points3d.metrics.scannet_segmentation_tracker import ScannetSegmentationTracker\n",
    "tracker = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "\n",
    "seen_data = get_seen_points(mm_data)\n",
    "tracker.track(pred_labels=mm_data.data.pred, gt_labels=seen_data.data.y, model=None)       \n",
    "\n",
    "tracker.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0,      0,      0,  ..., 513984, 513985, 513986])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewing_conditions = mm_data.modalities['image'][0].mappings.values[2]\n",
    "\n",
    "input_preds = mm_data.modalities['image'][0].get_mapped_m2f_features()\n",
    "input_preds_one_hot = torch.nn.functional.one_hot(input_preds.long().squeeze(), 20)\n",
    "attention_input = torch.concat((viewing_conditions, input_preds_one_hot), dim=1)\n",
    "\n",
    "\n",
    "csr_idx = mm_data.modalities['image'][0].view_csr_indexing\n",
    "csr_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mode_pred(data):\n",
    "    pixel_validity = data.data.mvfusion_input[:, :, 0].bool()\n",
    "    mv_preds = data.data.mvfusion_input[:, :, -1].long()\n",
    "            \n",
    "    valid_m2f_feats = []\n",
    "    for i in range(len(mv_preds)):\n",
    "        valid_m2f_feats.append(mv_preds[i][pixel_validity[i]])\n",
    "\n",
    "    mode_preds = []\n",
    "    for m2feats_of_seen_point in valid_m2f_feats:\n",
    "        mode_preds.append(torch.mode(m2feats_of_seen_point.squeeze(), dim=0)[0])\n",
    "    mode_preds = torch.stack(mode_preds, dim=0)\n",
    "        \n",
    "    return mode_preds\n",
    "\n",
    "def get_random_view_pred(data):\n",
    "    pixel_validity = data.data.mvfusion_input[:, :, 0].bool()\n",
    "    mv_preds = data.data.mvfusion_input[:, :, -1].long()\n",
    "            \n",
    "    valid_m2f_feats = []\n",
    "    for i in range(len(mv_preds)):\n",
    "        valid_m2f_feats.append(mv_preds[i][pixel_validity[i]])\n",
    "\n",
    "    selected_view_preds = []\n",
    "    for m2feats_of_seen_point in valid_m2f_feats:\n",
    "        selected_idx = torch.randint(low=0, high=m2feats_of_seen_point.shape[0], size=(1,))\n",
    "        selected_pred = m2feats_of_seen_point[selected_idx].squeeze(0)\n",
    "        selected_view_preds.append(selected_pred)\n",
    "    selected_view_preds = torch.stack(selected_view_preds, dim=0)\n",
    "        \n",
    "    return selected_view_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Load predicted 2D semantic segmentation labels from directory  ViT_masks\n",
      "initialize train dataset\n",
      "initialize val dataset\n",
      "Time = 7.8 sec.\n"
     ]
    }
   ],
   "source": [
    "# Set your dataset root directory, where the data was/will be downloaded\n",
    "DATA_ROOT = '/scratch-shared/fsun/dvata'\n",
    "\n",
    "dataset_config = 'segmentation/multimodal/Feng/scannet-neucon-smallres-m2f.yaml'   \n",
    "models_config = 'segmentation/multimodal/Feng/view_selection_experiment.yaml'    # model family\n",
    "model_name = 'Deepset_3D'                       # specific model\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    f'data={dataset_config}',\n",
    "    f'models={models_config}',\n",
    "    f'model_name={model_name}',\n",
    "    f'data.dataroot={DATA_ROOT}',\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n",
    "cfg.data.load_m2f_masks = True   # load Mask2Former predicted masks\n",
    "cfg.data.m2f_preds_dirname = 'ViT_masks'\n",
    "cfg.data.n_views = cfg.models[model_name].backbone.transformer.n_views\n",
    "print(cfg.data.n_views)\n",
    "\n",
    "# cfg.models.MVFusion_small_6views.backbone.transformer.max_n_points = 10000\n",
    "# print(cfg.models.MVFusion_small_6views.backbone.transformer.max_n_points)\n",
    "\n",
    "\n",
    "# cfg.data.store_random_pred = True\n",
    "# cfg.data.store_mode_pred = True\n",
    "# print(cfg.data.store_mode_pred)\n",
    "\n",
    "\n",
    "# Dataset instantiation\n",
    "start = time()\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)|\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: Deepset_3D\n",
      "task:  segmentation.multimodal\n",
      "tested_model_name:  Deepset_3D\n",
      "class_name:  MVAttentionAPIModel\n",
      "model_module:  torch_points3d.models.segmentation.multimodal.Feng.mvattention_attention_weighted_m2f_pred\n",
      "name, cls of chosen model_cls:  MVAttentionAPIModel <class 'torch_points3d.models.segmentation.multimodal.Feng.mvattention_attention_weighted_m2f_pred.MVAttentionAPIModel'>\n",
      "x feature dim:  {'FEAT': 3}\n",
      "nc_in:  35\n",
      "nc_in:  64\n",
      "nc_in:  32\n",
      "nc_in:  64\n",
      "nc_in:  128\n",
      "nc_in:  256\n",
      "nc_in:  128\n",
      "nc_in:  128\n",
      "nc_in:  96\n",
      "nc_in:  96\n",
      "MVAttentionAPIModel(\n",
      "  (backbone): MVAttentionSparseConv3dUnet(\n",
      "    (inner_modules): ModuleList(\n",
      "      (0): Identity()\n",
      "    )\n",
      "    (down_modules): ModuleList(\n",
      "      (0): MultimodalBlockDown(\n",
      "        (block_1): Identity()\n",
      "        (block_2): Identity()\n",
      "        (image): MVAttentionUnimodalBranch(\n",
      "          drop_3d=None\n",
      "          drop_mod=None\n",
      "          keep_last_view=False\n",
      "          checkpointing=c\n",
      "          (attn_fusion): DeepSetFeat_ViewFusion(\n",
      "            pool=['max']\n",
      "            fusion=concatenation\n",
      "            use_num=True\n",
      "            (mlp_elt_1): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Linear(in_features=28, out_features=32, bias=False)\n",
      "                (1): FastBatchNorm1d(\n",
      "                  (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                )\n",
      "                (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "              )\n",
      "              (1): Sequential(\n",
      "                (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "                (1): FastBatchNorm1d(\n",
      "                  (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                )\n",
      "                (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (mlp_set): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Linear(in_features=33, out_features=32, bias=False)\n",
      "                (1): FastBatchNorm1d(\n",
      "                  (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                )\n",
      "                (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "              )\n",
      "              (1): Sequential(\n",
      "                (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "                (1): FastBatchNorm1d(\n",
      "                  (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                )\n",
      "                (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (mlp_elt_2): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Linear(in_features=64, out_features=32, bias=False)\n",
      "                (1): FastBatchNorm1d(\n",
      "                  (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                )\n",
      "                (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "              )\n",
      "              (1): Sequential(\n",
      "                (0): Linear(in_features=32, out_features=32, bias=False)\n",
      "                (1): FastBatchNorm1d(\n",
      "                  (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                )\n",
      "                (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (E_score): Linear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (fusion): BimodalFusion(mode=concatenation)\n",
      "        )\n",
      "      )\n",
      "      (1): MultimodalBlockDown(\n",
      "        (block_1): ResNetDown(\n",
      "          (conv_in): Seq(\n",
      "            (0): Conv3d(in_channels=35, out_channels=64, kernel_size=3, stride=1, dilation=1)\n",
      "            (1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (block_2): Identity()\n",
      "        (image): IdentityBranch()\n",
      "      )\n",
      "      (2): MultimodalBlockDown(\n",
      "        (block_1): ResNetDown(\n",
      "          (conv_in): Seq(\n",
      "            (0): Conv3d(in_channels=64, out_channels=64, kernel_size=2, stride=2, dilation=1)\n",
      "            (1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (blocks): Seq(\n",
      "            (0): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=64, out_channels=32, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "              (downsample): Seq(\n",
      "                (0): Conv3d(in_channels=64, out_channels=32, kernel_size=1, stride=1, dilation=1)\n",
      "                (1): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (block_2): Identity()\n",
      "        (image): IdentityBranch()\n",
      "      )\n",
      "      (3): MultimodalBlockDown(\n",
      "        (block_1): ResNetDown(\n",
      "          (conv_in): Seq(\n",
      "            (0): Conv3d(in_channels=32, out_channels=32, kernel_size=2, stride=2, dilation=1)\n",
      "            (1): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (blocks): Seq(\n",
      "            (0): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=64, out_channels=64, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "              (downsample): Seq(\n",
      "                (0): Conv3d(in_channels=32, out_channels=64, kernel_size=1, stride=1, dilation=1)\n",
      "                (1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=64, out_channels=64, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=64, out_channels=64, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=64, out_channels=64, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=64, out_channels=64, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (block_2): Identity()\n",
      "        (image): IdentityBranch()\n",
      "      )\n",
      "      (4): MultimodalBlockDown(\n",
      "        (block_1): ResNetDown(\n",
      "          (conv_in): Seq(\n",
      "            (0): Conv3d(in_channels=64, out_channels=64, kernel_size=2, stride=2, dilation=1)\n",
      "            (1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (blocks): Seq(\n",
      "            (0): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=128, out_channels=128, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "              (downsample): Seq(\n",
      "                (0): Conv3d(in_channels=64, out_channels=128, kernel_size=1, stride=1, dilation=1)\n",
      "                (1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=128, out_channels=128, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=128, out_channels=128, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=128, out_channels=128, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=128, out_channels=128, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (3): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=128, out_channels=128, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=128, out_channels=128, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (block_2): Identity()\n",
      "        (image): IdentityBranch()\n",
      "      )\n",
      "      (5): MultimodalBlockDown(\n",
      "        (block_1): ResNetDown(\n",
      "          (conv_in): Seq(\n",
      "            (0): Conv3d(in_channels=128, out_channels=128, kernel_size=2, stride=2, dilation=1)\n",
      "            (1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (blocks): Seq(\n",
      "            (0): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=256, out_channels=256, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "              (downsample): Seq(\n",
      "                (0): Conv3d(in_channels=128, out_channels=256, kernel_size=1, stride=1, dilation=1)\n",
      "                (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=256, out_channels=256, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=256, out_channels=256, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=256, out_channels=256, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=256, out_channels=256, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (3): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=256, out_channels=256, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=256, out_channels=256, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (4): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=256, out_channels=256, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=256, out_channels=256, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (5): ResBlock(\n",
      "              (block): Seq(\n",
      "                (0): Conv3d(in_channels=256, out_channels=256, kernel_size=3, stride=1, dilation=1)\n",
      "                (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv3d(in_channels=256, out_channels=256, kernel_size=3, stride=1, dilation=1)\n",
      "                (4): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (5): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (block_2): Identity()\n",
      "        (image): IdentityBranch()\n",
      "      )\n",
      "    )\n",
      "    (up_modules): ModuleList(\n",
      "      (0): ResNetUp(\n",
      "        (conv_in): Seq(\n",
      "          (0): Conv3d(in_channels=256, out_channels=256, kernel_size=2, stride=2, dilation=1)\n",
      "          (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (blocks): Seq(\n",
      "          (0): ResBlock(\n",
      "            (block): Seq(\n",
      "              (0): Conv3d(in_channels=384, out_channels=128, kernel_size=3, stride=1, dilation=1)\n",
      "              (1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "              (3): Conv3d(in_channels=128, out_channels=128, kernel_size=3, stride=1, dilation=1)\n",
      "              (4): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (5): ReLU(inplace=True)\n",
      "            )\n",
      "            (downsample): Seq(\n",
      "              (0): Conv3d(in_channels=384, out_channels=128, kernel_size=1, stride=1, dilation=1)\n",
      "              (1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ResNetUp(\n",
      "        (conv_in): Seq(\n",
      "          (0): Conv3d(in_channels=128, out_channels=128, kernel_size=2, stride=2, dilation=1)\n",
      "          (1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (blocks): Seq(\n",
      "          (0): ResBlock(\n",
      "            (block): Seq(\n",
      "              (0): Conv3d(in_channels=192, out_channels=128, kernel_size=3, stride=1, dilation=1)\n",
      "              (1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "              (3): Conv3d(in_channels=128, out_channels=128, kernel_size=3, stride=1, dilation=1)\n",
      "              (4): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (5): ReLU(inplace=True)\n",
      "            )\n",
      "            (downsample): Seq(\n",
      "              (0): Conv3d(in_channels=192, out_channels=128, kernel_size=1, stride=1, dilation=1)\n",
      "              (1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ResNetUp(\n",
      "        (conv_in): Seq(\n",
      "          (0): Conv3d(in_channels=128, out_channels=128, kernel_size=2, stride=2, dilation=1)\n",
      "          (1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (blocks): Seq(\n",
      "          (0): ResBlock(\n",
      "            (block): Seq(\n",
      "              (0): Conv3d(in_channels=160, out_channels=96, kernel_size=3, stride=1, dilation=1)\n",
      "              (1): BatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "              (3): Conv3d(in_channels=96, out_channels=96, kernel_size=3, stride=1, dilation=1)\n",
      "              (4): BatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (5): ReLU(inplace=True)\n",
      "            )\n",
      "            (downsample): Seq(\n",
      "              (0): Conv3d(in_channels=160, out_channels=96, kernel_size=1, stride=1, dilation=1)\n",
      "              (1): BatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ResNetUp(\n",
      "        (conv_in): Seq(\n",
      "          (0): Conv3d(in_channels=96, out_channels=96, kernel_size=2, stride=2, dilation=1)\n",
      "          (1): BatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (blocks): Seq(\n",
      "          (0): ResBlock(\n",
      "            (block): Seq(\n",
      "              (0): Conv3d(in_channels=160, out_channels=96, kernel_size=3, stride=1, dilation=1)\n",
      "              (1): BatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "              (3): Conv3d(in_channels=96, out_channels=96, kernel_size=3, stride=1, dilation=1)\n",
      "              (4): BatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (5): ReLU(inplace=True)\n",
      "            )\n",
      "            (downsample): Seq(\n",
      "              (0): Conv3d(in_channels=160, out_channels=96, kernel_size=1, stride=1, dilation=1)\n",
      "              (1): BatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): ResNetUp(\n",
      "        (conv_in): Seq(\n",
      "          (0): Conv3d(in_channels=96, out_channels=96, kernel_size=3, stride=1, dilation=1)\n",
      "          (1): BatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (blocks): Seq(\n",
      "          (0): ResBlock(\n",
      "            (block): Seq(\n",
      "              (0): Conv3d(in_channels=96, out_channels=96, kernel_size=3, stride=1, dilation=1)\n",
      "              (1): BatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace=True)\n",
      "              (3): Conv3d(in_channels=96, out_channels=96, kernel_size=3, stride=1, dilation=1)\n",
      "              (4): BatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (5): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=96, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from torch_points3d.models.model_factory import instantiate_model\n",
    "\n",
    "# ViT_masks 3rd run\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/ViT_masks_3rd_run' # 3rd run\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/MVFusion_3D_6_views_m2f_masks'\n",
    "\n",
    "# # MVFusion 100 epochs (no superconvergence) old version (31-10-2022)\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/m2f_masks_MVFusion_medium_9views'\n",
    "\n",
    "# MVFusion_orig 100 epochs (18-1-2023 using old code, no superconvergence)\n",
    "# checkpoint_dir ='/home/fsun/DeepViewAgg/outputs/MVFusion_orig'\n",
    "\n",
    "\n",
    "# Create the model\n",
    "print(f\"Creating model: {cfg.model_name}\")\n",
    "model = instantiate_model(cfg, dataset)\n",
    "print(model)\n",
    "\n",
    "# # Load the checkpoint and recover the 'best_miou' model weights\n",
    "# checkpoint = torch.load(f'{checkpoint_dir}/{model_name}.pt', map_location='cpu')\n",
    "# model.load_state_dict_with_same_shape(checkpoint['models']['best_miou'], strict=False)\n",
    "\n",
    "# # Prepare the model for training\n",
    "model = model.cuda()\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data = dataset.val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMData(\n",
       "    data = Data(coords=[97387, 3], grid_size=[1], id_scan=[1], mapping_index=[97387], mvfusion_input=[73331, 6, 10], origin_id=[97387], pos=[97387, 3], rgb=[97387, 3], x=[97387, 3], y=[97387])\n",
       "    image = ImageData(num_settings=1, num_views=100, num_points=97387, device=cpu)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMData(\n",
       "    data = Data(coords=[97387, 3], grid_size=[1], id_scan=[1], mapping_index=[97387], mvfusion_input=[73331, 6, 10], origin_id=[97387], pos=[97387, 3], pred=[97387], rgb=[97387, 3], x=[97387, 3], y=[97387])\n",
       "    image = ImageData(num_settings=1, num_views=100, num_points=97387, device=cpu)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a MMBatch and run inference\n",
    "batch = MMBatch.from_mm_data_list([mm_data])\n",
    "\n",
    "model.set_input(batch, model.device)\n",
    "model(batch)\n",
    "\n",
    "# Recover the predicted labels for visualization\n",
    "mm_data.data.pred = model.output.detach().cpu().argmax(1)\n",
    "\n",
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seen_points(mm_data):\n",
    "    ### Select seen points\n",
    "    csr_idx = mm_data.modalities['image'][0].view_csr_indexing\n",
    "    dense_idx_list = torch.arange(mm_data.modalities['image'][0].num_points).repeat_interleave(csr_idx[1:] - csr_idx[:-1])\n",
    "    # take subset of only seen points without re-indexing the same point\n",
    "    mm_data = mm_data[dense_idx_list.unique()]\n",
    "    return mm_data\n",
    "\n",
    "seen_mm_data = get_seen_points(mm_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_20.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_mm_data(seen_mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_pred_mask', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
