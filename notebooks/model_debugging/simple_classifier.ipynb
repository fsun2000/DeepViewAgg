{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77c4242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMData debug() function changed, please uncomment the 3rd assert line when doing inference without M2F features!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "from time import time\n",
    "from omegaconf import OmegaConf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# torch.cuda.set_device(I_GPU)\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "from torch_geometric.data import Data\n",
    "from torch_points3d.core.multimodal.data import MMData, MMBatch\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
    "from torch_points3d.core.multimodal.image import SameSettingImageData, ImageData\n",
    "from torch_points3d.datasets.segmentation.multimodal.scannet import ScannetDatasetMM\n",
    "from torch_points3d.datasets.segmentation.scannet import CLASS_COLORS, CLASS_NAMES, CLASS_LABELS\n",
    "from torch_points3d.metrics.scannet_segmentation_tracker import ScannetSegmentationTracker\n",
    "from torch_points3d.datasets.segmentation import IGNORE_LABEL\n",
    "from torch_points3d.models.model_factory import instantiate_model\n",
    "from torch_points3d.metrics.colored_tqdm import Coloredtqdm as Ctq\n",
    "\n",
    "\n",
    "from pykeops.torch import LazyTensor\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "CLASS_COLORS[0] = (174.0, 199.0, 232.0)\n",
    "CLASS_COLORS[-1] = (0, 0, 0)\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "#pio.renderers.default = 'jupyterlab'        # for local notebook\n",
    "pio.renderers.default = 'iframe_connected'  # for remote notebook. Other working (but seemingly slower) options are: 'sphinx_gallery' and 'iframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3eb5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Load predicted 2D semantic segmentation labels from directory  ViT_masks\n",
      "initialize train dataset\n",
      "initialize val dataset\n",
      "Time = 8.3 sec.\n",
      "Creating model: MVFusion_3D_small_6views\n",
      "task:  segmentation.multimodal\n",
      "tested_model_name:  MVFusion_3D_small_6views\n",
      "class_name:  MVFusionAPIModel\n",
      "model_module:  torch_points3d.models.segmentation.multimodal.Feng.mvfusion_3d\n",
      "name, cls of chosen model_cls:  MVFusionAPIModel <class 'torch_points3d.models.segmentation.multimodal.Feng.mvfusion_3d.MVFusionAPIModel'>\n",
      "x feature dim:  {'FEAT': 3}\n",
      "nc_in:  67\n",
      "nc_in:  64\n",
      "nc_in:  32\n",
      "nc_in:  64\n",
      "nc_in:  128\n",
      "Return attention maps!\n",
      "Enabling checkpointing for:  c\n",
      "nc_in:  256\n",
      "nc_in:  128\n",
      "nc_in:  128\n",
      "nc_in:  96\n",
      "nc_in:  96\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = '/scratch-shared/fsun/dvata'\n",
    "\n",
    "dataset_config = 'segmentation/multimodal/Feng/scannet-neucon-smallres-m2f'   \n",
    "models_config = 'segmentation/multimodal/Feng/mvfusion'    # model family\n",
    "model_name = 'MVFusion_3D_small_6views'                       # specific model\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    f'data={dataset_config}',\n",
    "    f'models={models_config}',\n",
    "    f'model_name={model_name}',\n",
    "    f'data.dataroot={DATA_ROOT}',\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n",
    "cfg.data.load_m2f_masks = True   # load Mask2Former predicted masks\n",
    "cfg.data.m2f_preds_dirname = 'ViT_masks'\n",
    "cfg.data.n_views = cfg.models[model_name].backbone.transformer.n_views\n",
    "print(cfg.data.n_views)\n",
    "\n",
    "# Dataset instantiation\n",
    "start = time()\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)|\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")\n",
    "\n",
    "\n",
    "\n",
    "# ViT_masks 3rd run\n",
    "checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/ViT_masks_3rd_run' # 3rd run\n",
    "\n",
    "\n",
    "# Create the model\n",
    "print(f\"Creating model: {cfg.model_name}\")\n",
    "model = instantiate_model(cfg, dataset)\n",
    "# print(model)\n",
    "\n",
    "# Load the checkpoint and recover the 'best_miou' model weights\n",
    "checkpoint = torch.load(f'{checkpoint_dir}/{model_name}.pt', map_location='cpu')\n",
    "model.load_state_dict_with_same_shape(checkpoint['models']['best_miou'], strict=False)\n",
    "\n",
    "# Prepare the model for training\n",
    "model = model.cuda()\n",
    "print('Model loaded')\n",
    "\n",
    "# Create validation loader\n",
    "dataset.create_dataloaders(\n",
    "    model,\n",
    "    1,\n",
    "    False,\n",
    "    17,\n",
    "    False,\n",
    "    train_only=False,\n",
    "    val_only=True,\n",
    "    test_batch_size=1\n",
    ")\n",
    "\n",
    "tracker = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61efb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 1.0968017578125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 312,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7b84bbee9e478eac199493f529e9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_list = []\n",
    "input_labels_list = []\n",
    "gt_labels_list = []\n",
    "\n",
    "with Ctq(dataset._val_loader) as loader:\n",
    "    for mm_data in loader:\n",
    "\n",
    "        data = mm_data.modalities['image'][0]._mappings.values[-1]\n",
    "        input_labels = mm_data.modalities['image'][0].get_mapped_m2f_features()\n",
    "        gt_labels = mm_data.modalities['image'][0].get_mapped_gt_labels()\n",
    "\n",
    "        data_list.append(data)\n",
    "        input_labels_list.append(input_labels)\n",
    "        gt_labels_list.append(gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c5ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.vstack(data_list)\n",
    "input_labels = torch.vstack(input_labels_list)\n",
    "gt_labels = torch.vstack(gt_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b285822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['feat1', 'feat2', 'feat3', 'feat4', 'feat5', 'feat6', 'feat7', 'feat8', 'input_label', 'gt_label']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d_backup",
   "language": "python",
   "name": "pytorch3d_backup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
