{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ac4756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMData debug() function changed, please uncomment the 3rd assert line when doing inference without M2F features!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select you GPU\n",
    "I_GPU = 0\n",
    "\n",
    "# Uncomment to use autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "start = time()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# torch.cuda.set_device(I_GPU)\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "from torch_geometric.data import Data\n",
    "from torch_points3d.core.multimodal.data import MMData, MMBatch\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
    "from torch_points3d.core.multimodal.image import SameSettingImageData, ImageData\n",
    "from torch_points3d.datasets.segmentation.multimodal.scannet import ScannetDatasetMM\n",
    "from torch_points3d.datasets.segmentation.scannet import CLASS_COLORS, CLASS_NAMES, CLASS_LABELS\n",
    "from torch_points3d.metrics.segmentation_tracker import SegmentationTracker\n",
    "\n",
    "from pykeops.torch import LazyTensor\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "#pio.renderers.default = 'jupyterlab'        # for local notebook\n",
    "pio.renderers.default = 'iframe_connected'  # for remote notebook. Other working (but seemingly slower) options are: 'sphinx_gallery' and 'iframe'\n",
    "\n",
    "CLASS_COLORS[0] = (174.0, 199.0, 232.0)\n",
    "CLASS_COLORS[-1] = (0, 0, 0)\n",
    "\n",
    "# from torch_points3d.datasets.segmentation.scannet import CLASS_COLORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd99300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchnet as tnt\n",
    "import torch\n",
    "from typing import Dict, Any\n",
    "import wandb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "from torch_points3d.metrics.confusion_matrix import ConfusionMatrix\n",
    "from torch_points3d.models import model_interface\n",
    "from torch_points3d.metrics.base_tracker import BaseTracker, meter_value\n",
    "from torch_points3d.metrics.meters import APMeter\n",
    "from torch_points3d.datasets.segmentation import IGNORE_LABEL\n",
    "\n",
    "from torch_geometric.nn.unpool import knn_interpolate\n",
    "from torch_points3d.core.data_transform import SaveOriginalPosId\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def meter_value(meter, dim=0):\n",
    "    return float(meter.value()[dim]) if meter.n > 0 else 0.0\n",
    "\n",
    "\n",
    "class BaseTracker:\n",
    "    def __init__(self, stage: str, wandb_log: bool, use_tensorboard: bool):\n",
    "        self._wandb = wandb_log\n",
    "        self._use_tensorboard = use_tensorboard\n",
    "        self._tensorboard_dir = os.path.join(os.getcwd(), \"tensorboard\")\n",
    "        self._n_iter = 0\n",
    "        self._finalised = False\n",
    "        self._conv_type = None\n",
    "\n",
    "        if self._use_tensorboard:\n",
    "            log.info(\n",
    "                \"Access tensorboard with the following command <tensorboard --logdir={}>\".format(self._tensorboard_dir)\n",
    "            )\n",
    "            self._writer = SummaryWriter(log_dir=self._tensorboard_dir)\n",
    "\n",
    "    def reset(self, stage=\"train\"):\n",
    "        self._stage = stage\n",
    "        self._loss_meters = {}\n",
    "        self._finalised = False\n",
    "\n",
    "    def get_metrics(self, verbose=False) -> Dict[str, Any]:\n",
    "        metrics = {}\n",
    "        for key, loss_meter in self._loss_meters.items():\n",
    "            value = meter_value(loss_meter, dim=0)\n",
    "            if value:\n",
    "                metrics[key] = meter_value(loss_meter, dim=0)\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def metric_func(self):\n",
    "        self._metric_func = {\"loss\": min}\n",
    "        return self._metric_func\n",
    "\n",
    "    def track(self, model: model_interface.TrackerInterface, **kwargs):\n",
    "        if self._finalised:\n",
    "            raise RuntimeError(\"Cannot track new values with a finalised tracker, you need to reset it first\")\n",
    "            \n",
    "        if model is not None:\n",
    "            losses = self._convert(model.get_current_losses())\n",
    "            self._append_losses(losses)\n",
    "\n",
    "    def finalise(self, *args, **kwargs):\n",
    "        \"\"\" Lifcycle method that is called at the end of an epoch. Use this to compute\n",
    "        end of epoch metrics.\n",
    "        \"\"\"\n",
    "        self._finalised = True\n",
    "\n",
    "    def _append_losses(self, losses):\n",
    "        for key, loss in losses.items():\n",
    "            if loss is None:\n",
    "                continue\n",
    "            loss_key = \"%s_%s\" % (self._stage, key)\n",
    "            if loss_key not in self._loss_meters:\n",
    "                self._loss_meters[loss_key] = tnt.meter.AverageValueMeter()\n",
    "            self._loss_meters[loss_key].add(loss)\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert(x):\n",
    "        if torch.is_tensor(x):\n",
    "            return x.detach().cpu().numpy()\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def publish_to_tensorboard(self, metrics, step):\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            metric_name = \"{}/{}\".format(metric_name.replace(self._stage + \"_\", \"\"), self._stage)\n",
    "            self._writer.add_scalar(metric_name, metric_value, step)\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_stage_from_metric_keys(stage, metrics):\n",
    "        new_metrics = {}\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            new_metrics[metric_name.replace(stage + \"_\", \"\")] = metric_value\n",
    "        return new_metrics\n",
    "\n",
    "    def publish(self, step):\n",
    "        \"\"\" Publishes the current metrics to wandb and tensorboard\n",
    "        Arguments:\n",
    "            step: current epoch\n",
    "        \"\"\"\n",
    "        metrics = self.get_metrics()\n",
    "\n",
    "        if self._wandb:\n",
    "            wandb.log(metrics, step=step)\n",
    "\n",
    "        if self._use_tensorboard:\n",
    "            self.publish_to_tensorboard(metrics, step)\n",
    "\n",
    "        # Some metrics may be intended for wandb or tensorboard\n",
    "        # tracking but not for final final model selection. Those are\n",
    "        # the metrics absent from self.metric_func and must be excluded\n",
    "        # from the output of self.publish\n",
    "        current_metrics = {\n",
    "            k: v\n",
    "            for k, v in self._remove_stage_from_metric_keys(self._stage, metrics).items()\n",
    "            if k in self.metric_func.keys()}\n",
    "\n",
    "        return {\n",
    "            \"stage\": self._stage,\n",
    "            \"epoch\": step,\n",
    "            \"current_metrics\": current_metrics,\n",
    "        }\n",
    "\n",
    "    def print_summary(self):\n",
    "        metrics = self.get_metrics(verbose=True)\n",
    "        log.info(\"\".join([\"=\" for i in range(50)]))\n",
    "        for key, value in metrics.items():\n",
    "            log.info(\"    {} = {}\".format(key, value))\n",
    "        log.info(\"\".join([\"=\" for i in range(50)]))\n",
    "\n",
    "    @staticmethod\n",
    "    def _dict_to_str(dictionnary):\n",
    "        string = \"{\"\n",
    "        for key, value in dictionnary.items():\n",
    "            string += \"%s: %.2f,\" % (str(key), value)\n",
    "        string += \"}\"\n",
    "        return string\n",
    "\n",
    "\n",
    "class SegmentationTracker(BaseTracker):\n",
    "    def __init__(\n",
    "        self, dataset, stage=\"train\", wandb_log=False, use_tensorboard: bool = False, ignore_label: int = IGNORE_LABEL\n",
    "    ):\n",
    "        \"\"\" This is a generic tracker for multimodal tasks.\n",
    "        It uses a confusion matrix in the back-end to track results.\n",
    "        Use the tracker to track an epoch.\n",
    "        You can use the reset function before you start a new epoch\n",
    "\n",
    "        Arguments:\n",
    "            dataset  -- dataset to track (used for the number of classes)\n",
    "\n",
    "        Keyword Arguments:\n",
    "            stage {str} -- current stage. (train, validation, test, etc...) (default: {\"train\"})\n",
    "            wandb_log {str} --  Log using weight and biases\n",
    "        \"\"\"\n",
    "        super(SegmentationTracker, self).__init__(stage, wandb_log, use_tensorboard)\n",
    "        self._num_classes = dataset.num_classes\n",
    "        self._ignore_label = ignore_label\n",
    "        self._dataset = dataset\n",
    "        self.reset(stage)\n",
    "        self._metric_func = {\n",
    "            \"miou\": max,\n",
    "            \"macc\": max,\n",
    "            \"acc\": max,\n",
    "            \"loss\": min,\n",
    "            \"map\": max,\n",
    "        }  # Those map subsentences to their optimization functions\n",
    "\n",
    "    def reset(self, stage=\"train\"):\n",
    "        super().reset(stage=stage)\n",
    "        self._confusion_matrix = ConfusionMatrix(self._num_classes)\n",
    "        self._acc = 0\n",
    "        self._macc = 0\n",
    "        self._miou = 0\n",
    "        self._miou_per_class = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def detach_tensor(tensor):\n",
    "        if torch.torch.is_tensor(tensor):\n",
    "            tensor = tensor.detach()\n",
    "        return tensor\n",
    "\n",
    "    @property\n",
    "    def confusion_matrix(self):\n",
    "        return self._confusion_matrix.confusion_matrix\n",
    "\n",
    "    def track(self, model: model_interface.TrackerInterface, pred_labels=None, gt_labels=None, **kwargs):\n",
    "        \"\"\" Add current model predictions (usually the result of a batch) to the tracking\n",
    "        \"\"\"\n",
    "        if not self._dataset.has_labels(self._stage):\n",
    "            return\n",
    "\n",
    "        # Feng: to evaluate M2F predictions instead of model logits\n",
    "        if pred_labels is not None and gt_labels is not None:\n",
    "            outputs = pred_labels\n",
    "            targets = gt_labels\n",
    "        else:\n",
    "            super().track(model)\n",
    "            \n",
    "            outputs = model.get_output()\n",
    "            targets = model.get_labels()\n",
    "        self._compute_metrics(outputs, targets)\n",
    "\n",
    "    def _compute_metrics(self, outputs, labels):\n",
    "        mask = labels != self._ignore_label\n",
    "        outputs = outputs[mask]\n",
    "        labels = labels[mask]\n",
    "\n",
    "        outputs = self._convert(outputs)\n",
    "        labels = self._convert(labels)\n",
    "\n",
    "        if len(labels) == 0:\n",
    "            return\n",
    "\n",
    "        assert outputs.shape[0] == len(labels)\n",
    "        \n",
    "        # Check if output is predicted label or logits\n",
    "        if len(outputs.shape) > 1:\n",
    "            self._confusion_matrix.count_predicted_batch(labels, np.argmax(outputs, 1))\n",
    "        else:\n",
    "            \n",
    "            self._confusion_matrix.count_predicted_batch(labels, outputs)\n",
    "\n",
    "        self._acc = 100 * self._confusion_matrix.get_overall_accuracy()\n",
    "        self._macc = 100 * self._confusion_matrix.get_mean_class_accuracy()\n",
    "        self._miou = 100 * self._confusion_matrix.get_average_intersection_union()\n",
    "        self._miou_per_class = {\n",
    "            i: \"{:.2f}\".format(100 * v)\n",
    "            for i, v in enumerate(self._confusion_matrix.get_intersection_union_per_class()[0])\n",
    "        }\n",
    "\n",
    "    def get_metrics(self, verbose=False) -> Dict[str, Any]:\n",
    "        \"\"\" Returns a dictionnary of all metrics and losses being tracked\n",
    "        \"\"\"\n",
    "        metrics = super().get_metrics(verbose)\n",
    "\n",
    "        metrics[\"{}_acc\".format(self._stage)] = self._acc\n",
    "        metrics[\"{}_macc\".format(self._stage)] = self._macc\n",
    "        metrics[\"{}_miou\".format(self._stage)] = self._miou\n",
    "\n",
    "        if verbose:\n",
    "            metrics[\"{}_miou_per_class\".format(self._stage)] = self._miou_per_class\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def metric_func(self):\n",
    "        return self._metric_func\n",
    "\n",
    "\n",
    "class ScannetSegmentationTracker(SegmentationTracker):\n",
    "    def reset(self, stage=\"train\"):\n",
    "        super().reset(stage=stage)\n",
    "        self._full_confusion_matrix = ConfusionMatrix(self._num_classes)\n",
    "        self._raw_datas = {}\n",
    "        self._votes = {}\n",
    "        self._vote_counts = {}\n",
    "        self._full_preds = {}\n",
    "        self._full_acc = None\n",
    "\n",
    "    def track(self, model: model_interface.TrackerInterface, full_res=False, pred_labels=None, gt_labels=None, **kwargs):\n",
    "        \"\"\" Add current model predictions (usually the result of a batch) to the tracking\n",
    "        \"\"\"\n",
    "        if pred_labels is not None and gt_labels is not None:\n",
    "            super().track(model=None, pred_labels=pred_labels, gt_labels=gt_labels)\n",
    "        else:\n",
    "            super().track(model)\n",
    "\n",
    "            # Set conv type\n",
    "            self._conv_type = model.conv_type\n",
    "\n",
    "            # Train mode or low res, nothing special to do\n",
    "            if not full_res or self._stage == \"train\" or kwargs.get(\"data\") is None:\n",
    "                return\n",
    "\n",
    "            data = kwargs.get(\"data\", model.get_input())\n",
    "            data = data.data if model.is_multimodal else data\n",
    "            self._vote(data, model.get_output())\n",
    "\n",
    "    def get_metrics(self, verbose=False) -> Dict[str, Any]:\n",
    "        \"\"\" Returns a dictionnary of all metrics and losses being tracked\n",
    "        \"\"\"\n",
    "        metrics = super().get_metrics(verbose)\n",
    "        if self._full_acc:\n",
    "            metrics[\"{}_full_acc\".format(self._stage)] = self._full_acc\n",
    "            metrics[\"{}_full_macc\".format(self._stage)] = self._full_macc\n",
    "            metrics[\"{}_full_miou\".format(self._stage)] = self._full_miou\n",
    "        return metrics\n",
    "\n",
    "    def finalise(self, full_res=False, make_submission=False, **kwargs):\n",
    "        if not full_res and not make_submission:\n",
    "            return\n",
    "        \n",
    "        self._predict_full_res()\n",
    "\n",
    "        # Compute full res metrics\n",
    "        if self._dataset.has_labels(self._stage):\n",
    "            for scan_id in self._full_preds:\n",
    "                full_labels = self._raw_datas[scan_id].y\n",
    "                # Mask ignored labels\n",
    "                mask = full_labels != self._ignore_label\n",
    "                full_labels = full_labels[mask]\n",
    "                full_preds = self._full_preds[scan_id].cpu()[mask].numpy()\n",
    "                self._full_confusion_matrix.count_predicted_batch(full_labels, full_preds)\n",
    "\n",
    "            self._full_acc = 100 * self._full_confusion_matrix.get_overall_accuracy()\n",
    "            self._full_macc = 100 * self._full_confusion_matrix.get_mean_class_accuracy()\n",
    "            self._full_miou = 100 * self._full_confusion_matrix.get_average_intersection_union()\n",
    "            \n",
    "        # Save files to disk\n",
    "        if make_submission and self._stage == \"test\":\n",
    "            self._make_submission()\n",
    "\n",
    "    def _make_submission(self):\n",
    "        orginal_class_ids = np.asarray(self._dataset.train_dataset.valid_class_idx)\n",
    "        path_to_submission = self._dataset.path_to_submission\n",
    "        for scan_id in self._full_preds:\n",
    "            full_pred = self._full_preds[scan_id].cpu().numpy().astype(np.int8)\n",
    "            full_pred = orginal_class_ids[full_pred]  # remap labels to original labels between 0 and 40\n",
    "            scan_name = self._raw_datas[scan_id].scan_name\n",
    "            path_file = osp.join(path_to_submission, \"{}.txt\".format(scan_name))\n",
    "            np.savetxt(path_file, full_pred, delimiter=\"/n\", fmt=\"%d\")\n",
    "\n",
    "    def _vote(self, data, output):\n",
    "        \"\"\" Populates scores for the points in data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : Data\n",
    "            should contain `pos` and `SaveOriginalPosId.KEY` keys\n",
    "        output : torch.Tensor\n",
    "            probablities out of the model, shape: [N,nb_classes]\n",
    "        \"\"\"\n",
    "        id_scans = data.id_scan\n",
    "        if id_scans.dim() > 1:\n",
    "            id_scans = id_scans.squeeze()\n",
    "        if self._conv_type == \"DENSE\":\n",
    "            batch_size = len(id_scans)\n",
    "            output = output.view(batch_size, -1, output.shape[-1])\n",
    "\n",
    "        for idx_batch, id_scan in enumerate(id_scans):\n",
    "            # First time we see this scan\n",
    "            if id_scan not in self._raw_datas:\n",
    "                raw_data = self._dataset.get_raw_data(self._stage, id_scan, remap_labels=True)\n",
    "                self._raw_datas[id_scan] = raw_data\n",
    "                self._vote_counts[id_scan] = torch.zeros(raw_data.pos.shape[0], dtype=torch.int)\n",
    "                self._votes[id_scan] = torch.zeros((raw_data.pos.shape[0], self._num_classes), dtype=torch.float)\n",
    "            else:\n",
    "                raw_data = self._raw_datas[id_scan]\n",
    "\n",
    "            batch_mask = idx_batch\n",
    "            if self._conv_type != \"DENSE\":\n",
    "                batch_mask = data.batch == idx_batch\n",
    "            idx = data[SaveOriginalPosId.KEY][batch_mask]\n",
    "\n",
    "            self._votes[id_scan][idx] += output[batch_mask].cpu()\n",
    "            self._vote_counts[id_scan][idx] += 1\n",
    "\n",
    "    def _predict_full_res(self):\n",
    "        \"\"\" Predict full resolution results based on votes \"\"\"\n",
    "        for id_scan in self._votes:\n",
    "            has_prediction = self._vote_counts[id_scan] > 0\n",
    "            self._votes[id_scan][has_prediction] /= self._vote_counts[id_scan][has_prediction].unsqueeze(-1)\n",
    "\n",
    "            # Upsample and predict\n",
    "            full_pred = knn_interpolate(\n",
    "                self._votes[id_scan][has_prediction],\n",
    "                self._raw_datas[id_scan].pos[has_prediction],\n",
    "                self._raw_datas[id_scan].pos,\n",
    "                k=1,\n",
    "            )\n",
    "            self._full_preds[id_scan] = full_pred.argmax(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e27b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Load predicted 2D semantic segmentation labels from directory  ViT_masks\n",
      "initialize train dataset\n",
      "initialize val dataset\n",
      "Time = 7.5 sec.\n"
     ]
    }
   ],
   "source": [
    "# Set your dataset root directory, where the data was/will be downloaded\n",
    "DATA_ROOT = '/scratch-shared/fsun/dvata'\n",
    "\n",
    "dataset_config = 'segmentation/multimodal/Feng/scannet-neucon-smallres-m2f'   \n",
    "models_config = 'segmentation/multimodal/Feng/mvfusion'    # model family\n",
    "model_name = 'MVFusion_3D_small_6views'                       # specific model\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    f'data={dataset_config}',\n",
    "    f'models={models_config}',\n",
    "    f'model_name={model_name}',\n",
    "    f'data.dataroot={DATA_ROOT}',\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n",
    "cfg.data.load_m2f_masks = True   # load Mask2Former predicted masks\n",
    "\n",
    "cfg.data.m2f_preds_dirname = 'ViT_masks'\n",
    "cfg.data.n_views = 6 #cfg.models[model_name].backbone.transformer.n_views\n",
    "print(cfg.data.n_views)\n",
    "\n",
    "# Dataset instantiation\n",
    "start = time()\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90201f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_mvfusion = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "tracker_m2f = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da311f5",
   "metadata": {},
   "source": [
    "# Temporal Consistency using optical flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b07b2467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 22:40:23,677 - mmflow - INFO - Freeze the parameters in FlowNetCSS\n",
      "2023-01-11 22:40:24,687 - mmflow - INFO - Freeze the parameters in FlowNetS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /home/fsun/DeepViewAgg/flow/pretrained/flownet2_8x1_sfine_flyingthings3d_subset_384x768.pth\n"
     ]
    }
   ],
   "source": [
    "from mmflow.apis import init_model, inference_model\n",
    "from mmflow.datasets import visualize_flow, write_flow\n",
    "import mmcv\n",
    "from mmflow.ops import Warp\n",
    "\n",
    "# Specify the path to model config and checkpoint file\n",
    "# config_file = '/home/fsun/DeepViewAgg/flow/configs/pwcnet_ft_4x1_300k_sintel_final_384x768.py'\n",
    "# checkpoint_file = '/home/fsun/DeepViewAgg/flow/pretrained/pwcnet_ft_4x1_300k_sintel_final_384x768.pth'\n",
    "\n",
    "config_file = '/home/fsun/DeepViewAgg/flow/configs/flownet2_8x1_sfine_flyingthings3d_subset_384x768.py'\n",
    "checkpoint_file = '/home/fsun/DeepViewAgg/flow/pretrained/flownet2_8x1_sfine_flyingthings3d_subset_384x768.pth'\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_model(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "warp = Warp(mode='nearest',\n",
    "            padding_mode='zeros',\n",
    "            align_corners=False,\n",
    "            use_mask=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc74b006",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_acc': 95.78605746787561,\n",
       " 'train_macc': 80.25658729487196,\n",
       " 'train_miou': 72.34121676818701}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_dir = \"/scratch-shared/fsun/data/scannet/scans\"\n",
    "\n",
    "scene_id = 'scene0000_00'\n",
    "scene_dir = osp.join(scan_dir, scene_id)\n",
    "\n",
    "mask_im_dir = osp.join('/home/fsun/data/scannet/scans', scene_id, 'ViT_masks')\n",
    "# mask_im_dir = osp.join('/scratch-shared/fsun/data/scannet/scans', scene_id, 'color_resized')\n",
    "\n",
    "\n",
    "im_ids = sorted(os.listdir(mask_im_dir), key=lambda i: int(os.path.splitext(os.path.basename(i))[0]))\n",
    "\n",
    "\n",
    "tracker_m2f.reset(stage='train')\n",
    "tracker_mvfusion.reset(stage='train')\n",
    "\n",
    "for i in range(len(im_ids)-1):\n",
    "    \n",
    "    im1_p = osp.join(scene_dir, 'color_resized', im_ids[i])\n",
    "    im2_p = osp.join(scene_dir, 'color_resized', im_ids[i+1])\n",
    "    \n",
    "    # compute flow map from im1 to im2\n",
    "    result = inference_model(model, im2_p, im1_p)\n",
    "    flow_map = torch.tensor(result).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "#     im1 = to_tensor(Image.open(im1_p)).unsqueeze(0)\n",
    "#     im2 = to_tensor(Image.open(im1_p)).unsqueeze(0)\n",
    "\n",
    "#     # warps im1 to im2\n",
    "#     im_warped = warp(im1, flow_map).permute(0, 2, 3, 1)[0]\n",
    "\n",
    "#     plt.imshow(Image.open(im1_p))\n",
    "#     plt.show()\n",
    "#     plt.imshow(Image.open(im2_p))\n",
    "#     plt.show()\n",
    "#     plt.imshow(im_warped)\n",
    "#     plt.show()\n",
    "\n",
    "    \n",
    "    # img loading\n",
    "    seg_im_p1 = osp.join(mask_im_dir, im_ids[i])\n",
    "    seg_im_p2 = osp.join(mask_im_dir, im_ids[i+1])\n",
    "    seg_im1 = np.asarray(Image.open(seg_im_p1)) \n",
    "    seg_im2 = np.asarray(Image.open(seg_im_p2)).astype(np.int) - 1   # Adjust labels\n",
    "    \n",
    "    \n",
    "    # warping im1 to im2\n",
    "    seg_im1_semantic = torch.tensor(seg_im1).unsqueeze(0).unsqueeze(0).float()\n",
    "    seg_im_warped = warp(seg_im1_semantic, flow_map).permute(0, 2, 3, 1)[0].squeeze() - 1    # Adjust labels\n",
    "\n",
    "    # take im2 as 'pseudo gt' for temporal consistency. Thus, invalidly warped pixels should be ignored\n",
    "    # by setting the corresponding gt label to the IGNORE_LABEL\n",
    "    seg_im2[seg_im_warped == -1] = -1\n",
    "    tracker_m2f.track(pred_labels=seg_im_warped.long(), gt_labels=seg_im2, model=None)\n",
    "#     tracker_mvfusion.track(pred_labels=mm_data.data.pred, gt_labels=mm_data.data.y, model=None)\n",
    "\n",
    "    \n",
    "    seg_im1_rgb = np.array(CLASS_COLORS)[seg_im_warped.long()]\n",
    "    seg_im1_rgb = Image.fromarray(seg_im1_rgb.astype('uint8'))\n",
    "    \n",
    "    seg_im2_rgb = np.array(CLASS_COLORS)[seg_im2]\n",
    "    seg_im2_rgb = Image.fromarray(seg_im2_rgb.astype('uint8'))\n",
    "    \n",
    "\n",
    "#     plt.imshow(seg_im1_rgb)\n",
    "#     plt.show()\n",
    "#     plt.imshow(seg_im2_rgb)\n",
    "#     plt.show()\n",
    "\n",
    "    \n",
    "#     if i == 3:\n",
    "#         break\n",
    "        \n",
    "tracker_m2f.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1aac3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d_backup",
   "language": "python",
   "name": "pytorch3d_backup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
