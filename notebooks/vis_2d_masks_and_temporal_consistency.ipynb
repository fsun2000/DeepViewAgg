{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ac4756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMData debug() function changed, please uncomment the 3rd assert line when doing inference without M2F features!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select you GPU\n",
    "I_GPU = 0\n",
    "\n",
    "# Uncomment to use autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "start = time()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# torch.cuda.set_device(I_GPU)\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "from torch_geometric.data import Data\n",
    "from torch_points3d.core.multimodal.data import MMData, MMBatch\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
    "from torch_points3d.core.multimodal.image import SameSettingImageData, ImageData\n",
    "from torch_points3d.datasets.segmentation.multimodal.scannet import ScannetDatasetMM\n",
    "from torch_points3d.datasets.segmentation.scannet import CLASS_COLORS, CLASS_NAMES, CLASS_LABELS\n",
    "from torch_points3d.metrics.segmentation_tracker import SegmentationTracker\n",
    "\n",
    "from pykeops.torch import LazyTensor\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "#pio.renderers.default = 'jupyterlab'        # for local notebook\n",
    "pio.renderers.default = 'iframe_connected'  # for remote notebook. Other working (but seemingly slower) options are: 'sphinx_gallery' and 'iframe'\n",
    "\n",
    "CLASS_COLORS[0] = (174.0, 199.0, 232.0)\n",
    "CLASS_COLORS[-1] = (0, 0, 0)\n",
    "\n",
    "# from torch_points3d.datasets.segmentation.scannet import CLASS_COLORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd99300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchnet as tnt\n",
    "import torch\n",
    "from typing import Dict, Any\n",
    "import wandb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "from torch_points3d.metrics.confusion_matrix import ConfusionMatrix\n",
    "from torch_points3d.models import model_interface\n",
    "from torch_points3d.metrics.base_tracker import BaseTracker, meter_value\n",
    "from torch_points3d.metrics.meters import APMeter\n",
    "from torch_points3d.datasets.segmentation import IGNORE_LABEL\n",
    "\n",
    "from torch_geometric.nn.unpool import knn_interpolate\n",
    "from torch_points3d.core.data_transform import SaveOriginalPosId\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def meter_value(meter, dim=0):\n",
    "    return float(meter.value()[dim]) if meter.n > 0 else 0.0\n",
    "\n",
    "\n",
    "class BaseTracker:\n",
    "    def __init__(self, stage: str, wandb_log: bool, use_tensorboard: bool):\n",
    "        self._wandb = wandb_log\n",
    "        self._use_tensorboard = use_tensorboard\n",
    "        self._tensorboard_dir = os.path.join(os.getcwd(), \"tensorboard\")\n",
    "        self._n_iter = 0\n",
    "        self._finalised = False\n",
    "        self._conv_type = None\n",
    "\n",
    "        if self._use_tensorboard:\n",
    "            log.info(\n",
    "                \"Access tensorboard with the following command <tensorboard --logdir={}>\".format(self._tensorboard_dir)\n",
    "            )\n",
    "            self._writer = SummaryWriter(log_dir=self._tensorboard_dir)\n",
    "\n",
    "    def reset(self, stage=\"train\"):\n",
    "        self._stage = stage\n",
    "        self._loss_meters = {}\n",
    "        self._finalised = False\n",
    "\n",
    "    def get_metrics(self, verbose=False) -> Dict[str, Any]:\n",
    "        metrics = {}\n",
    "        for key, loss_meter in self._loss_meters.items():\n",
    "            value = meter_value(loss_meter, dim=0)\n",
    "            if value:\n",
    "                metrics[key] = meter_value(loss_meter, dim=0)\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def metric_func(self):\n",
    "        self._metric_func = {\"loss\": min}\n",
    "        return self._metric_func\n",
    "\n",
    "    def track(self, model: model_interface.TrackerInterface, **kwargs):\n",
    "        if self._finalised:\n",
    "            raise RuntimeError(\"Cannot track new values with a finalised tracker, you need to reset it first\")\n",
    "            \n",
    "        if model is not None:\n",
    "            losses = self._convert(model.get_current_losses())\n",
    "            self._append_losses(losses)\n",
    "\n",
    "    def finalise(self, *args, **kwargs):\n",
    "        \"\"\" Lifcycle method that is called at the end of an epoch. Use this to compute\n",
    "        end of epoch metrics.\n",
    "        \"\"\"\n",
    "        self._finalised = True\n",
    "\n",
    "    def _append_losses(self, losses):\n",
    "        for key, loss in losses.items():\n",
    "            if loss is None:\n",
    "                continue\n",
    "            loss_key = \"%s_%s\" % (self._stage, key)\n",
    "            if loss_key not in self._loss_meters:\n",
    "                self._loss_meters[loss_key] = tnt.meter.AverageValueMeter()\n",
    "            self._loss_meters[loss_key].add(loss)\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert(x):\n",
    "        if torch.is_tensor(x):\n",
    "            return x.detach().cpu().numpy()\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def publish_to_tensorboard(self, metrics, step):\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            metric_name = \"{}/{}\".format(metric_name.replace(self._stage + \"_\", \"\"), self._stage)\n",
    "            self._writer.add_scalar(metric_name, metric_value, step)\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_stage_from_metric_keys(stage, metrics):\n",
    "        new_metrics = {}\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            new_metrics[metric_name.replace(stage + \"_\", \"\")] = metric_value\n",
    "        return new_metrics\n",
    "\n",
    "    def publish(self, step):\n",
    "        \"\"\" Publishes the current metrics to wandb and tensorboard\n",
    "        Arguments:\n",
    "            step: current epoch\n",
    "        \"\"\"\n",
    "        metrics = self.get_metrics()\n",
    "\n",
    "        if self._wandb:\n",
    "            wandb.log(metrics, step=step)\n",
    "\n",
    "        if self._use_tensorboard:\n",
    "            self.publish_to_tensorboard(metrics, step)\n",
    "\n",
    "        # Some metrics may be intended for wandb or tensorboard\n",
    "        # tracking but not for final final model selection. Those are\n",
    "        # the metrics absent from self.metric_func and must be excluded\n",
    "        # from the output of self.publish\n",
    "        current_metrics = {\n",
    "            k: v\n",
    "            for k, v in self._remove_stage_from_metric_keys(self._stage, metrics).items()\n",
    "            if k in self.metric_func.keys()}\n",
    "\n",
    "        return {\n",
    "            \"stage\": self._stage,\n",
    "            \"epoch\": step,\n",
    "            \"current_metrics\": current_metrics,\n",
    "        }\n",
    "\n",
    "    def print_summary(self):\n",
    "        metrics = self.get_metrics(verbose=True)\n",
    "        log.info(\"\".join([\"=\" for i in range(50)]))\n",
    "        for key, value in metrics.items():\n",
    "            log.info(\"    {} = {}\".format(key, value))\n",
    "        log.info(\"\".join([\"=\" for i in range(50)]))\n",
    "\n",
    "    @staticmethod\n",
    "    def _dict_to_str(dictionnary):\n",
    "        string = \"{\"\n",
    "        for key, value in dictionnary.items():\n",
    "            string += \"%s: %.2f,\" % (str(key), value)\n",
    "        string += \"}\"\n",
    "        return string\n",
    "\n",
    "\n",
    "class SegmentationTracker(BaseTracker):\n",
    "    def __init__(\n",
    "        self, dataset, stage=\"train\", wandb_log=False, use_tensorboard: bool = False, ignore_label: int = IGNORE_LABEL\n",
    "    ):\n",
    "        \"\"\" This is a generic tracker for multimodal tasks.\n",
    "        It uses a confusion matrix in the back-end to track results.\n",
    "        Use the tracker to track an epoch.\n",
    "        You can use the reset function before you start a new epoch\n",
    "\n",
    "        Arguments:\n",
    "            dataset  -- dataset to track (used for the number of classes)\n",
    "\n",
    "        Keyword Arguments:\n",
    "            stage {str} -- current stage. (train, validation, test, etc...) (default: {\"train\"})\n",
    "            wandb_log {str} --  Log using weight and biases\n",
    "        \"\"\"\n",
    "        super(SegmentationTracker, self).__init__(stage, wandb_log, use_tensorboard)\n",
    "        self._num_classes = dataset.num_classes\n",
    "        self._ignore_label = ignore_label\n",
    "        self._dataset = dataset\n",
    "        self.reset(stage)\n",
    "        self._metric_func = {\n",
    "            \"miou\": max,\n",
    "            \"macc\": max,\n",
    "            \"acc\": max,\n",
    "            \"loss\": min,\n",
    "            \"map\": max,\n",
    "        }  # Those map subsentences to their optimization functions\n",
    "\n",
    "    def reset(self, stage=\"train\"):\n",
    "        super().reset(stage=stage)\n",
    "        self._confusion_matrix = ConfusionMatrix(self._num_classes)\n",
    "        self._acc = 0\n",
    "        self._macc = 0\n",
    "        self._miou = 0\n",
    "        self._miou_per_class = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def detach_tensor(tensor):\n",
    "        if torch.torch.is_tensor(tensor):\n",
    "            tensor = tensor.detach()\n",
    "        return tensor\n",
    "\n",
    "    @property\n",
    "    def confusion_matrix(self):\n",
    "        return self._confusion_matrix.confusion_matrix\n",
    "\n",
    "    def track(self, model: model_interface.TrackerInterface, pred_labels=None, gt_labels=None, **kwargs):\n",
    "        \"\"\" Add current model predictions (usually the result of a batch) to the tracking\n",
    "        \"\"\"\n",
    "        if not self._dataset.has_labels(self._stage):\n",
    "            return\n",
    "\n",
    "        # Feng: to evaluate M2F predictions instead of model logits\n",
    "        if pred_labels is not None and gt_labels is not None:\n",
    "            outputs = pred_labels\n",
    "            targets = gt_labels\n",
    "        else:\n",
    "            super().track(model)\n",
    "            \n",
    "            outputs = model.get_output()\n",
    "            targets = model.get_labels()\n",
    "        self._compute_metrics(outputs, targets)\n",
    "\n",
    "    def _compute_metrics(self, outputs, labels):\n",
    "        mask = labels != self._ignore_label\n",
    "        outputs = outputs[mask]\n",
    "        labels = labels[mask]\n",
    "\n",
    "        outputs = self._convert(outputs)\n",
    "        labels = self._convert(labels)\n",
    "\n",
    "        if len(labels) == 0:\n",
    "            return\n",
    "\n",
    "        assert outputs.shape[0] == len(labels)\n",
    "        \n",
    "        # Check if output is predicted label or logits\n",
    "        if len(outputs.shape) > 1:\n",
    "            self._confusion_matrix.count_predicted_batch(labels, np.argmax(outputs, 1))\n",
    "        else:\n",
    "            \n",
    "            self._confusion_matrix.count_predicted_batch(labels, outputs)\n",
    "\n",
    "        self._acc = 100 * self._confusion_matrix.get_overall_accuracy()\n",
    "        self._macc = 100 * self._confusion_matrix.get_mean_class_accuracy()\n",
    "        self._miou = 100 * self._confusion_matrix.get_average_intersection_union()\n",
    "        self._miou_per_class = {\n",
    "            i: \"{:.2f}\".format(100 * v)\n",
    "            for i, v in enumerate(self._confusion_matrix.get_intersection_union_per_class()[0])\n",
    "        }\n",
    "\n",
    "    def get_metrics(self, verbose=False) -> Dict[str, Any]:\n",
    "        \"\"\" Returns a dictionnary of all metrics and losses being tracked\n",
    "        \"\"\"\n",
    "        metrics = super().get_metrics(verbose)\n",
    "\n",
    "        metrics[\"{}_acc\".format(self._stage)] = self._acc\n",
    "        metrics[\"{}_macc\".format(self._stage)] = self._macc\n",
    "        metrics[\"{}_miou\".format(self._stage)] = self._miou\n",
    "\n",
    "        if verbose:\n",
    "            metrics[\"{}_miou_per_class\".format(self._stage)] = self._miou_per_class\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def metric_func(self):\n",
    "        return self._metric_func\n",
    "\n",
    "\n",
    "class ScannetSegmentationTracker(SegmentationTracker):\n",
    "    def reset(self, stage=\"train\"):\n",
    "        super().reset(stage=stage)\n",
    "        self._full_confusion_matrix = ConfusionMatrix(self._num_classes)\n",
    "        self._raw_datas = {}\n",
    "        self._votes = {}\n",
    "        self._vote_counts = {}\n",
    "        self._full_preds = {}\n",
    "        self._full_acc = None\n",
    "\n",
    "    def track(self, model: model_interface.TrackerInterface, full_res=False, pred_labels=None, gt_labels=None, **kwargs):\n",
    "        \"\"\" Add current model predictions (usually the result of a batch) to the tracking\n",
    "        \"\"\"\n",
    "        if pred_labels is not None and gt_labels is not None:\n",
    "            super().track(model=None, pred_labels=pred_labels, gt_labels=gt_labels)\n",
    "        else:\n",
    "            super().track(model)\n",
    "\n",
    "            # Set conv type\n",
    "            self._conv_type = model.conv_type\n",
    "\n",
    "            # Train mode or low res, nothing special to do\n",
    "            if not full_res or self._stage == \"train\" or kwargs.get(\"data\") is None:\n",
    "                return\n",
    "\n",
    "            data = kwargs.get(\"data\", model.get_input())\n",
    "            data = data.data if model.is_multimodal else data\n",
    "            self._vote(data, model.get_output())\n",
    "\n",
    "    def get_metrics(self, verbose=False) -> Dict[str, Any]:\n",
    "        \"\"\" Returns a dictionnary of all metrics and losses being tracked\n",
    "        \"\"\"\n",
    "        metrics = super().get_metrics(verbose)\n",
    "        if self._full_acc:\n",
    "            metrics[\"{}_full_acc\".format(self._stage)] = self._full_acc\n",
    "            metrics[\"{}_full_macc\".format(self._stage)] = self._full_macc\n",
    "            metrics[\"{}_full_miou\".format(self._stage)] = self._full_miou\n",
    "        return metrics\n",
    "\n",
    "    def finalise(self, full_res=False, make_submission=False, **kwargs):\n",
    "        if not full_res and not make_submission:\n",
    "            return\n",
    "        \n",
    "        self._predict_full_res()\n",
    "\n",
    "        # Compute full res metrics\n",
    "        if self._dataset.has_labels(self._stage):\n",
    "            for scan_id in self._full_preds:\n",
    "                full_labels = self._raw_datas[scan_id].y\n",
    "                # Mask ignored labels\n",
    "                mask = full_labels != self._ignore_label\n",
    "                full_labels = full_labels[mask]\n",
    "                full_preds = self._full_preds[scan_id].cpu()[mask].numpy()\n",
    "                self._full_confusion_matrix.count_predicted_batch(full_labels, full_preds)\n",
    "\n",
    "            self._full_acc = 100 * self._full_confusion_matrix.get_overall_accuracy()\n",
    "            self._full_macc = 100 * self._full_confusion_matrix.get_mean_class_accuracy()\n",
    "            self._full_miou = 100 * self._full_confusion_matrix.get_average_intersection_union()\n",
    "            \n",
    "        # Save files to disk\n",
    "        if make_submission and self._stage == \"test\":\n",
    "            self._make_submission()\n",
    "\n",
    "    def _make_submission(self):\n",
    "        orginal_class_ids = np.asarray(self._dataset.train_dataset.valid_class_idx)\n",
    "        path_to_submission = self._dataset.path_to_submission\n",
    "        for scan_id in self._full_preds:\n",
    "            full_pred = self._full_preds[scan_id].cpu().numpy().astype(np.int8)\n",
    "            full_pred = orginal_class_ids[full_pred]  # remap labels to original labels between 0 and 40\n",
    "            scan_name = self._raw_datas[scan_id].scan_name\n",
    "            path_file = osp.join(path_to_submission, \"{}.txt\".format(scan_name))\n",
    "            np.savetxt(path_file, full_pred, delimiter=\"/n\", fmt=\"%d\")\n",
    "\n",
    "    def _vote(self, data, output):\n",
    "        \"\"\" Populates scores for the points in data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : Data\n",
    "            should contain `pos` and `SaveOriginalPosId.KEY` keys\n",
    "        output : torch.Tensor\n",
    "            probablities out of the model, shape: [N,nb_classes]\n",
    "        \"\"\"\n",
    "        id_scans = data.id_scan\n",
    "        if id_scans.dim() > 1:\n",
    "            id_scans = id_scans.squeeze()\n",
    "        if self._conv_type == \"DENSE\":\n",
    "            batch_size = len(id_scans)\n",
    "            output = output.view(batch_size, -1, output.shape[-1])\n",
    "\n",
    "        for idx_batch, id_scan in enumerate(id_scans):\n",
    "            # First time we see this scan\n",
    "            if id_scan not in self._raw_datas:\n",
    "                raw_data = self._dataset.get_raw_data(self._stage, id_scan, remap_labels=True)\n",
    "                self._raw_datas[id_scan] = raw_data\n",
    "                self._vote_counts[id_scan] = torch.zeros(raw_data.pos.shape[0], dtype=torch.int)\n",
    "                self._votes[id_scan] = torch.zeros((raw_data.pos.shape[0], self._num_classes), dtype=torch.float)\n",
    "            else:\n",
    "                raw_data = self._raw_datas[id_scan]\n",
    "\n",
    "            batch_mask = idx_batch\n",
    "            if self._conv_type != \"DENSE\":\n",
    "                batch_mask = data.batch == idx_batch\n",
    "            idx = data[SaveOriginalPosId.KEY][batch_mask]\n",
    "\n",
    "            self._votes[id_scan][idx] += output[batch_mask].cpu()\n",
    "            self._vote_counts[id_scan][idx] += 1\n",
    "\n",
    "    def _predict_full_res(self):\n",
    "        \"\"\" Predict full resolution results based on votes \"\"\"\n",
    "        for id_scan in self._votes:\n",
    "            has_prediction = self._vote_counts[id_scan] > 0\n",
    "            self._votes[id_scan][has_prediction] /= self._vote_counts[id_scan][has_prediction].unsqueeze(-1)\n",
    "\n",
    "            # Upsample and predict\n",
    "            full_pred = knn_interpolate(\n",
    "                self._votes[id_scan][has_prediction],\n",
    "                self._raw_datas[id_scan].pos[has_prediction],\n",
    "                self._raw_datas[id_scan].pos,\n",
    "                k=1,\n",
    "            )\n",
    "            self._full_preds[id_scan] = full_pred.argmax(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b038de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Load predicted 2D semantic segmentation labels from directory  ViT_masks\n",
      "initialize train dataset\n",
      "initialize val dataset\n",
      "Time = 8.1 sec.\n"
     ]
    }
   ],
   "source": [
    "# Set your dataset root directory, where the data was/will be downloaded\n",
    "DATA_ROOT = '/scratch-shared/fsun/dvata'\n",
    "\n",
    "dataset_config = 'segmentation/multimodal/Feng/scannet-neucon-smallres-m2f-TC'   \n",
    "models_config = 'segmentation/multimodal/Feng/mvfusion'    # model family\n",
    "model_name = 'MVFusion_3D_small_6views'                       # specific model\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    f'data={dataset_config}',\n",
    "    f'models={models_config}',\n",
    "    f'model_name={model_name}',\n",
    "    f'data.dataroot={DATA_ROOT}',\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n",
    "cfg.data.load_m2f_masks = True   # load Mask2Former predicted masks\n",
    "\n",
    "cfg.data.m2f_preds_dirname = 'ViT_masks'\n",
    "cfg.data.n_views = 6 #cfg.models[model_name].backbone.transformer.n_views\n",
    "print(cfg.data.n_views)\n",
    "\n",
    "# Dataset instantiation\n",
    "start = time()\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f6f3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_mvfusion = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "tracker_m2f = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "tracker_gt = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c964b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: MVFusion_3D_small_6views\n",
      "task:  segmentation.multimodal\n",
      "tested_model_name:  MVFusion_3D_small_6views\n",
      "class_name:  MVFusionAPIModel\n",
      "model_module:  torch_points3d.models.segmentation.multimodal.Feng.mvfusion_3d\n",
      "name, cls of chosen model_cls:  MVFusionAPIModel <class 'torch_points3d.models.segmentation.multimodal.Feng.mvfusion_3d.MVFusionAPIModel'>\n",
      "x feature dim:  {'FEAT': 3}\n",
      "nc_in:  67\n",
      "nc_in:  64\n",
      "nc_in:  32\n",
      "nc_in:  64\n",
      "nc_in:  128\n",
      "nc_in:  256\n",
      "nc_in:  128\n",
      "nc_in:  128\n",
      "nc_in:  96\n",
      "nc_in:  96\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from torch_points3d.models.model_factory import instantiate_model\n",
    "\n",
    "# Set your parameters\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/2022-12-07/12-07-34' # 3rd run\n",
    "checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/ViT_masks_3rd_run'\n",
    "\n",
    "# Create the model\n",
    "print(f\"Creating model: {cfg.model_name}\")\n",
    "model = instantiate_model(cfg, dataset)\n",
    "\n",
    "# Load the checkpoint and recover the model weights\n",
    "checkpoint = torch.load(f'{checkpoint_dir}/{model_name}.pt', map_location='cpu')\n",
    "model.load_state_dict_with_same_shape(checkpoint['models']['latest'], strict=False)\n",
    "\n",
    "# Prepare the model for inference\n",
    "model = model.eval().cuda()\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da311f5",
   "metadata": {},
   "source": [
    "# Temporal Consistency using optical flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae06a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.utils.multimodal import lexargsort\n",
    "from torch_points3d.core.multimodal.csr import CSRData\n",
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b07b2467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 14:22:18,093 - mmflow - INFO - Freeze the parameters in FlowNetCSS\n",
      "2023-01-12 14:22:19,032 - mmflow - INFO - Freeze the parameters in FlowNetS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /home/fsun/DeepViewAgg/flow/pretrained/flownet2_8x1_sfine_flyingthings3d_subset_384x768.pth\n"
     ]
    }
   ],
   "source": [
    "from mmflow.apis import init_model, inference_model\n",
    "from mmflow.datasets import visualize_flow, write_flow\n",
    "import mmcv\n",
    "from mmflow.ops import Warp\n",
    "\n",
    "# Specify the path to model config and checkpoint file\n",
    "# config_file = '/home/fsun/DeepViewAgg/flow/configs/pwcnet_ft_4x1_300k_sintel_final_384x768.py'\n",
    "# checkpoint_file = '/home/fsun/DeepViewAgg/flow/pretrained/pwcnet_ft_4x1_300k_sintel_final_384x768.pth'\n",
    "\n",
    "config_file = '/home/fsun/DeepViewAgg/flow/configs/flownet2_8x1_sfine_flyingthings3d_subset_384x768.py'\n",
    "checkpoint_file = '/home/fsun/DeepViewAgg/flow/pretrained/flownet2_8x1_sfine_flyingthings3d_subset_384x768.pth'\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "flow_model = init_model(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "warp = Warp(mode='nearest',\n",
    "            padding_mode='zeros',\n",
    "            align_corners=False,\n",
    "            use_mask=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4af93bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm_data = dataset.val_dataset[0]\n",
    "# mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cc74b006",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tracker_m2f.reset(stage='train')\n",
    "# tracker_mvfusion.reset(stage='train')\n",
    "\n",
    "\n",
    "# for sample_idx in range(len(dataset.val_dataset)):\n",
    "    \n",
    "# #     mm_data = dataset.val_dataset[sample_idx]\n",
    "    \n",
    "#     # Create a MMBatch and run inference\n",
    "#     batch = MMBatch.from_mm_data_list([mm_data])\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         model.set_input(batch, model.device)\n",
    "#         model(batch)\n",
    "\n",
    "        \n",
    "#     #################### ORIG CODE IN TRAINER.PY _track_2d_results\n",
    "        \n",
    "#     # Recover the predicted labels for visualization\n",
    "#     mm_data.data.pred = model.output.detach().cpu().argmax(1)\n",
    "    \n",
    "#     mappings = mm_data.modalities['image'][0].mappings\n",
    "#     point_ids = torch.arange(\n",
    "#                     mappings.num_groups, device=mappings.device).repeat_interleave(\n",
    "#                     mappings.pointers[1:] - mappings.pointers[:-1])\n",
    "#     image_ids = mappings.images.repeat_interleave(\n",
    "#                     mappings.values[1].pointers[1:] - mappings.values[1].pointers[:-1])    \n",
    "#     pixels_full = mappings.pixels\n",
    "\n",
    "#     # Sort point and image ids based on image_id\n",
    "#     idx_sort = lexargsort(image_ids, point_ids)\n",
    "#     image_ids = image_ids[idx_sort]\n",
    "#     point_ids = point_ids[idx_sort]\n",
    "#     pixels_full = pixels_full[idx_sort].long()\n",
    "\n",
    "#     # Get pointers for easy indexing\n",
    "#     pointers = CSRData._sorted_indices_to_pointers(image_ids)\n",
    "\n",
    "    \n",
    "#     im_paths = mm_data.modalities['image'][0].gt_mask_path\n",
    "#     scan_dir = os.sep.join(im_paths[0].split(os.sep)[:-2])\n",
    "    \n",
    "#     color_im_dir = osp.join(scan_dir, 'color_resized')\n",
    "    \n",
    "#     input_mask_name = mm_data.modalities['image'][0].m2f_pred_mask_path[0].split(os.sep)[-2]\n",
    "    \n",
    "#     # Dirty workaround for masks in different directory\n",
    "#     if input_mask_name == 'ViT_masks':\n",
    "#         scan_id = scan_dir.split(os.sep)[-1]\n",
    "#         mask_im_dir = osp.join(\"/home/fsun/data/scannet/scans\", scan_id, input_mask_name)\n",
    "#         refined_mask_im_dir = osp.join(scan_dir, input_mask_name + '_refined')\n",
    "#     else:\n",
    "#         mask_im_dir = osp.join(scan_dir, input_mask_name)\n",
    "#         refined_mask_im_dir = osp.join(scan_dir, input_mask_name + '_refined')\n",
    "#     print(refined_mask_im_dir)\n",
    "#     os.makedirs(refined_mask_im_dir, exist_ok=True)\n",
    "    \n",
    "#     im_names = [p.split(os.sep)[-1] for p in im_paths]\n",
    "#     # Image indices of sorted list\n",
    "#     im_sort_indices = sorted(range(len(im_names)), key=lambda k: int(os.path.splitext(os.path.basename(im_names[k]))[0]))\n",
    "#     # Sorted image names\n",
    "# #     im_names = sorted(im_names, key=lambda i: int(os.path.splitext(os.path.basename(i))[0]))\n",
    "\n",
    "\n",
    "#     # Loop over all N views\n",
    "# #     for i, x in enumerate(mm_data.modalities['image'][0]):\n",
    "\n",
    "#     # Skip last image since we grab pairs\n",
    "#     for i in range(len(im_sort_indices[:-1])):\n",
    "        \n",
    "#         print(im_sort_indices[i], im_sort_indices[i+1])\n",
    "\n",
    "#         im1_name, im2_name = im_names[im_sort_indices[i]], im_names[im_sort_indices[i+1]]\n",
    "#         print(im1_name, im2_name)\n",
    "#         if ( int(im2_name.split(\".\")[0]) - int(im1_name.split(\".\")[0]) ) > 50:\n",
    "#             continue\n",
    "        \n",
    "#         x = mm_data.modalities['image'][0][i]\n",
    "\n",
    "#         # Grab the 3D points corresponding to ith view\n",
    "#         start, end = pointers[im_sort_indices[i]], pointers[im_sort_indices[i] + 1]    \n",
    "#         points = point_ids[start:end]\n",
    "#         pixels = pixels_full[start:end]\n",
    "#         # Image (x, y) pixel index\n",
    "#         w, h = pixels[:, 0], pixels[:, 1]\n",
    "\n",
    "#         # Grab set of points visible in current view\n",
    "#         mm_data_of_view = mm_data[points]\n",
    "\n",
    "#         im_ref_w, im_ref_h = x.ref_size\n",
    "\n",
    "#         # Get nearest neighbor interpolated projection image filled with 3D labels\n",
    "#         pred_mask_2d = -1 * torch.ones((im_ref_h, im_ref_w), dtype=torch.long, device=mm_data_of_view.device)    \n",
    "#         pred_mask_2d[h, w] = mm_data_of_view.data.pred.squeeze()\n",
    "\n",
    "#         nearest_neighbor = scipy.ndimage.morphology.distance_transform_edt(\n",
    "#             pred_mask_2d==-1, return_distances=False, return_indices=True)    \n",
    "#         pred_mask_2d = pred_mask_2d[nearest_neighbor].numpy().astype(np.uint8)\n",
    "#         pred_mask_2d = Image.fromarray(pred_mask_2d, 'L') \n",
    "        \n",
    "#         pred_mask_2d = pred_mask_2d.resize((640, 480), resample=0)\n",
    "        \n",
    "# #         # SAVE REFINED MASK IN GIVEN DIR\n",
    "# #         print(osp.join(scan_dir, input_mask_name + '_refined', im1_name))\n",
    "# #         pred_mask_2d.save(osp.join(scan_dir, input_mask_name + '_refined', im1_name))\n",
    "\n",
    "        \n",
    "#         pred_mask_2d = np.asarray(pred_mask_2d)\n",
    "        \n",
    "    \n",
    "            \n",
    "#         im1_p = osp.join(color_im_dir, im1_name)\n",
    "#         im2_p = osp.join(color_im_dir, im2_name)\n",
    "\n",
    "#         # compute flow map from im1 to im2\n",
    "#         result = inference_model(flow_model, im2_p, im1_p)\n",
    "#         flow_map = torch.tensor(result).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "#         # load 2D input mask \n",
    "#         seg_im_p1 = osp.join(mask_im_dir, im1_name)\n",
    "#         seg_im_p2 = osp.join(mask_im_dir, im2_name)\n",
    "#         seg_im1 = np.asarray(Image.open(seg_im_p1)) \n",
    "#         seg_im2 = np.asarray(Image.open(seg_im_p2)).astype(np.int) - 1   # Adjust labels\n",
    "\n",
    "\n",
    "#         # warping im1 to im2\n",
    "#         seg_im1_semantic = torch.tensor(seg_im1).unsqueeze(0).unsqueeze(0).float()\n",
    "#         seg_im_warped = warp(seg_im1_semantic, flow_map).permute(0, 2, 3, 1)[0].squeeze() - 1    # Adjust labels\n",
    "\n",
    "#         # take im2 as 'pseudo gt' for temporal consistency. Thus, invalidly warped pixels should be ignored\n",
    "#         # by setting the corresponding gt label to the IGNORE_LABEL\n",
    "#         seg_im2[seg_im_warped == -1] = -1\n",
    "#         tracker_m2f.track(pred_labels=seg_im_warped.long(), gt_labels=seg_im2, model=None)\n",
    "        \n",
    "#         # warping refined im1 to im2\n",
    "# #         pred_mask_2d\n",
    "        \n",
    "# #         tracker_mvfusion.track(pred_labels=mm_data.data.pred, gt_labels=mm_data.data.y, model=None)\n",
    "\n",
    "\n",
    "# #         # Visualizations of warped images\n",
    "# #         seg_im1_rgb = np.array(CLASS_COLORS)[seg_im_warped.long()]\n",
    "# #         seg_im1_rgb = Image.fromarray(seg_im1_rgb.astype('uint8'))\n",
    "\n",
    "# #         seg_im2_rgb = np.array(CLASS_COLORS)[seg_im2]\n",
    "# #         seg_im2_rgb = Image.fromarray(seg_im2_rgb.astype('uint8'))\n",
    "\n",
    "\n",
    "# #         plt.imshow(seg_im1_rgb)\n",
    "# #         plt.show()\n",
    "# #         plt.imshow(seg_im2_rgb)\n",
    "# #         plt.show()\n",
    "        \n",
    "#     break\n",
    "        \n",
    "    \n",
    "#     ###############################################################\n",
    "\n",
    "# #     break\n",
    "    \n",
    "# #     # Uncomment\n",
    "# #     for i in range(len(im_ids)-1):\n",
    "\n",
    "# #         im1_p = osp.join(scene_dir, 'color_resized', im_ids[i])\n",
    "# #         im2_p = osp.join(scene_dir, 'color_resized', im_ids[i+1])\n",
    "\n",
    "# #         # compute flow map from im1 to im2\n",
    "# #         result = inference_model(model, im2_p, im1_p)\n",
    "# #         flow_map = torch.tensor(result).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "# #     #     im1 = to_tensor(Image.open(im1_p)).unsqueeze(0)\n",
    "# #     #     im2 = to_tensor(Image.open(im1_p)).unsqueeze(0)\n",
    "\n",
    "# #     #     # warps im1 to im2\n",
    "# #     #     im_warped = warp(im1, flow_map).permute(0, 2, 3, 1)[0]\n",
    "\n",
    "# #     #     plt.imshow(Image.open(im1_p))\n",
    "# #     #     plt.show()\n",
    "# #     #     plt.imshow(Image.open(im2_p))\n",
    "# #     #     plt.show()\n",
    "# #     #     plt.imshow(im_warped)\n",
    "# #     #     plt.show()\n",
    "\n",
    "\n",
    "# #         # img loading\n",
    "# #         seg_im_p1 = osp.join(mask_im_dir, im_ids[i])\n",
    "# #         seg_im_p2 = osp.join(mask_im_dir, im_ids[i+1])\n",
    "# #         seg_im1 = np.asarray(Image.open(seg_im_p1)) \n",
    "# #         seg_im2 = np.asarray(Image.open(seg_im_p2)).astype(np.int) - 1   # Adjust labels\n",
    "\n",
    "\n",
    "# #         # warping im1 to im2\n",
    "# #         seg_im1_semantic = torch.tensor(seg_im1).unsqueeze(0).unsqueeze(0).float()\n",
    "# #         seg_im_warped = warp(seg_im1_semantic, flow_map).permute(0, 2, 3, 1)[0].squeeze() - 1    # Adjust labels\n",
    "\n",
    "# #         # take im2 as 'pseudo gt' for temporal consistency. Thus, invalidly warped pixels should be ignored\n",
    "# #         # by setting the corresponding gt label to the IGNORE_LABEL\n",
    "# #         seg_im2[seg_im_warped == -1] = -1\n",
    "# #         tracker_m2f.track(pred_labels=seg_im_warped.long(), gt_labels=seg_im2, model=None)\n",
    "# #     #     tracker_mvfusion.track(pred_labels=mm_data.data.pred, gt_labels=mm_data.data.y, model=None)\n",
    "\n",
    "\n",
    "# #         seg_im1_rgb = np.array(CLASS_COLORS)[seg_im_warped.long()]\n",
    "# #         seg_im1_rgb = Image.fromarray(seg_im1_rgb.astype('uint8'))\n",
    "\n",
    "# #         seg_im2_rgb = np.array(CLASS_COLORS)[seg_im2]\n",
    "# #         seg_im2_rgb = Image.fromarray(seg_im2_rgb.astype('uint8'))\n",
    "\n",
    "\n",
    "# #     #     plt.imshow(seg_im1_rgb)\n",
    "# #     #     plt.show()\n",
    "# #     #     plt.imshow(seg_im2_rgb)\n",
    "# #     #     plt.show()\n",
    "\n",
    "\n",
    "# #     #     if i == 3:\n",
    "# #     #         break\n",
    "\n",
    "# tracker_m2f.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c2f5861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene0011_00\n",
      "Temporal Consistency Scores measured over all validation scenes, using all image views\n",
      "Mask2Former and refined scores and gt scores:\n",
      "m2f  {'train_acc': 91.79619787115168, 'train_macc': 73.6956810795494, 'train_miou': 47.85911383305274}\n",
      "mvfusion  {'train_acc': 90.45814649800153, 'train_macc': 74.24482720161333, 'train_miou': 47.142153319771786}\n",
      "gt  {'train_acc': 92.0258673582081, 'train_macc': 86.74310367516654, 'train_miou': 70.76243953631104}\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "\n",
    "###########\n",
    "# mask_foldername = 'ViT_masks'\n",
    "# refined_mask_foldername = 'ViT_masks_refined'\n",
    "# mask_scans_dir = '/home/fsun/data/scannet/scans'\n",
    "###########\n",
    "mask_foldername = 'm2f_masks'\n",
    "refined_mask_foldername = 'm2f_masks_refined'\n",
    "mask_scans_dir = '/scratch-shared/fsun/data/scannet/scans'\n",
    "###########\n",
    "\n",
    "tracker_m2f.reset(stage='train')\n",
    "tracker_mvfusion.reset(stage='train')\n",
    "tracker_gt.reset(stage='train')\n",
    "\n",
    "with open(\"/scratch-shared/fsun/data/scannet/splits/scannetv2_val.txt\", 'r') as f:\n",
    "    scan_ids = [x.split()[0] for x in f.readlines()]    \n",
    "    scan_ids = sorted(scan_ids)\n",
    "\n",
    "scans_dir = \"/scratch-shared/fsun/data/scannet/scans\"\n",
    "\n",
    "for scan_id in scan_ids:\n",
    "    print(scan_id)\n",
    "    scan_dir = osp.join(scans_dir, scan_id)\n",
    "    \n",
    "    mask_scan_dir = osp.join(mask_scans_dir, scan_id)\n",
    "    \n",
    "    mask_dir = osp.join(mask_scan_dir, mask_foldername)\n",
    "    refined_mask_dir = osp.join(scan_dir, refined_mask_foldername)\n",
    "    gt_mask_dir = osp.join(scan_dir, 'label-filt-scannet20')\n",
    "    \n",
    "    refined_mask_names = os.listdir(refined_mask_dir)\n",
    "    refined_mask_names = sorted(refined_mask_names, key=lambda i: int(os.path.splitext(os.path.basename(i))[0]))\n",
    "    \n",
    "    color_im_dir = osp.join(scan_dir, 'color_resized')\n",
    "    \n",
    "    # Loop over all pairs of consecutive images\n",
    "    for i in range(len(refined_mask_names) - 1):\n",
    "        cur_frame_name, next_frame_name = refined_mask_names[i], refined_mask_names[i+1]\n",
    "        cur_frame_id, next_frame_id = int(refined_mask_names[i].split(\".\")[0]), int(refined_mask_names[i+1].split(\".\")[0])\n",
    "        \n",
    "        if next_frame_id - cur_frame_id > 50:\n",
    "            continue\n",
    "                \n",
    "        cur_color_p, next_color_p = osp.join(color_im_dir, cur_frame_name), osp.join(color_im_dir, next_frame_name)\n",
    "        cur_color_im, next_color_im = Image.open(cur_color_p), Image.open(next_color_p)\n",
    "        \n",
    "        result = inference_model(flow_model, next_color_p, cur_color_p)\n",
    "        flow_map = torch.tensor(result).permute(2, 0, 1).unsqueeze(0)\n",
    "        \n",
    "        ################# 2D input masks\n",
    "        seg_im_p1 = osp.join(mask_dir, cur_frame_name)\n",
    "        seg_im_p2 = osp.join(mask_dir, next_frame_name)\n",
    "        seg_im1 = np.asarray(Image.open(seg_im_p1)) \n",
    "        seg_im2 = np.asarray(Image.open(seg_im_p2)).astype(np.int) - 1   # Adjust labels\n",
    "        \n",
    "        # warping im1 to im2\n",
    "        seg_im1_semantic = torch.tensor(seg_im1).unsqueeze(0).unsqueeze(0).float()\n",
    "        seg_im_warped = warp(seg_im1_semantic, flow_map).permute(0, 2, 3, 1)[0].squeeze() - 1    # Adjust labels\n",
    "\n",
    "        # take im2 as 'pseudo gt' for temporal consistency. Thus, invalidly warped pixels should be ignored\n",
    "        # by setting the corresponding gt label to the IGNORE_LABEL\n",
    "        seg_im2[seg_im_warped == -1] = -1\n",
    "        tracker_m2f.track(pred_labels=seg_im_warped.long(), gt_labels=seg_im2, model=None)\n",
    "\n",
    "        ################ 2D refined masks\n",
    "        seg_im_p1 = osp.join(refined_mask_dir, cur_frame_name)\n",
    "        seg_im_p2 = osp.join(refined_mask_dir, next_frame_name)\n",
    "        seg_im1 = np.asarray(Image.open(seg_im_p1))  + 1 # Adjust labels for flow warping (invalid pixels will be set to 0)\n",
    "        \n",
    "        # Resize in case\n",
    "        seg_im2 = Image.open(seg_im_p2).resize((640, 480), resample=0)\n",
    "        seg_im2 = np.asarray(seg_im2).astype(np.int) \n",
    "        \n",
    "        # warping im1 to im2\n",
    "        seg_im1_semantic = torch.tensor(seg_im1).unsqueeze(0).unsqueeze(0).float()\n",
    "        seg_im_warped = warp(seg_im1_semantic, flow_map).permute(0, 2, 3, 1)[0].squeeze() - 1 # Adjust labels back to normal\n",
    "\n",
    "        # take im2 as 'pseudo gt' for temporal consistency. Thus, invalidly warped pixels should be ignored\n",
    "        # by setting the corresponding gt label to the IGNORE_LABEL\n",
    "        seg_im2[seg_im_warped == -1] = -1\n",
    "        tracker_mvfusion.track(pred_labels=seg_im_warped.long(), gt_labels=seg_im2, model=None)\n",
    "        \n",
    "        ################ 2D GT masks\n",
    "        seg_im_p1 = osp.join(gt_mask_dir, cur_frame_name)\n",
    "        seg_im_p2 = osp.join(gt_mask_dir, next_frame_name)\n",
    "        seg_im1 = np.asarray(Image.open(seg_im_p1))  # invalid pixels will be set to 0\n",
    "        \n",
    "        # Resize in case\n",
    "        seg_im2 = Image.open(seg_im_p2).resize((640, 480), resample=0)\n",
    "        seg_im2 = np.asarray(seg_im2).astype(np.int) - 1  # adjust labels\n",
    "        \n",
    "        # warping im1 to im2\n",
    "        seg_im1_semantic = torch.tensor(seg_im1).unsqueeze(0).unsqueeze(0).float()\n",
    "        seg_im_warped = warp(seg_im1_semantic, flow_map).permute(0, 2, 3, 1)[0].squeeze() - 1  # adjust labels\n",
    "\n",
    "        # take im2 as 'pseudo gt' for temporal consistency. Thus, invalidly warped pixels should be ignored\n",
    "        # by setting the corresponding gt label to the IGNORE_LABEL\n",
    "        seg_im2[seg_im_warped == -1] = -1\n",
    "        tracker_gt.track(pred_labels=seg_im_warped.long(), gt_labels=seg_im2, model=None)       \n",
    "        \n",
    "#         seg_im1_rgb = np.array(CLASS_COLORS)[seg_im_warped.long()]\n",
    "#         seg_im1_rgb = Image.fromarray(seg_im1_rgb.astype('uint8'))\n",
    "\n",
    "#         seg_im2_rgb = np.array(CLASS_COLORS)[seg_im2]\n",
    "#         seg_im2_rgb = Image.fromarray(seg_im2_rgb.astype('uint8'))\n",
    "        \n",
    "#         plt.imshow(seg_im1_rgb)\n",
    "#         plt.show()\n",
    "#         plt.imshow(seg_im2_rgb)\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "#     break\n",
    "\n",
    "print(\"Temporal Consistency Scores measured over all validation scenes, using all image views\")\n",
    "print(\"Mask2Former and refined scores and gt scores:\")\n",
    "print(\"m2f \", tracker_m2f.get_metrics())\n",
    "print(\"mvfusion \", tracker_mvfusion.get_metrics())\n",
    "print(\"gt \", tracker_gt.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "63672f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m2f  {'train_acc': 90.5461629810143, 'train_macc': 87.48580362561194, 'train_miou': 78.02501976060385}\n",
      "mvfusion  {'train_acc': 95.68884017721575, 'train_macc': 95.12010618343393, 'train_miou': 90.59642914159319}\n",
      "gt  {'train_acc': 97.54413197503662, 'train_macc': 97.44599314771565, 'train_miou': 94.79914213901814}\n"
     ]
    }
   ],
   "source": [
    "print(\"m2f \", tracker_m2f.get_metrics())\n",
    "print(\"mvfusion \", tracker_mvfusion.get_metrics())\n",
    "print(\"gt \", tracker_gt.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6768c871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'90.48 & 92.49 & 74.11 & 89.10 & 84.05 & 85.09 & 84.04 & 80.20 & 80.28 & 86.60 & 68.65 & 75.91 & 76.75 & 82.47 & 75.49 & 81.56 & 91.85 & 82.37 & 85.53 & 71.37'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\" & \".join(list(tracker_m2f._miou_per_class.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "04a36021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'94.06 & 92.56 & 91.99 & 95.58 & 88.37 & 95.12 & 91.12 & 90.76 & 91.35 & 94.83 & 91.32 & 89.51 & 90.82 & 92.90 & 92.93 & 88.93 & 89.07 & 87.16 & 90.50 & 90.74'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\" & \".join(list(tracker_mvfusion._miou_per_class.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97575f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" & \".join(list(tracker_mvfusion._miou_per_class.values())))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d_backup",
   "language": "python",
   "name": "pytorch3d_backup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
