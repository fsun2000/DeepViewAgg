{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ScanNet\n",
    "\n",
    "This notebook lets you instantiate the **[ScanNet](http://www.scan-net.org/)** dataset from scratch and visualize **3D+2D room samples**.\n",
    "\n",
    "Note that you will need **at least 1.2T** available for the SanNet raw dataset and **at least 64G** for the processed files at **5cm voxel resolution** and **320x240 image resolution**. \n",
    "\n",
    "The ScanNet dataset is composed of **rooms** of video acquisitions of indoor scenes. Thes video streams were used to produce a point cloud and images.\n",
    "\n",
    "Each room is small enough to be loaded at once into a **64G RAM** memory. The `ScannetDatasetMM` class from `torch_points3d.datasets.segmentation.multimodal.scannet` deals with loading the room and part of the images of the associated video stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select you GPU\n",
    "I_GPU = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps]: Warning, no cuda detected. Switching to cpu only.\n",
      "MMData debug() function changed, please uncomment the 3rd assert line when doing inference without M2F features!\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to use autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from omegaconf import OmegaConf\n",
    "start = time()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# torch.cuda.set_device(I_GPU)\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "from torch_geometric.data import Data\n",
    "from torch_points3d.core.multimodal.data import MMData, MMBatch\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
    "from torch_points3d.core.multimodal.image import SameSettingImageData, ImageData\n",
    "from torch_points3d.datasets.segmentation.multimodal.scannet import ScannetDatasetMM\n",
    "from torch_points3d.datasets.segmentation.scannet import CLASS_COLORS, CLASS_NAMES, CLASS_LABELS\n",
    "from torch_points3d.metrics.segmentation_tracker import SegmentationTracker\n",
    "\n",
    "from pykeops.torch import LazyTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `visualize_mm_data` does not throw any error but the visualization does not appear, you may need to change your plotly renderer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "#pio.renderers.default = 'jupyterlab'        # for local notebook\n",
    "pio.renderers.default = 'iframe_connected'  # for remote notebook. Other working (but seemingly slower) options are: 'sphinx_gallery' and 'iframe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation\n",
    "\n",
    "The following will instantiate the dataset. If the data is not found at `DATA_ROOT`, the folder structure will be created there and the raw dataset will be downloaded there. \n",
    "\n",
    "**Memory-friendly tip** : if you have already downloaded the dataset once and simply want to instantiate a new dataset with different preprocessing (*e.g* change 3D or 2D resolution, mapping parameterization, etc), I recommend you manually replicate the folder hierarchy of your already-existing dataset and create a symlink to its `raw/` directory to avoid downloading and storing (very) large files twice.\n",
    "\n",
    "You will find the config file ruling the dataset creation at `conf/data/segmentation/multimodal/scannet-sparse.yaml`. You may edit this file or create new configs inheriting from this one using Hydra and create the associated dataset by modifying `dataset_config` accordingly in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your dataset root directory, where the data was/will be downloaded\n",
    "DATA_ROOT = '/project/fsun/dvata'\n",
    "\n",
    "dataset_config = 'segmentation/multimodal/Feng/scannet-neucon-smallres-m2f'   \n",
    "models_config = 'segmentation/multimodal/Feng/mvfusion'    # model family\n",
    "model_name = 'MVFusion_3D'                       # specific model\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    f'data={dataset_config}',\n",
    "    f'models={models_config}',\n",
    "    f'model_name={model_name}',\n",
    "    f'data.dataroot={DATA_ROOT}',\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n",
    "cfg.data.load_m2f_masks = True   # load Mask2Former predicted masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will now be created based on the parsed configuration. I recommend having **at least 1.2T** available for the SanNet raw dataset and **at least 64G** for the processed files at **5cm voxel resolution** and **320x240 image resolution**. \n",
    "\n",
    "As long as you do not change core dataset parameters, preprocessing should only be performed once for your dataset. It may take some time, **mostly depending on the 3D and 2D resolutions** you choose to work with (the larger the slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_m2f_masks:  True\n",
      "initialize test dataset\n",
      "line 720 scannet.py: split == 'test'\n",
      "Time = 1.3 sec.\n"
     ]
    }
   ],
   "source": [
    "# Dataset instantiation\n",
    "start = time()\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)|\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.test_dataset[0].num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the multimodal samples produced by the dataset, we need to remove some of the dataset transforms that affect points, images and mappings. The `sample_real_data` function will be used to get samples without breaking mappings consistency for visualization.\n",
    "\n",
    "At training and evaluation time, these transforms are used for data augmentation, dynamic size batching (see our [paper](https://arxiv.org/submit/4264152)), etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import *\n",
    "from torch_points3d.core.data_transform import *\n",
    "from torch_points3d.core.data_transform.multimodal.image import *\n",
    "from torch_points3d.datasets.base_dataset import BaseDataset\n",
    "from torch_points3d.datasets.base_dataset_multimodal import BaseDatasetMM\n",
    "\n",
    "# Transforms on 3D points that we need to exclude for visualization purposes\n",
    "augmentations_3d = [\n",
    "    ElasticDistortion, Random3AxisRotation, RandomNoise, RandomRotate, \n",
    "    RandomScaleAnisotropic, RandomSymmetry, ShiftVoxels]\n",
    "exclude_3d_viz = augmentations_3d + [AddFeatsByKeys, Center, GridSampling3D]\n",
    "\n",
    "# Transforms on 2D images and mappings that we need to exclude for visualization\n",
    "# purposes\n",
    "augmentations_2d = [JitterMappingFeatures, ColorJitter, RandomHorizontalFlip]\n",
    "exclude_2d_viz = [RandomHorizontalFlip]\n",
    "exclude_2d_viz = augmentations_2d + [ToFloatImage, Normalize]\n",
    "\n",
    "\n",
    "\n",
    "def sample_real_data(tg_dataset, idx=0, exclude_3d=None, exclude_2d=None):\n",
    "    \"\"\"\n",
    "    Temporarily remove the 3D and 2D transforms affecting the point \n",
    "    positions and images from the dataset to better visualize points \n",
    "    and images relative positions.\n",
    "    \"\"\"    \n",
    "    # Remove some 3D transforms\n",
    "    transform_3d = tg_dataset.transform\n",
    "    if exclude_3d:\n",
    "        tg_dataset.transform = BaseDataset.remove_transform(transform_3d, exclude_3d)\n",
    "\n",
    "    # Remove some 2D transforms, if any\n",
    "    is_multimodal = hasattr(tg_dataset, 'transform_image')\n",
    "    if is_multimodal and exclude_2d:\n",
    "        transform_2d = tg_dataset.transform_image\n",
    "        tg_dataset.transform_image = BaseDatasetMM.remove_multimodal_transform(transform_2d, exclude_2d)\n",
    "    \n",
    "    # Get a sample from the dataset, with transforms excluded\n",
    "    out = tg_dataset[idx]\n",
    "    \n",
    "    # Restore transforms\n",
    "    tg_dataset.transform = transform_3d\n",
    "    if is_multimodal and exclude_2d:\n",
    "        tg_dataset.transform_image = transform_2d\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a single multimodal sample\n",
    "\n",
    "We can now pick samples from the train, val and test datasets.\n",
    "\n",
    "Please refer to `torch_points3d/visualization/multimodal_data` for more details on visualization options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm_data.modalities['image'][0].x.shape\n",
    "# mm_data.modalities['image'][0].m2f_pred_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMData(\n",
       "    data = Data(coords=[52285, 3], grid_size=[1], id_scan=[1], mapping_index=[52285], mvfusion_input=[42774, 9, 10], origin_id=[52285], pos=[52285, 3], x=[52285, 3], y=[52285])\n",
       "    image = ImageData(num_settings=1, num_views=100, num_points=52285, device=cpu)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_data = dataset.train_dataset[0]\n",
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = mm_data[torch.round(torch.linspace(0, 52285-1, 5000)).long()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm_data = sample_real_data(dataset.train_dataset, idx=2, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
    "# print(mm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_22.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFD\n",
      "bacl:  m2f_pred_mask\n",
      "LOLKEK\n",
      "back is m2f_pred_mask\n",
      "torch.Size([42, 1, 240, 320])\n",
      "class_colors:  [(174.0, 199.0, 232.0), (152.0, 223.0, 138.0), (31.0, 119.0, 180.0), (255.0, 187.0, 120.0), (188.0, 189.0, 34.0), (140.0, 86.0, 75.0), (255.0, 152.0, 150.0), (214.0, 39.0, 40.0), (197.0, 176.0, 213.0), (148.0, 103.0, 189.0), (196.0, 156.0, 148.0), (23.0, 190.0, 207.0), (247.0, 182.0, 210.0), (219.0, 219.0, 141.0), (255.0, 127.0, 14.0), (158.0, 218.0, 229.0), (44.0, 160.0, 44.0), (112.0, 128.0, 144.0), (227.0, 119.0, 194.0), (82.0, 84.0, 163.0), (225, 225, 255)]\n",
      "torch.ByteTensor(class_colors) torch.Size([21, 3])\n",
      "im.pred torch.Size([42, 240, 320])\n",
      " im.background  torch.Size([42, 3, 240, 320])\n",
      "images[-1].background torch.Size([42, 3, 240, 320])\n",
      "im pred:  torch.Size([42, 240, 320])\n",
      "pred\n",
      "data.y tensor([ 1,  1,  1,  ..., -1, -1,  1])\n",
      "class_colors [(174.0, 199.0, 232.0), (152.0, 223.0, 138.0), (31.0, 119.0, 180.0), (255.0, 187.0, 120.0), (188.0, 189.0, 34.0), (140.0, 86.0, 75.0), (255.0, 152.0, 150.0), (214.0, 39.0, 40.0), (197.0, 176.0, 213.0), (148.0, 103.0, 189.0), (196.0, 156.0, 148.0), (23.0, 190.0, 207.0), (247.0, 182.0, 210.0), (219.0, 219.0, 141.0), (255.0, 127.0, 14.0), (158.0, 218.0, 229.0), (44.0, 160.0, 44.0), (112.0, 128.0, 144.0), (227.0, 119.0, 194.0), (82.0, 84.0, 163.0), (225, 225, 255)]\n",
      "viz.shape:  torch.Size([42, 3, 240, 320])\n",
      "color.shape:  torch.Size([18781, 3])\n",
      "idx:  (tensor([38,  9, 21,  ...,  1,  5,  1]), Ellipsis, tensor([146, 160, 182,  ..., 156, 183, 155]), tensor([234, 116,   9,  ..., 211, 237, 197]))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_22.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_mm_data(temp, class_names=CLASS_NAMES, class_colors=CLASS_COLORS, error_color=(0, 0, 0), front='y', back='m2f_pred_mask', figsize=1000, pointsize=3, voxel=0.03, show_2d=True, alpha=0.3)\n",
    "\n",
    "# if it gives NotImplementedError in multimodal_data.py, please retain original features in data.data.x \n",
    "# inside the dataset __getitem__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # exact splatting\n",
    "# i_room = 1\n",
    "\n",
    "# # Pick a room in the Train set\n",
    "# mm_data = sample_real_data(dataset.train_dataset, idx=i_room, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
    "\n",
    "# # Pick a room in the Val set\n",
    "# # mm_data = sample_real_data(dataset.val_dataset, idx=i_room, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
    "\n",
    "# mm_data\n",
    "\n",
    "# # 2.6422173976898193 previous for_1 loop\n",
    "# # 3.252363681793213 for1 with random sample\n",
    "# # 2.0284531116485596 for1 with numpy choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeScores(data, preds, dataset, stage='train', tracker=None, reset=False):\n",
    "    if len(preds.shape) < 2:\n",
    "        preds = torch.nn.functional.one_hot(preds, num_classes=20)\n",
    "\n",
    "    if tracker is None:\n",
    "        tracker = SegmentationTracker(dataset, stage=stage, wandb_log=False, use_tensorboard=False, ignore_label=-1)\n",
    "    \n",
    "    if reset is True:\n",
    "        tracker.reset()\n",
    "    \n",
    "    tracker._compute_metrics(outputs=preds, labels=data.data.y)\n",
    "    \n",
    "    return tracker.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_m2f_features(data, n_views):\n",
    "# #     n_views = 9\n",
    "\n",
    "# #     ### Make a 3D point cloud populated by mode M2F labels!\n",
    "# #     m2f_mapped_feats = data.modalities['image'].get_mapped_m2f_features(interpolate=True)[0]\n",
    "# #     csr_idx = data.modalities['image'].view_cat_csr_indexing\n",
    "\n",
    "# #     # Calculate amount of empty views. There should be n_points * n_views filled view conditions in total.\n",
    "# #     n_seen = csr_idx[1:] - csr_idx[:-1]\n",
    "# #     unfilled_points = n_seen[n_seen < n_views]\n",
    "# #     n_views_to_fill = int(len(unfilled_points) * n_views - sum(unfilled_points))\n",
    "\n",
    "# #     random_m2f_preds = m2f_mapped_feats[np.random.choice(range(len(m2f_mapped_feats)), size=n_views_to_fill, replace=True)]\n",
    "\n",
    "\n",
    "# #     combined_m2f_tensor = torch.cat((m2f_mapped_feats, random_m2f_preds), dim=0)\n",
    "\n",
    "# #     unused_invalid_view_idx = len(m2f_mapped_feats)\n",
    "# #     combined_idx = []\n",
    "# #     for i, n in enumerate(n_seen):\n",
    "# #         if n < n_views:\n",
    "# #             n_empty_views = n_views -  n\n",
    "# #             combined_idx += list(range(csr_idx[i], csr_idx[i+1])) + \\\n",
    "# #                             list(range(unused_invalid_view_idx, unused_invalid_view_idx + n_empty_views))\n",
    "# #             unused_invalid_view_idx += n_empty_views\n",
    "# #         elif n > n_views:\n",
    "# #             sampled_idx = sorted(np.random.choice(range(csr_idx[i], csr_idx[i+1]), size=n_views, replace=False))\n",
    "# #             combined_idx += sampled_idx\n",
    "# #         else:\n",
    "# #             combined_idx += list(range(csr_idx[i], csr_idx[i+1]))\n",
    "\n",
    "# #     # re-index tensor for MVFusion format\n",
    "# #     combined_m2f_tensor = combined_m2f_tensor[combined_idx].reshape(data.num_points, n_views)\n",
    "    \n",
    "# #     return combined_m2f_tensor\n",
    "\n",
    "\n",
    "#     pixel_validity = data.data.x[:, 0, 0]\n",
    "#     print(pixel_validity, pixel_validity.shape)\n",
    "    \n",
    "    \n",
    "#     # same for m2f feats\n",
    "#     out = data.data.x\n",
    "#     print(out, out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_mode_pred(data):\n",
    "    pixel_validity = data.data.x[:, :, 0].bool()\n",
    "    mv_preds = data.data.x[:, :, -1].long()\n",
    "        \n",
    "    n_views = 9\n",
    "    \n",
    "    valid_m2f_feats = []\n",
    "    for i in range(len(mv_preds)):\n",
    "        valid_m2f_feats.append(mv_preds[i][pixel_validity[i]])\n",
    "\n",
    "    mode_preds = []\n",
    "    for m2feats_of_seen_point in valid_m2f_feats:\n",
    "        mode_preds.append(torch.mode(m2feats_of_seen_point.squeeze(), dim=0)[0])\n",
    "    mode_preds = torch.stack(mode_preds, dim=0)\n",
    "        \n",
    "    return mode_preds\n",
    "\n",
    "def eval_small_dataset(stage='train', eval_unseen=False, exclude_2d=None, exclude_3d=None):\n",
    "    # train, val\n",
    "\n",
    "    tracker = SegmentationTracker(dataset, stage=stage, wandb_log=False, use_tensorboard=False, ignore_label=-1)\n",
    "\n",
    "    for i_room in range(3):\n",
    "        if stage == 'train':\n",
    "#             data = dataset.train_dataset[i_room]\n",
    "            data = sample_real_data(dataset.train_dataset, idx=i_room, exclude_3d=exclude_3d, exclude_2d=exclude_2d)\n",
    "        elif stage == 'val':\n",
    "            data = dataset.val_dataset[i_room]\n",
    "        \n",
    "#         if not eval_unseen:\n",
    "#             # Take subset of seen points\n",
    "#             images = data.modalities['image']\n",
    "#             dense_idx_list = [\n",
    "#                         torch.arange(im.num_points, device=images.device).repeat_interleave(\n",
    "#                             im.view_csr_indexing[1:] - im.view_csr_indexing[:-1])\n",
    "#                         for im in images]\n",
    "#             data = data[dense_idx_list[0].unique()]\n",
    "\n",
    "#         # Remove points with ignore label for visual clearance\n",
    "#         data = data[data.data.y != -1]\n",
    "\n",
    "        # Get M2F features \n",
    "        # Semantic predictions as mode of multi-view predictions\n",
    "        mode_preds = get_mode_pred(data)\n",
    "\n",
    "#         print(\"M2F preds: \", mode_preds.unique())\n",
    "#         print(\"GT labels: \", data.data.y.unique()[1:])\n",
    "        \n",
    "        if eval_unseen:\n",
    "            seen_mask = csr_idx[1:] > csr_idx[:-1]\n",
    "                        \n",
    "            xyz_query_keops = LazyTensor(data.pos[~seen_mask][:, None, :])\n",
    "            xyz_search_keops = LazyTensor(data.pos[seen_mask][None, :, :])\n",
    "            d_keops = ((xyz_query_keops - xyz_search_keops) ** 2).sum(dim=2)\n",
    "            nn_idx = d_keops.argmin(dim=1)\n",
    "            del xyz_query_keops, xyz_search_keops, d_keops\n",
    "\n",
    "            temp = torch.zeros(seen_mask.shape)\n",
    "            temp[seen_mask] = mode_preds\n",
    "            \n",
    "            temp[~seen_mask] = temp[seen_mask][nn_idx].squeeze()\n",
    "            mode_preds = temp\n",
    "                \n",
    "\n",
    "        # Calculate scores\n",
    "        scores = computeScores(data, mode_preds, dataset, stage=stage, tracker=tracker, reset=False)\n",
    "        return scores[f'{stage}_miou']\n",
    "        \n",
    "#         if i_room == 1:\n",
    "#             data.data.pred = mode_preds\n",
    "#             data.data.x = None\n",
    "#             visualize_mm_data(data, class_names=CLASS_NAMES, class_colors=CLASS_COLORS, error_color=(0, 0, 0), front='y', back='m2f_pred_mask', figsize=1000, pointsize=3, voxel=0.05, show_2d=True, alpha=0.3)\n",
    "     \n",
    "# s = []\n",
    "# for i in range(5):\n",
    "#     s.append(eval_small_dataset(stage='train', eval_unseen=False))\n",
    "    \n",
    "# print('avg miou over 5 loops: ', sum(s) / len(s))\n",
    "\n",
    "# train set when invalid points are removed\n",
    "#{'train_acc': 91.86458520553573, 'train_macc': 85.37448181167012, 'train_miou': 67.662546940765}\n",
    "#{'train_acc': 91.77955465848957, 'train_macc': 84.59272635063425, 'train_miou': 63.49855216414911}\n",
    "#{'train_acc': 91.74896300196484, 'train_macc': 84.32568566228107, 'train_miou': 66.4833291671551}\n",
    "# avg miou over 10 loops:  62.50393662841291\n",
    "\n",
    "\n",
    "\n",
    "# train set with invalid points\n",
    "#{'train_acc': 92.08282038361197, 'train_macc': 85.37962358651747, 'train_miou': 68.2135868206652}\n",
    "#{'train_acc': 91.5983554227636, 'train_macc': 85.08323064550525, 'train_miou': 67.1099501308721}\n",
    "#{'train_acc': 92.09589199021353, 'train_macc': 86.20414449812343, 'train_miou': 68.7643864009719}\n",
    "#{'train_acc': 91.98125655730847, 'train_macc': 85.79685840582141, 'train_miou': 68.38436115692215}\n",
    "# avg miou over 10 loops:  63.550427023383996\n",
    "\n",
    "\n",
    "# val set\n",
    "# avg miou over 10 loops:  56.41112108349539\n",
    "\n",
    "# avg miou over 10 loops:  54.78561234690687\n",
    "\n",
    "\n",
    "# train: keeping invalid points\n",
    "# 3 seeds -> \n",
    "\n",
    "# val\n",
    "# avg miou over 3 loops:  57.956132626631245\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations_3d = [\n",
    "    ElasticDistortion, Random3AxisRotation, RandomNoise, RandomRotate, \n",
    "    RandomScaleAnisotropic, RandomSymmetry, ShiftVoxels]\n",
    "\n",
    "exclude_3d_viz = augmentations_3d + [AddFeatsByKeys, Center, GridSampling3D]\n",
    "# exclude_3d_viz = None\n",
    "\n",
    "\n",
    "\n",
    "augmentations_2d = [JitterMappingFeatures, ColorJitter, RandomHorizontalFlip]\n",
    "exclude_2d_viz = augmentations_2d + [ToFloatImage, Normalize]\n",
    "# exclude_2d_viz = None #[RandomHorizontalFlip]\n",
    "\n",
    "\n",
    "# s = []\n",
    "# for i in range(5):\n",
    "#     s.append(eval_small_dataset(stage='train', eval_unseen=False, \n",
    "#                                 exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz))\n",
    "\n",
    "# print()\n",
    "# print('avg miou over 5 loops: ', sum(s) / len(s))\n",
    "# print()\n",
    "\n",
    "s = []\n",
    "for i in range(3):\n",
    "    s.append(eval_small_dataset(stage='val', eval_unseen=False, \n",
    "                                exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz))\n",
    "print()\n",
    "print('avg miou over 3 loops: ', sum(s) / len(s))\n",
    "print()    \n",
    "\n",
    "# boosdoeners zijn niet: ToFloatImage, Normalize, JitterMappingFeatures, ColorJitter\n",
    "# miOu ~ 63\n",
    "# RandomHorizontalFlip -> 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms on 3D points that we need to exclude for visualization purposes\n",
    "augmentations_3d = [\n",
    "    ElasticDistortion, Random3AxisRotation, RandomNoise, RandomRotate, \n",
    "    RandomScaleAnisotropic, RandomSymmetry, ShiftVoxels]\n",
    "exclude_3d_viz = augmentations_3d + [AddFeatsByKeys, Center, GridSampling3D]\n",
    "\n",
    "# Transforms on 2D images and mappings that we need to exclude for visualization\n",
    "# purposes\n",
    "augmentations_2d = [JitterMappingFeatures, ColorJitter, RandomHorizontalFlip]\n",
    "exclude_2d_viz = augmentations_2d + [ToFloatImage, Normalize]\n",
    "\n",
    "for i in range(len(exclude_3d_viz)):\n",
    "    temp_3d = exclude_3d_viz.copy()\n",
    "    included_3d_transformation = temp_3d.pop(i)\n",
    "    \n",
    "    s = []\n",
    "    for i in range(5):\n",
    "        s.append(eval_small_dataset(stage='train', eval_unseen=False, \n",
    "                                    exclude_3d=temp_3d, exclude_2d=exclude_2d_viz))\n",
    "        \n",
    "    print()\n",
    "    print(included_3d_transformation)\n",
    "    print('avg miou over 5 loops: ', sum(s) / len(s))\n",
    "    print()\n",
    "        \n",
    "for i in range(len(exclude_2d_viz)):\n",
    "    temp_2d = exclude_2d_viz.copy()\n",
    "    included_2d_transformation = temp_2d.pop(i)\n",
    "    \n",
    "    s = []\n",
    "    for i in range(5):\n",
    "        s.append(eval_small_dataset(stage='train', eval_unseen=False, \n",
    "                                    exclude_3d=exclude_3d_viz, exclude_2d=temp_2d))\n",
    "    print()\n",
    "    print(included_2d_transformation)\n",
    "    print('avg miou over 5 loops: ', sum(s) / len(s))\n",
    "    print()    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = mm_data.modalities['image']\n",
    "# Take subset of only seen points\n",
    "# NOTE: each point is contained multiple times if it has multiple correspondences\n",
    "dense_idx_list = [\n",
    "            torch.arange(im.num_points, device=images.device).repeat_interleave(\n",
    "                im.view_csr_indexing[1:] - im.view_csr_indexing[:-1])\n",
    "            for im in images]\n",
    "# take subset of only seen points without re-indexing the same point\n",
    "data = mm_data[dense_idx_list[0].unique()]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_mm_data(data, class_names=CLASS_NAMES, class_colors=CLASS_COLORS, front='y', back='m2f_pred_mask', figsize=1000, pointsize=3, voxel=0.05, show_2d=True, alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per scene evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### M2F mode pred scores\n",
    "mode_preds = torch.mode(combined_m2f_tensor, dim=-1)[0].long()\n",
    "\n",
    "\n",
    "# mode_preds[mode_preds == 4] = 0\n",
    "# data.data.y[0] = 4\n",
    "\n",
    "\n",
    "print(mode_preds.unique())\n",
    "print(data.data.y.unique()[1:])\n",
    "print(CLASS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mode_preds.unique(return_counts=True))\n",
    "\n",
    "\n",
    "### Check if bookshelf preds are still in preds after filtering out ignore_label from pred and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined tracker\n",
    "computeScores(data, mode_preds, dataset, stage='train', tracker=tracker, reset=True)\n",
    "\n",
    "# \"\"\"\n",
    "# {'train_acc': 84.92801236834477,\n",
    "#  'train_macc': 72.13466411119538,\n",
    "#  'train_miou': 57.735325170792414}\n",
    "#  \"\"\"\n",
    "\n",
    "\n",
    "# Score with chair prediction, scene0000_00\n",
    "# {'train_acc': 78.26635359866494,\n",
    "#  'train_macc': 68.09951978180109,\n",
    "#  'train_miou': 53.20302327275945}\n",
    "\n",
    "# without chair pred\n",
    "# {'train_acc': 78.26635359866494,\n",
    "#  'train_macc': 68.09951978180109,\n",
    "#  'train_miou': 55.494691728583014}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100 * tracker._confusion_matrix.get_average_intersection_union(missing_as_one=False))\n",
    "print(tracker._confusion_matrix.get_intersection_union_per_class())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([6.37603604e-01, 7.16119607e-01, 6.55039517e-01, 7.14544805e-01,\n",
    "       1.00000000e-08, 8.21926115e-01, 3.66729689e-01, 6.36812422e-01,\n",
    "       2.43762006e-01,   2.46813451e-01,\n",
    "       4.66349027e-01, 7.12784598e-01, 7.48108936e-01, \n",
    "       4.62686577e-01, 6.39296198e-01, 4.37693110e-01]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_confusion_matrix(cm, path2save=None, ordered_names=None):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    sns.set(font_scale=5)\n",
    "    \n",
    "#     template_path = os.path.join(path2save, \"{}.svg\")\n",
    "    # PRECISION\n",
    "    cmn = cm.astype(\"float\") / cm.sum(axis=-1)[:, np.newaxis]\n",
    "    cmn[np.isnan(cmn) | np.isinf(cmn)] = 0\n",
    "    fig, ax = plt.subplots(figsize=(31, 31))\n",
    "    sns.heatmap(\n",
    "        cmn, annot=True, fmt=\".2f\", xticklabels=ordered_names, yticklabels=ordered_names, annot_kws={\"size\": 20}\n",
    "    )\n",
    "    # g.set_xticklabels(g.get_xticklabels(), rotation = 35, fontsize = 20)\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.show()\n",
    "    \n",
    "#     path_precision = template_path.format(\"precision\")\n",
    "#     plt.savefig(path_precision, format=\"svg\")\n",
    "\n",
    "    # RECALL\n",
    "    cmn = cm.astype(\"float\") / cm.sum(axis=0)[np.newaxis, :]\n",
    "    cmn[np.isnan(cmn) | np.isinf(cmn)] = 0\n",
    "    fig, ax = plt.subplots(figsize=(31, 31))\n",
    "    sns.heatmap(\n",
    "        cmn, annot=True, fmt=\".2f\", xticklabels=ordered_names, yticklabels=ordered_names, annot_kws={\"size\": 20}\n",
    "    )\n",
    "    # g.set_xticklabels(g.get_xticklabels(), rotation = 35, fontsize = 20)\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.show()\n",
    "    \n",
    "#     path_recall = template_path.format(\"recall\")\n",
    "    #plt.savefig(path_recall, format=\"svg\")\n",
    "\n",
    "\n",
    "save_confusion_matrix(tracker._confusion_matrix.confusion_matrix, ordered_names=CLASS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scene 0000_00\n",
    "computeScores(data, mode_preds, dataset, stage='train', tracker=None)\n",
    "\n",
    "{'train_acc': 78.5082992024143,\n",
    " 'train_macc': 68.44232036545344,\n",
    " 'train_miou': 53.298953827959814}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scene0190_0\n",
    "computeScores(data, mode_preds, dataset, stage='train', tracker=None)\\\n",
    "\n",
    "{'train_acc': 88.12224636669923,\n",
    " 'train_macc': 66.77507086551887,\n",
    " 'train_miou': 41.859285338265536}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scene0190_1\n",
    "computeScores(data, mode_preds, dataset, stage='train', tracker=None)\n",
    "\n",
    "{'train_acc': 88.02479415302064,\n",
    " 'train_macc': 75.62845810740538,\n",
    " 'train_miou': 46.844401523580046}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process MMData to compatible input for MVFusion model\n",
    "\n",
    "#### Step by step process:\n",
    "1. ~Take the MMData object from the dataloader~\n",
    "2. ~Find out how the mappings are stored and accessed. Hint: check 'SelectMappingFromPointId' and 'PickImagesFromMemoryCredit'~\n",
    "3. Select 9 random views for each 3d point, while maximizing differences between camera positions. I have chosen to not use extrinsics as I hypothesize that there already will be large differences in poses when camera position is different.\n",
    "4. Get M2F labels for each view\n",
    "5. Add the missing features to each viewing condition vector\n",
    "6. Input the prost-processed sample to MVFusion model\n",
    "\n",
    "\n",
    "Random thoughts:\n",
    "- Adding noise to viewing conditions \n",
    "- 3. has to be vectorized (not for looping through n_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3\n",
    "# most_seen_point = dense_idx_list[0].unique(return_counts=True)[1].argmax()\n",
    "\n",
    "# mm_data = seen_mm_data#[[0, most_seen_point, 1, 2]]\n",
    "# image_data = mm_data.modalities['image']\n",
    "# samesetting_data = image_data[0]   # take first in SameSettingImageData since ScanNet only has 1 setting\n",
    "# print(samesetting_data)\n",
    "\n",
    "# print(samesetting_data.__dict__.keys())\n",
    "\n",
    "# # camera positions\n",
    "# print(samesetting_data.pos)\n",
    "\n",
    "# print(samesetting_data.mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to group each point's views?\n",
    "print(image_data.view_cat_csr_indexing)\n",
    "print(samesetting_data.view_csr_indexing)\n",
    "\n",
    "csr_idx = image_data.view_cat_csr_indexing\n",
    "\n",
    "# Compute dense indices from CSR indices\n",
    "n_groups = csr_idx.shape[0] - 1\n",
    "dense_idx = torch.arange(n_groups).to(csr_idx.device).repeat_interleave(\n",
    "    csr_idx[1:] - csr_idx[:-1])\n",
    "# if src.dim() > 1:\n",
    "#     dense_idx = dense_idx.view(-1, 1).repeat(1, src.shape[1])\n",
    "    \n",
    "print(dense_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data.num_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# s = time.time()\n",
    "\n",
    "# def extract_viewing_data_per_point(mm_data):\n",
    "#     n_views = 9\n",
    "    \n",
    "    \n",
    "#     image_data = mm_data.modalities['image']\n",
    "#     csr_idx = image_data.view_cat_csr_indexing\n",
    "\n",
    "    \n",
    "# #     # Compute dense indices from CSR indices\n",
    "# #     n_groups = csr_idx.shape[0] - 1\n",
    "# #     dense_idx = torch.arange(n_groups).to(csr_idx.device).repeat_interleave(\n",
    "# #         csr_idx[1:] - csr_idx[:-1])\n",
    "    \n",
    "# #     print(dense_idx)\n",
    "    \n",
    "# #     # batch_idx: to which image a view-condition feature belongs\n",
    "# #     # CSRData: n/a\n",
    "# #     # viewing_conditions: feature vector of an image-pixel match\n",
    "# #     for x in image_data[0].mappings.values:\n",
    "# #         print(x)\n",
    "        \n",
    "#     viewing_conditions = image_data[0].mappings.values[2]\n",
    "    \n",
    "#     # Add pixel validity as first feature!\n",
    "#     viewing_conditions = torch.cat((torch.ones(viewing_conditions.shape[0], 1).to(viewing_conditions.device),\n",
    "#                                     viewing_conditions), dim=1)\n",
    "    \n",
    "    \n",
    "#     # Calculate amount of empty views. There should be n_points * n_views filled view conditions in total.\n",
    "#     n_seen = csr_idx[1:] - csr_idx[:-1]\n",
    "    \n",
    "#     unfilled_points = n_seen[n_seen < n_views]\n",
    "#     n_views_to_fill = int(len(unfilled_points) * n_views - sum(unfilled_points))\n",
    "    \n",
    "#     random_invalid_views = viewing_conditions[np.random.choice(range(len(viewing_conditions)), size=n_views_to_fill, replace=True)]\n",
    "#     # set pixel validity to invalid\n",
    "#     random_invalid_views[:, 0] = 0\n",
    "    \n",
    "    \n",
    "#     # faster method: conccaat viewing conditions and random invalid views, then index the tensor such that each point\n",
    "#     # either has 9 valid subsampled views, or is filled to 9 views with random views\n",
    "#     combined_tensor = torch.cat((viewing_conditions, random_invalid_views), dim=0)\n",
    "    \n",
    "#     unused_invalid_view_idx = len(viewing_conditions)\n",
    "#     combined_idx = []\n",
    "#     for i, n in enumerate(n_seen):\n",
    "#         if n < n_views:\n",
    "#             n_empty_views = n_views -  n\n",
    "#             combined_idx += list(range(csr_idx[i], csr_idx[i+1])) + \\\n",
    "#                             list(range(unused_invalid_view_idx, unused_invalid_view_idx + n_empty_views))\n",
    "#             unused_invalid_view_idx += n_empty_views\n",
    "#         elif n > n_views:\n",
    "#             sampled_idx = sorted(np.random.choice(range(csr_idx[i], csr_idx[i+1]), size=n_views, replace=False))\n",
    "#             combined_idx += sampled_idx\n",
    "            \n",
    "#         else:\n",
    "#             combined_idx += list(range(csr_idx[i], csr_idx[i+1]))\n",
    "    \n",
    "#     combined_tensor = combined_tensor[combined_idx]    \n",
    "#     return combined_tensor.reshape(mm_data.num_points, n_views, -1)\n",
    "        \n",
    "# extract_viewing_data_per_point(mm_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Redundant code\n",
    "\n",
    "# s = time.time()\n",
    "# ### view sampling per point\n",
    "# view_data_per_point, random_invalid_views = extract_viewing_data_per_point(mm_data)\n",
    "\n",
    "# cum_sampled_views = 0\n",
    "# for i in range(len(view_data_per_point)):\n",
    "#     if len(view_data_per_point[i]) > 9:\n",
    "#         sampled_idx = sorted(np.random.choice(range(len(view_data_per_point[i])), size=9, replace=False))\n",
    "#         view_data_per_point[i] = view_data_per_point[i][sampled_idx]\n",
    "#     else:\n",
    "#         # Each point should have 9 views, so fill points with random invalid view conditions till it contains 9 views\n",
    "#         n_views = len(view_data_per_point[i])\n",
    "#         n_empty = 9 - n_views\n",
    "#         view_data_per_point[i] = torch.cat((view_data_per_point[i], \n",
    "#                                             random_invalid_views[cum_sampled_views:cum_sampled_views+n_empty]), dim=0)\n",
    "#         cum_sampled_views += n_empty\n",
    "                \n",
    "# print(time.time() - s)\n",
    "\n",
    "# # view_data_per_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# idx mapping from each pixel to point\n",
    "im_data = test_data.modalities['image']    # need SameSettingImageData object so no [0] slicing\n",
    "\n",
    "dense_idx_list = [\n",
    "            torch.arange(im.num_points, device=im_data.device).repeat_interleave(\n",
    "                im.view_csr_indexing[1:] - im.view_csr_indexing[:-1])\n",
    "            for im in im_data]\n",
    "dense_idx_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to point-image & point-pixel mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = mm_data.modalities['image'][0].mappings\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping.__dict__\n",
    "# pointers: csr_idx, aka which tensors belong to the same view\n",
    "# values[0]: indicates the image idx\n",
    "# values[1]: CSRData holding pixel indices, called with values[1].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image idx of each point-pixel match\n",
    "idx_batch = mapping.values[0].repeat_interleave(\n",
    "    mapping.values[1].pointers[1:] - mapping.values[1].pointers[:-1])\n",
    "idx_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx mapping from each pixel to point\n",
    "im_data = mm_data.modalities['image']\n",
    "\n",
    "dense_idx_list = [\n",
    "            torch.arange(im.num_points, device=im_data.device).repeat_interleave(\n",
    "                im.view_csr_indexing[1:] - im.view_csr_indexing[:-1])\n",
    "            for im in im_data]\n",
    "dense_idx_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_idx_list[0].unique(return_counts=True)[1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data.modalities['image'].get_mapped_features(interpolate=False)\n",
    "\n",
    "def get_mapped_features(mod_data, interpolate=False):\n",
    "    \"\"\"Return the mapped features, with optional interpolation. If\n",
    "    `interpolate=False`, the mappings will be adjusted to\n",
    "    `self.img_size`: the current size of the feature map `self.x`.\n",
    "    \"\"\"\n",
    "    # Compute the feature map's sampling ratio between the input\n",
    "    # `mapping_size` and the current `img_size`\n",
    "    # TODO: treat scales independently. Careful with min or max\n",
    "    #  depending on upscale and downscale\n",
    "    scale = 1 / mod_data.downscale\n",
    "\n",
    "    # If not interpolating, set the mapping to the proper scale\n",
    "    mappings = mod_data.mappings if interpolate \\\n",
    "        else mod_data.mappings.rescale_images(scale)\n",
    "\n",
    "    # Index the features with/without interpolation\n",
    "    if interpolate and scale != 1:\n",
    "        print(\"BAAAKA\")\n",
    "        resolution = torch.Tensor([mod_data.mapping_size]).to(mod_data.device)\n",
    "        coords = mappings.pixels / (resolution - 1)\n",
    "        coords = coords[:, [1, 0]]  # pixel mappings are in (W, H) format\n",
    "        batch = mappings.feature_map_indexing[0]\n",
    "        x = sparse_interpolation(mod_data.x, coords, batch)\n",
    "    else:\n",
    "        x = mod_data.x[mappings.feature_map_indexing]\n",
    "\n",
    "    return x\n",
    "    \n",
    "get_mapped_features(mm_data.modalities['image'][0], interpolate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Returns indices for extracting mapped data from a batch of image feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = mm_data.modalities['image'][0]\n",
    "print(im.feature_map_indexing)\n",
    "\n",
    "im = mm_data.modalities['image']\n",
    "print(im.feature_map_indexing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### I found out how indices are stored. \n",
    "#################### Now, check out how the ImageMapping is calculated! in MapImages transform\n",
    "\n",
    "\n",
    "def feature_map_indexing(SameSettingImageData):  # function within ImageMapping object\n",
    "    \"\"\"Return the indices for extracting mapped data from the\n",
    "    corresponding batch of image feature maps.\n",
    "\n",
    "    The batch of image feature maps X is expected to have the shape\n",
    "    `[B, C, H, W]`. The returned indexing object idx is intended to\n",
    "    be used for recovering the mapped features as: `X[idx]`.\n",
    "    \"\"\"\n",
    "    mappings = SameSettingImageData.mappings\n",
    "    \n",
    "    print(mappings.features[:10])\n",
    "    \n",
    "    idx_batch = mappings.images.repeat_interleave(\n",
    "        mappings.values[1].pointers[1:] - mappings.values[1].pointers[:-1])\n",
    "    idx_height = mappings.pixels[:, 1]\n",
    "    idx_width = mappings.pixels[:, 0]\n",
    "    idx = (idx_batch.long(), ..., idx_height.long(), idx_width.long())\n",
    "    return idx\n",
    "\n",
    "im = mm_data.modalities['image'][0]   # SameSettingImageData\n",
    "feature_map_indexing(im)\n",
    "\n",
    "mm_data.data.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = seen_mm_data.modalities['image']\n",
    "n_seen = sum([\n",
    "            im.mappings.pointers[1:] - im.mappings.pointers[:-1]\n",
    "            for im in images])\n",
    "n_seen.argmax()   # by how many views a 3d point is seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the unseen points boolean masks and split them in a\n",
    "# list of masks for easier popping\n",
    "\n",
    "# if self.use_coverage:\n",
    "def PickImages(data, images):\n",
    "    \"\"\"\n",
    "    Returns how many points each view sees\n",
    "    \"\"\"\n",
    "    img_unseen_points = torch.zeros(\n",
    "        images.num_views, data.num_nodes, dtype=torch.bool)\n",
    "    i_offset = 0\n",
    "    for im in images:\n",
    "        mappings = im.mappings\n",
    "        i_idx = mappings.images + i_offset\n",
    "        j_idx = mappings.points.repeat_interleave(\n",
    "            mappings.pointers[1:] - mappings.pointers[:-1])\n",
    "        img_unseen_points[i_idx, j_idx] = True\n",
    "        i_offset += im.num_views\n",
    "    img_unseen_points = [x.numpy() for x in img_unseen_points]\n",
    "    \n",
    "    print([img_unseen_points[x].sum() for x in range(len(img_unseen_points))])\n",
    "    \n",
    "PickImages(seen_mm_data.data, seen_mm_data.modalities['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare mapping feature statistics between train/val/test proccessed_2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mapping feature statistics:\n",
      "tensor([0.2711, 0.1401, 0.6778, 0.1822, 0.6568, 0.4669, 0.2979, 0.6933])\n",
      "tensor([0.0811, 0.1083, 0.2375, 0.1710, 0.2294, 0.2735, 0.1378, 0.2283])\n",
      "val mapping feature statistics:\n",
      "tensor([0.2745, 0.1412, 0.6740, 0.1847, 0.6517, 0.4674, 0.3046, 0.6818])\n",
      "tensor([0.0856, 0.1096, 0.2406, 0.1730, 0.2322, 0.2735, 0.1415, 0.2301])\n",
      "test mapping feature statistics:\n",
      "tensor([0.2839, 0.1362, 0.6823, 0.1815, 0.6587, 0.4749, 0.3009, 0.6832])\n",
      "tensor([0.0886, 0.1055, 0.2369, 0.1726, 0.2329, 0.2752, 0.1408, 0.2319])\n"
     ]
    }
   ],
   "source": [
    "for split in ['train', 'val', 'test']:\n",
    "    data_dir = f\"/project/fsun/dvata/scannet-neucon-smallres-m2f/processed/processed_2d_{split}\"\n",
    "\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    for name in os.listdir(data_dir):\n",
    "        file = osp.join(data_dir, name)\n",
    "\n",
    "        data = torch.load(file)\n",
    "        data = data._mappings.values[2]\n",
    "\n",
    "        # note that estimation is biased\n",
    "        mean += data.mean(axis=0)\n",
    "        std += data.std(axis=0)\n",
    "        \n",
    "    mean /= len(os.listdir(data_dir))\n",
    "    std /= len(os.listdir(data_dir))\n",
    "\n",
    "\n",
    "    print(f\"{split} mapping feature statistics:\")\n",
    "    print(mean)\n",
    "    print(std)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_m2f_masks:  True\n",
      "initialize train dataset\n",
      "initialize val dataset\n",
      "initialize test dataset\n",
      "line 720 scannet.py: split == 'test'\n",
      "Time = 4.8 sec.\n"
     ]
    }
   ],
   "source": [
    "# Set your dataset root directory, where the data was/will be downloaded\n",
    "DATA_ROOT = '/project/fsun/dvata'\n",
    "\n",
    "dataset_config = 'segmentation/multimodal/Feng/scannet-neucon-smallres-m2f-partial-subsampled'   \n",
    "models_config = 'segmentation/multimodal/Feng/mvfusion'    # model family\n",
    "model_name = 'MVFusion_3D'                       # specific model\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    f'data={dataset_config}',\n",
    "    f'models={models_config}',\n",
    "    f'model_name={model_name}',\n",
    "    f'data.dataroot={DATA_ROOT}',\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n",
    "cfg.data.load_m2f_masks = True   # load Mask2Former predicted masks\n",
    "\n",
    "# Dataset instantiation\n",
    "start = time()\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)|\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm_data_list = [dataset.test_dataset[0][i] for i in range(1)]\n",
    "# mm_data_list = [dataset.test_dataset[0][99]]\n",
    "\n",
    "index = 0\n",
    "mm_data_list = [dataset.val_dataset[index] for index in range(3)]\n",
    "\n",
    "# mm_data_list = [dataset.train_dataset[index] for index in range(3)]\n",
    "\n",
    "# mm_data_list = [dataset.test_dataset[0][index] for index in range(3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMBatch(\n",
      "    data = Batch(batch=[80077], coords=[80077, 3], grid_size=[3], id_scan=[3], mapping_index=[80077], mvfusion_input=[58962, 9, 10], origin_id=[80077], pos=[80077, 3], ptr=[4], x=[80077, 3], y=[80077])\n",
      "    image = ImageBatch(num_settings=1, num_views=300, num_points=80077, device=cpu)\n",
      ")\n",
      "tensor([-4.1350e-08,  1.1619e-07,  3.3875e-08])\n",
      "tensor([-0.0588, -0.3896,  0.7278], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# for x in mm_data_list:\n",
    "#     print(x)\n",
    "    \n",
    "batch = MMBatch.from_mm_data_list(mm_data_list)\n",
    "print(batch)\n",
    "print(batch.pos.mean(axis=0))\n",
    "print(batch.modalities['image'][0].pos.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 100, 300]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_batch = batch.clone()\n",
    "print(batch == new_batch)\n",
    "\n",
    "[0, 100, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.1350e-08,  1.1619e-07,  3.3875e-08]) tensor([-4.1350e-08,  1.1619e-07,  3.3875e-08])\n",
      "tensor([-0.5779,  0.5223,  0.2923]) tensor([-0.5779,  0.5223,  0.2923])\n"
     ]
    }
   ],
   "source": [
    "new_batch = batch.clone()\n",
    "print(new_batch.pos.mean(0), new_batch.x.mean(0))\n",
    "\n",
    "batch_size = new_batch.ptr.shape[0] - 1\n",
    "n_merge = 2\n",
    "\n",
    "new_ptr = [0]\n",
    "\n",
    "# Merge two individual point clouds to one MMData inside MMBatch. Uneven batches will leave the last sample untouched.\n",
    "# NOTE: only N_MERGE == 2 is supported.\n",
    "assert n_merge == 2\n",
    "for i in range(0, len(new_batch.ptr)-2, n_merge):\n",
    "    b1, e1 = new_batch.ptr[i], new_batch.ptr[i+1]\n",
    "    b2, e2 = new_batch.ptr[i+1], new_batch.ptr[i+2]\n",
    "    \n",
    "    coords = new_batch.pos[b2:e2]\n",
    "    r1 = coords.min(0)[0]\n",
    "    r2 = coords.max(0)[0]\n",
    "    offset = ( (r1 - r2) * torch.rand(1, 3) + r2 ) / 2\n",
    "    \n",
    "    # Slightly translate one of two point clouds\n",
    "    new_batch.pos[b2:e2] = new_batch.pos[b2:e2] + offset\n",
    "    new_batch.x[b2:e2] = new_batch.x[b2:e2] + offset\n",
    "\n",
    "    new_ptr.append(new_batch.ptr[i+2])\n",
    "    \n",
    "# Add last pointer for uneven batches\n",
    "if batch_size % n_merge == 1:\n",
    "    new_ptr.append(new_batch.ptr[-1])\n",
    "new_ptr = torch.LongTensor(new_ptr)\n",
    "\n",
    "# Update batch identifiers\n",
    "new_batch.ptr = new_ptr \n",
    "new_batch.batch = new_batch.batch // n_merge\n",
    "new_batch.data.batch = new_batch.batch\n",
    "\n",
    "# What is origin_id and do we need to adjust it when merging batches?\n",
    "print(new_batch.pos.mean(0), new_batch.x.mean(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_27.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_mm_data(new_batch, figsize=1000, pointsize=3, voxel=0.1, show_2d=False, back='m2f_pred_mask', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9455,  0.3256,  0.0000, -5.3844],\n",
       "        [-0.3256,  0.9455,  0.0000, -2.8718],\n",
       "        [ 0.0000,  0.0000,  1.0000, -0.0644],\n",
       "        [ 0.0000,  0.0000,  0.0000,  1.0000]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_axis_align_matrix(filename):\n",
    "    lines = open(filename).readlines()\n",
    "    axis_align_matrix = None\n",
    "    for line in lines:\n",
    "        if \"axisAlignment\" in line:\n",
    "            axis_align_matrix = torch.Tensor([float(x) for x in line.rstrip().strip(\"axisAlignment = \").split(\" \")]).reshape((4, 4))\n",
    "            break\n",
    "    return axis_align_matrix\n",
    "\n",
    "path = '/project/fsun/data/scannet/scans'\n",
    "scene_id = dataset.val_dataset.MAPPING_IDX_TO_SCAN_TRAIN_NAMES[index]\n",
    "path = osp.join(path, scene_id, scene_id + '.txt')\n",
    "matrix = read_axis_align_matrix(path)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.4552e-01,  3.2557e-01, -4.6566e-10,  1.8626e-09],\n",
      "        [-3.2557e-01,  9.4552e-01,  0.0000e+00,  3.7253e-09],\n",
      "        [ 0.0000e+00, -0.0000e+00,  1.0000e+00,  0.0000e+00],\n",
      "        [ 4.1561e+00,  4.4683e+00,  6.4350e-02,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "inv = torch.linalg.inv(matrix.T)\n",
    "print(inv)\n",
    "\n",
    "# inv2 = torch.linalg.inv(matrix[:3, :3])\n",
    "# inv2_t = (-inv2 @ matrix[:3, 3])\n",
    "\n",
    "# inv_final = torch.zeros((4, 4))\n",
    "# inv_final[:3, :3] = inv2\n",
    "# inv_final[:3, 3] = inv2_t\n",
    "# inv_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26715, 3])\n"
     ]
    }
   ],
   "source": [
    "batch.pos = torch.concat((batch.pos, torch.ones((len(batch.pos), 1))), axis=-1) @ inv\n",
    "batch.pos = batch.pos[:, :3]\n",
    "\n",
    "print(batch.pos.shape)\n",
    "\n",
    "# y = x @ M^T\n",
    "# y @ M = x @ (M^T @ M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.data.pos = batch.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reverses the inverse\n",
    "# batch.data.pos = torch.concat((batch.pos, torch.ones((len(batch.pos), 1))), axis=-1) @ matrix.T\n",
    "# batch.data.pos = batch.data.pos[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mm_data(batch, figsize=1000, pointsize=3, voxel=0.1, show_2d=False, back='m2f_pred_mask', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference from pretrained weights and visualize predictions\n",
    "It is possible to visualize the pointwise predictions and errors from a model. \n",
    "\n",
    "To do so, we will use the pretrained weights made available with this project. See `README.md` to get the download links and manually place the `.pt` files locally. You will need to provide `checkpoint_dir` where you saved those files in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.models.model_factory import instantiate_model\n",
    "\n",
    "# Set your parameters\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/2022-10-31/19-54-07'  # MVFusion without 3d, fully trained on 30k seen points per pcd\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/2022-11-09/12-51-37'\n",
    "checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/2022-11-11/17-18-09' # MVFusion_3d 0.1 LR, 0.03 vox, miou 74.7\n",
    "\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/2022-11-04/15-51-33' # 3D Backbone, 68.04 miou\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/model_checkpoints' # DVA best model\n",
    "\n",
    "# Create the model\n",
    "print(f\"Creating model: {cfg.model_name}\")\n",
    "model = instantiate_model(cfg, dataset)\n",
    "# print(model)\n",
    "\n",
    "# Load the checkpoint and recover the 'best_miou' model weights\n",
    "checkpoint = torch.load(f'{checkpoint_dir}/{model_name}.pt', map_location='cpu')\n",
    "model.load_state_dict_with_same_shape(checkpoint['models']['latest'], strict=False)\n",
    "\n",
    "# Prepare the model for inference\n",
    "model = model.eval().cuda()\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have loaded the model, we need to run a forward pass on a sample. Howver, if we want to be able to visualize the predictions, we need to pay special attention to which type of 3D and 2D transforms we apply on the data if we do not want to break the mappings. To do so, we will manually apply some sensitive transforms to be able to both infer on the data and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_room = 0\n",
    "\n",
    "# Pick a room in the Train set\n",
    "# mm_data = sample_real_data(dataset.train_dataset, idx=i_room, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
    "\n",
    "# Pick a room in the Val set\n",
    "mm_data = sample_real_data(dataset.val_dataset, idx=i_room, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
    "\n",
    "# Pick a room in the Test set\n",
    "# mm_data = sample_real_data(dataset.test_dataset[0], idx=i_room, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
    "\n",
    "# Extract point cloud and images from MMData object\n",
    "data = mm_data.data.clone()\n",
    "images = mm_data.modalities['image'].clone()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell for validation sample with original validation transformations\n",
    "mm_data = dataset.val_dataset[0]\n",
    "\n",
    "# data = mm_data.data.clone()\n",
    "# images = mm_data.modalities['image'].clone()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMData(\n",
       "    data = Data(coords=[97257, 3], grid_size=[1], id_scan=[1], mapping_index=[97257], mvfusion_input=[70209, 9, 10], origin_id=[97257], pos=[97257, 3], x=[97257, 3], y=[97257])\n",
       "    image = ImageData(num_settings=1, num_views=100, num_points=97257, device=cpu)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([237360])\n",
      "tensor(157279)\n"
     ]
    }
   ],
   "source": [
    "### Select seen points\n",
    "# csr_idx = mm_data.modalities['image'][0].view_csr_indexing\n",
    "# n_seen = csr_idx[1:] - csr_idx[:-1]\n",
    "# seen_mask = ( n_seen > 0 )\n",
    "# print(seen_mask.shape)\n",
    "# print(seen_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33260]) tensor(17904)\n"
     ]
    }
   ],
   "source": [
    "# # select only first N points\n",
    "# N = 33260\n",
    "# mm_data = mm_data[:N]\n",
    "# csr_idx = mm_data.modalities['image'][0].view_csr_indexing\n",
    "# n_seen = csr_idx[1:] - csr_idx[:-1]\n",
    "# seen_mask = ( n_seen > 0 )\n",
    "\n",
    "# print(seen_mask.shape, seen_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(id_scan=[1], linearity=[33260], mapping_index=[33260], mvfusion_input=[17904, 9, 10], norm=[33260, 3], origin_id=[33260], planarity=[33260], pos=[33260, 3], pos_x=[33260], pos_y=[33260], pos_z=[33260], rgb=[33260, 3], scattering=[33260], y=[33260])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Run block to grab only seen points\n",
    "\n",
    "# # csr_idx = images[0].view_csr_indexing\n",
    "# # dense_idx_list = torch.arange(images.num_points).repeat_interleave(csr_idx[1:] - csr_idx[:-1])\n",
    "# # # take subset of only seen points without re-indexing the same point\n",
    "# # mm_data = mm_data[dense_idx_list.unique()]\n",
    "\n",
    "\n",
    "\n",
    "# n_seen_points = seen_mask.sum()\n",
    "\n",
    "# mm_data.mvfusion_input = mm_data.mvfusion_input[:n_seen_points]\n",
    "# mm_data.data.mvfusion_input = mm_data.data.mvfusion_input[:n_seen_points]\n",
    "\n",
    "# # Extract point cloud and images from MMData object\n",
    "# data = mm_data.data.clone()\n",
    "# images = mm_data.modalities['image'].clone()\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMData(\n",
      "    data = Data(coords=[96829, 3], grid_size=[1], id_scan=[1], linearity=[96829], mapping_index=[96829], mvfusion_input=[155050, 9, 10], norm=[96829, 3], origin_id=[96829], planarity=[96829], pos=[96829, 3], rgb=[96829, 3], scattering=[96829], x=[96829, 3], y=[96829])\n",
      "    image = ImageData(num_settings=1, num_views=100, num_points=96829, device=cpu)\n",
      ")\n",
      "input batch:  MMBatch(\n",
      "    data = Batch(batch=[96829], coords=[96829, 3], grid_size=[1], id_scan=[1], linearity=[96829], mapping_index=[96829], mvfusion_input=[155050, 9, 10], norm=[96829, 3], origin_id=[96829], planarity=[96829], pos=[96829, 3], ptr=[2], rgb=[96829, 3], scattering=[96829], x=[96829, 3], y=[96829])\n",
      "    image = ImageBatch(num_settings=1, num_views=100, num_points=96829, device=cpu)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# For voxel-based 3D backbones such as SparseConv3d and MinkowskiNet, points need to be \n",
    "# preprocessed with Center and GridSampling3D. Unfortunately, Center breaks relative \n",
    "# positions between points and images. Besides, the combination of Center and GridSampling3D\n",
    "# may lead to some points being merged into the same voxels, so we must apply it to both the\n",
    "# inference and visualization data to make sure we have the same voxels. The workaround here \n",
    "# is to manually run these while keeping track of the centering offset\n",
    "center = data.pos.mean(dim=-2, keepdim=True)\n",
    "data = AddFeatsByKeys(list_add_to_x=[True, True, True], feat_names=['pos_x', 'pos_y', 'pos_z'], delete_feats=[True, True, True])(data)          # add z-height to the features\n",
    "data = Center()(data)                                                                                 # mean-center the data\n",
    "data = GridSampling3D(cfg.data.resolution_3d, quantize_coords=True, mode='last')(data)                # quantization for volumetric models\n",
    "\n",
    "# This last voxelization step with GridSampling3D might have removed some points, so we need\n",
    "# to update the mappings usign SelectMappingFromPointId. To control the size of the batch, we\n",
    "# use PickImagesFromMemoryCredit. Besides, 2D models expect normalized float images, which is\n",
    "# why we call ToFloatImage and Normalize\n",
    "data, images = SelectMappingFromPointId()(data, images)                                               # update mappings after GridSampling3D\n",
    "data, images = PickImagesFromMemoryCredit(\n",
    "    img_size=cfg.data.resolution_2d, \n",
    "    k_coverage=cfg.data.multimodal.settings.k_coverage, \n",
    "    n_img=cfg.data.multimodal.settings.test_pixel_credit)(data, images)                                      # select images to respect memory constraints\n",
    "data, images_infer = ToFloatImage()(data, images.clone())                                             # convert uint8 images to float\n",
    "data, images_infer = Normalize()(data, images_infer)                                                  # RGB normalization\n",
    "\n",
    "# Create a MMData for inference\n",
    "mm_data_infer = MMData(data, image=images_infer)\n",
    "print(mm_data_infer)\n",
    "\n",
    "# Create a MMBatch and run inference\n",
    "batch = MMBatch.from_mm_data_list([mm_data_infer])\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"input batch: \", batch)\n",
    "    model.set_input(batch, model.device)\n",
    "    model(batch)\n",
    "\n",
    "# Create a MMData for visualization\n",
    "data.pos += center\n",
    "mm_data = MMData(data, image=images)\n",
    "\n",
    "# Recover the predicted labels for visualization\n",
    "mm_data.data.pred = model.output.detach().cpu().argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch:  MMBatch(\n",
      "    data = Batch(batch=[55011], coords=[55011, 3], grid_size=[1], id_scan=[1], mapping_index=[55011], mvfusion_input=[39801, 9, 10], origin_id=[55011], pos=[55011, 3], ptr=[2], x=[55011, 3])\n",
      "    image = ImageBatch(num_settings=1, num_views=90, num_points=55011, device=cpu)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Run inference with augmentations\n",
    "# mm_data = dataset.train_dataset[0]\n",
    "# mm_data = dataset.val_dataset[0]\n",
    "mm_data = dataset.test_dataset[0][0]\n",
    "\n",
    "\n",
    "\n",
    "# Create a MMBatch and run inference\n",
    "batch = MMBatch.from_mm_data_list([mm_data])\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"input batch: \", batch)\n",
    "    model.set_input(batch, model.device)\n",
    "    model(batch)\n",
    "    \n",
    "# Recover the predicted labels for visualization\n",
    "mm_data.data.pred = model.output.detach().cpu().argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mode_pred(data):\n",
    "    pixel_validity = data.data.mvfusion_input[:, :, 0].bool()\n",
    "    mv_preds = data.data.mvfusion_input[:, :, -1].long()\n",
    "        \n",
    "    n_views = 9\n",
    "    \n",
    "    valid_m2f_feats = []\n",
    "    for i in range(len(mv_preds)):\n",
    "        valid_m2f_feats.append(mv_preds[i][pixel_validity[i]])\n",
    "\n",
    "    mode_preds = []\n",
    "    for m2feats_of_seen_point in valid_m2f_feats:\n",
    "        mode_preds.append(torch.mode(m2feats_of_seen_point.squeeze(), dim=0)[0])\n",
    "    mode_preds = torch.stack(mode_preds, dim=0)\n",
    "        \n",
    "    return mode_preds\n",
    "\n",
    "mode_preds = get_mode_pred(mm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2f_mm_data = mm_data.clone()\n",
    "m2f_mm_data.data.x = None\n",
    "m2f_mm_data.data.pred = mode_preds\n",
    "# m2f_mm_data.data.pred = m2f_mm_data.data.pred[m2f_mm_data.data.y != -1]\n",
    "m2f_mm_data = m2f_mm_data[m2f_mm_data.data.y != -1]\n",
    "\n",
    "visualize_mm_data(m2f_mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_mask_pred', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  4,  5,  6,  7,  8, 11, 14, 17, 19])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  4,  6,  7,  8, 11, 14, 17, 19])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_data.data.x = None\n",
    "mm_data.data.pred = mm_data.data.pred[mm_data.data.y != -1]\n",
    "mm_data = mm_data[mm_data.data.y != -1]\n",
    "\n",
    "\n",
    "print(mm_data.data.pred.unique())\n",
    "mm_data.data.y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_pred_mask', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate number of correct predictions (accuracy)\n",
    "\n",
    "print(sum(mm_data.y == mm_data.pred) / len(mm_data.y))\n",
    "\n",
    "print(sum(m2f_mm_data.y == m2f_mm_data.pred) / len(m2f_mm_data.y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
