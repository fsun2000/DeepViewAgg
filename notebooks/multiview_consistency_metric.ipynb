{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-view consistency tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMData debug() function changed, please uncomment the 3rd assert line when doing inference without M2F features!\n"
     ]
    }
   ],
   "source": [
    "# Select you GPU\n",
    "I_GPU = 0\n",
    "\n",
    "# Uncomment to use autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "start = time()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# torch.cuda.set_device(I_GPU)\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "from torch_geometric.data import Data\n",
    "from torch_points3d.core.multimodal.data import MMData, MMBatch\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
    "from torch_points3d.core.multimodal.image import SameSettingImageData, ImageData\n",
    "from torch_points3d.datasets.segmentation.multimodal.scannet import ScannetDatasetMM\n",
    "from torch_points3d.datasets.segmentation.scannet import CLASS_COLORS, CLASS_NAMES, CLASS_LABELS\n",
    "from torch_points3d.metrics.segmentation_tracker import SegmentationTracker\n",
    "\n",
    "from pykeops.torch import LazyTensor\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "#pio.renderers.default = 'jupyterlab'        # for local notebook\n",
    "pio.renderers.default = 'iframe_connected'  # for remote notebook. Other working (but seemingly slower) options are: 'sphinx_gallery' and 'iframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchnet as tnt\n",
    "import torch\n",
    "from typing import Dict, Any\n",
    "import wandb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "from torch_points3d.metrics.confusion_matrix import ConfusionMatrix\n",
    "from torch_points3d.models import model_interface\n",
    "from torch_points3d.metrics.base_tracker import BaseTracker, meter_value\n",
    "from torch_points3d.metrics.meters import APMeter\n",
    "from torch_points3d.datasets.segmentation import IGNORE_LABEL\n",
    "\n",
    "from torch_geometric.nn.unpool import knn_interpolate\n",
    "from torch_points3d.core.data_transform import SaveOriginalPosId\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def meter_value(meter, dim=0):\n",
    "    return float(meter.value()[dim]) if meter.n > 0 else 0.0\n",
    "\n",
    "\n",
    "class BaseTracker:\n",
    "    def __init__(self, stage: str, wandb_log: bool, use_tensorboard: bool):\n",
    "        self._wandb = wandb_log\n",
    "        self._use_tensorboard = use_tensorboard\n",
    "        self._tensorboard_dir = os.path.join(os.getcwd(), \"tensorboard\")\n",
    "        self._n_iter = 0\n",
    "        self._finalised = False\n",
    "        self._conv_type = None\n",
    "\n",
    "        if self._use_tensorboard:\n",
    "            log.info(\n",
    "                \"Access tensorboard with the following command <tensorboard --logdir={}>\".format(self._tensorboard_dir)\n",
    "            )\n",
    "            self._writer = SummaryWriter(log_dir=self._tensorboard_dir)\n",
    "\n",
    "    def reset(self, stage=\"train\"):\n",
    "        self._stage = stage\n",
    "        self._loss_meters = {}\n",
    "        self._finalised = False\n",
    "\n",
    "    def get_metrics(self, verbose=False) -> Dict[str, Any]:\n",
    "        metrics = {}\n",
    "        for key, loss_meter in self._loss_meters.items():\n",
    "            value = meter_value(loss_meter, dim=0)\n",
    "            if value:\n",
    "                metrics[key] = meter_value(loss_meter, dim=0)\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def metric_func(self):\n",
    "        self._metric_func = {\"loss\": min}\n",
    "        return self._metric_func\n",
    "\n",
    "    def track(self, model: model_interface.TrackerInterface, **kwargs):\n",
    "        if self._finalised:\n",
    "            raise RuntimeError(\"Cannot track new values with a finalised tracker, you need to reset it first\")\n",
    "            \n",
    "        if model is not None:\n",
    "            losses = self._convert(model.get_current_losses())\n",
    "            self._append_losses(losses)\n",
    "\n",
    "    def finalise(self, *args, **kwargs):\n",
    "        \"\"\" Lifcycle method that is called at the end of an epoch. Use this to compute\n",
    "        end of epoch metrics.\n",
    "        \"\"\"\n",
    "        self._finalised = True\n",
    "\n",
    "    def _append_losses(self, losses):\n",
    "        for key, loss in losses.items():\n",
    "            if loss is None:\n",
    "                continue\n",
    "            loss_key = \"%s_%s\" % (self._stage, key)\n",
    "            if loss_key not in self._loss_meters:\n",
    "                self._loss_meters[loss_key] = tnt.meter.AverageValueMeter()\n",
    "            self._loss_meters[loss_key].add(loss)\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert(x):\n",
    "        if torch.is_tensor(x):\n",
    "            return x.detach().cpu().numpy()\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def publish_to_tensorboard(self, metrics, step):\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            metric_name = \"{}/{}\".format(metric_name.replace(self._stage + \"_\", \"\"), self._stage)\n",
    "            self._writer.add_scalar(metric_name, metric_value, step)\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_stage_from_metric_keys(stage, metrics):\n",
    "        new_metrics = {}\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            new_metrics[metric_name.replace(stage + \"_\", \"\")] = metric_value\n",
    "        return new_metrics\n",
    "\n",
    "    def publish(self, step):\n",
    "        \"\"\" Publishes the current metrics to wandb and tensorboard\n",
    "        Arguments:\n",
    "            step: current epoch\n",
    "        \"\"\"\n",
    "        metrics = self.get_metrics()\n",
    "\n",
    "        if self._wandb:\n",
    "            wandb.log(metrics, step=step)\n",
    "\n",
    "        if self._use_tensorboard:\n",
    "            self.publish_to_tensorboard(metrics, step)\n",
    "\n",
    "        # Some metrics may be intended for wandb or tensorboard\n",
    "        # tracking but not for final final model selection. Those are\n",
    "        # the metrics absent from self.metric_func and must be excluded\n",
    "        # from the output of self.publish\n",
    "        current_metrics = {\n",
    "            k: v\n",
    "            for k, v in self._remove_stage_from_metric_keys(self._stage, metrics).items()\n",
    "            if k in self.metric_func.keys()}\n",
    "\n",
    "        return {\n",
    "            \"stage\": self._stage,\n",
    "            \"epoch\": step,\n",
    "            \"current_metrics\": current_metrics,\n",
    "        }\n",
    "\n",
    "    def print_summary(self):\n",
    "        metrics = self.get_metrics(verbose=True)\n",
    "        log.info(\"\".join([\"=\" for i in range(50)]))\n",
    "        for key, value in metrics.items():\n",
    "            log.info(\"    {} = {}\".format(key, value))\n",
    "        log.info(\"\".join([\"=\" for i in range(50)]))\n",
    "\n",
    "    @staticmethod\n",
    "    def _dict_to_str(dictionnary):\n",
    "        string = \"{\"\n",
    "        for key, value in dictionnary.items():\n",
    "            string += \"%s: %.2f,\" % (str(key), value)\n",
    "        string += \"}\"\n",
    "        return string\n",
    "\n",
    "\n",
    "class SegmentationTracker(BaseTracker):\n",
    "    def __init__(\n",
    "        self, dataset, stage=\"train\", wandb_log=False, use_tensorboard: bool = False, ignore_label: int = IGNORE_LABEL\n",
    "    ):\n",
    "        \"\"\" This is a generic tracker for multimodal tasks.\n",
    "        It uses a confusion matrix in the back-end to track results.\n",
    "        Use the tracker to track an epoch.\n",
    "        You can use the reset function before you start a new epoch\n",
    "\n",
    "        Arguments:\n",
    "            dataset  -- dataset to track (used for the number of classes)\n",
    "\n",
    "        Keyword Arguments:\n",
    "            stage {str} -- current stage. (train, validation, test, etc...) (default: {\"train\"})\n",
    "            wandb_log {str} --  Log using weight and biases\n",
    "        \"\"\"\n",
    "        super(SegmentationTracker, self).__init__(stage, wandb_log, use_tensorboard)\n",
    "        self._num_classes = dataset.num_classes\n",
    "        self._ignore_label = ignore_label\n",
    "        self._dataset = dataset\n",
    "        self.reset(stage)\n",
    "        self._metric_func = {\n",
    "            \"miou\": max,\n",
    "            \"macc\": max,\n",
    "            \"acc\": max,\n",
    "            \"loss\": min,\n",
    "            \"map\": max,\n",
    "        }  # Those map subsentences to their optimization functions\n",
    "\n",
    "    def reset(self, stage=\"train\"):\n",
    "        super().reset(stage=stage)\n",
    "        self._confusion_matrix = ConfusionMatrix(self._num_classes)\n",
    "        self._acc = 0\n",
    "        self._macc = 0\n",
    "        self._miou = 0\n",
    "        self._miou_per_class = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def detach_tensor(tensor):\n",
    "        if torch.torch.is_tensor(tensor):\n",
    "            tensor = tensor.detach()\n",
    "        return tensor\n",
    "\n",
    "    @property\n",
    "    def confusion_matrix(self):\n",
    "        return self._confusion_matrix.confusion_matrix\n",
    "\n",
    "    def track(self, model: model_interface.TrackerInterface, pred_labels=None, gt_labels=None, **kwargs):\n",
    "        \"\"\" Add current model predictions (usually the result of a batch) to the tracking\n",
    "        \"\"\"\n",
    "        if not self._dataset.has_labels(self._stage):\n",
    "            return\n",
    "\n",
    "        # Feng: to evaluate M2F predictions instead of model logits\n",
    "        if pred_labels is not None and gt_labels is not None:\n",
    "            outputs = pred_labels\n",
    "            targets = gt_labels\n",
    "        else:\n",
    "            super().track(model)\n",
    "            \n",
    "            outputs = model.get_output()\n",
    "            targets = model.get_labels()\n",
    "        self._compute_metrics(outputs, targets)\n",
    "\n",
    "    def _compute_metrics(self, outputs, labels):\n",
    "        mask = labels != self._ignore_label\n",
    "        outputs = outputs[mask]\n",
    "        labels = labels[mask]\n",
    "\n",
    "        outputs = self._convert(outputs)\n",
    "        labels = self._convert(labels)\n",
    "\n",
    "        if len(labels) == 0:\n",
    "            return\n",
    "\n",
    "        assert outputs.shape[0] == len(labels)\n",
    "        \n",
    "        # Check if output is predicted label or logits\n",
    "        if len(outputs.shape) > 1:\n",
    "            self._confusion_matrix.count_predicted_batch(labels, np.argmax(outputs, 1))\n",
    "        else:\n",
    "            \n",
    "            self._confusion_matrix.count_predicted_batch(labels, outputs)\n",
    "\n",
    "        self._acc = 100 * self._confusion_matrix.get_overall_accuracy()\n",
    "        self._macc = 100 * self._confusion_matrix.get_mean_class_accuracy()\n",
    "        self._miou = 100 * self._confusion_matrix.get_average_intersection_union()\n",
    "        self._miou_per_class = {\n",
    "            i: \"{:.2f}\".format(100 * v)\n",
    "            for i, v in enumerate(self._confusion_matrix.get_intersection_union_per_class()[0])\n",
    "        }\n",
    "\n",
    "    def get_metrics(self, verbose=False) -> Dict[str, Any]:\n",
    "        \"\"\" Returns a dictionnary of all metrics and losses being tracked\n",
    "        \"\"\"\n",
    "        metrics = super().get_metrics(verbose)\n",
    "\n",
    "        metrics[\"{}_acc\".format(self._stage)] = self._acc\n",
    "        metrics[\"{}_macc\".format(self._stage)] = self._macc\n",
    "        metrics[\"{}_miou\".format(self._stage)] = self._miou\n",
    "\n",
    "        if verbose:\n",
    "            metrics[\"{}_miou_per_class\".format(self._stage)] = self._miou_per_class\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def metric_func(self):\n",
    "        return self._metric_func\n",
    "\n",
    "\n",
    "class ScannetSegmentationTracker(SegmentationTracker):\n",
    "    def reset(self, stage=\"train\"):\n",
    "        super().reset(stage=stage)\n",
    "        self._full_confusion_matrix = ConfusionMatrix(self._num_classes)\n",
    "        self._raw_datas = {}\n",
    "        self._votes = {}\n",
    "        self._vote_counts = {}\n",
    "        self._full_preds = {}\n",
    "        self._full_acc = None\n",
    "\n",
    "    def track(self, model: model_interface.TrackerInterface, full_res=False, pred_labels=None, gt_labels=None, **kwargs):\n",
    "        \"\"\" Add current model predictions (usually the result of a batch) to the tracking\n",
    "        \"\"\"\n",
    "        if pred_labels is not None and gt_labels is not None:\n",
    "            super().track(model=None, pred_labels=pred_labels, gt_labels=gt_labels)\n",
    "        else:\n",
    "            super().track(model)\n",
    "\n",
    "            # Set conv type\n",
    "            self._conv_type = model.conv_type\n",
    "\n",
    "            # Train mode or low res, nothing special to do\n",
    "            if not full_res or self._stage == \"train\" or kwargs.get(\"data\") is None:\n",
    "                return\n",
    "\n",
    "            data = kwargs.get(\"data\", model.get_input())\n",
    "            data = data.data if model.is_multimodal else data\n",
    "            self._vote(data, model.get_output())\n",
    "\n",
    "    def get_metrics(self, verbose=False) -> Dict[str, Any]:\n",
    "        \"\"\" Returns a dictionnary of all metrics and losses being tracked\n",
    "        \"\"\"\n",
    "        metrics = super().get_metrics(verbose)\n",
    "        if self._full_acc:\n",
    "            metrics[\"{}_full_acc\".format(self._stage)] = self._full_acc\n",
    "            metrics[\"{}_full_macc\".format(self._stage)] = self._full_macc\n",
    "            metrics[\"{}_full_miou\".format(self._stage)] = self._full_miou\n",
    "        return metrics\n",
    "\n",
    "    def finalise(self, full_res=False, make_submission=False, **kwargs):\n",
    "        if not full_res and not make_submission:\n",
    "            return\n",
    "        \n",
    "        self._predict_full_res()\n",
    "\n",
    "        # Compute full res metrics\n",
    "        if self._dataset.has_labels(self._stage):\n",
    "            for scan_id in self._full_preds:\n",
    "                full_labels = self._raw_datas[scan_id].y\n",
    "                # Mask ignored labels\n",
    "                mask = full_labels != self._ignore_label\n",
    "                full_labels = full_labels[mask]\n",
    "                full_preds = self._full_preds[scan_id].cpu()[mask].numpy()\n",
    "                self._full_confusion_matrix.count_predicted_batch(full_labels, full_preds)\n",
    "\n",
    "            self._full_acc = 100 * self._full_confusion_matrix.get_overall_accuracy()\n",
    "            self._full_macc = 100 * self._full_confusion_matrix.get_mean_class_accuracy()\n",
    "            self._full_miou = 100 * self._full_confusion_matrix.get_average_intersection_union()\n",
    "            \n",
    "        # Save files to disk\n",
    "        if make_submission and self._stage == \"test\":\n",
    "            self._make_submission()\n",
    "\n",
    "    def _make_submission(self):\n",
    "        orginal_class_ids = np.asarray(self._dataset.train_dataset.valid_class_idx)\n",
    "        path_to_submission = self._dataset.path_to_submission\n",
    "        for scan_id in self._full_preds:\n",
    "            full_pred = self._full_preds[scan_id].cpu().numpy().astype(np.int8)\n",
    "            full_pred = orginal_class_ids[full_pred]  # remap labels to original labels between 0 and 40\n",
    "            scan_name = self._raw_datas[scan_id].scan_name\n",
    "            path_file = osp.join(path_to_submission, \"{}.txt\".format(scan_name))\n",
    "            np.savetxt(path_file, full_pred, delimiter=\"/n\", fmt=\"%d\")\n",
    "\n",
    "    def _vote(self, data, output):\n",
    "        \"\"\" Populates scores for the points in data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : Data\n",
    "            should contain `pos` and `SaveOriginalPosId.KEY` keys\n",
    "        output : torch.Tensor\n",
    "            probablities out of the model, shape: [N,nb_classes]\n",
    "        \"\"\"\n",
    "        id_scans = data.id_scan\n",
    "        if id_scans.dim() > 1:\n",
    "            id_scans = id_scans.squeeze()\n",
    "        if self._conv_type == \"DENSE\":\n",
    "            batch_size = len(id_scans)\n",
    "            output = output.view(batch_size, -1, output.shape[-1])\n",
    "\n",
    "        for idx_batch, id_scan in enumerate(id_scans):\n",
    "            # First time we see this scan\n",
    "            if id_scan not in self._raw_datas:\n",
    "                raw_data = self._dataset.get_raw_data(self._stage, id_scan, remap_labels=True)\n",
    "                self._raw_datas[id_scan] = raw_data\n",
    "                self._vote_counts[id_scan] = torch.zeros(raw_data.pos.shape[0], dtype=torch.int)\n",
    "                self._votes[id_scan] = torch.zeros((raw_data.pos.shape[0], self._num_classes), dtype=torch.float)\n",
    "            else:\n",
    "                raw_data = self._raw_datas[id_scan]\n",
    "\n",
    "            batch_mask = idx_batch\n",
    "            if self._conv_type != \"DENSE\":\n",
    "                batch_mask = data.batch == idx_batch\n",
    "            idx = data[SaveOriginalPosId.KEY][batch_mask]\n",
    "\n",
    "            self._votes[id_scan][idx] += output[batch_mask].cpu()\n",
    "            self._vote_counts[id_scan][idx] += 1\n",
    "\n",
    "    def _predict_full_res(self):\n",
    "        \"\"\" Predict full resolution results based on votes \"\"\"\n",
    "        for id_scan in self._votes:\n",
    "            has_prediction = self._vote_counts[id_scan] > 0\n",
    "            self._votes[id_scan][has_prediction] /= self._vote_counts[id_scan][has_prediction].unsqueeze(-1)\n",
    "\n",
    "            # Upsample and predict\n",
    "            full_pred = knn_interpolate(\n",
    "                self._votes[id_scan][has_prediction],\n",
    "                self._raw_datas[id_scan].pos[has_prediction],\n",
    "                self._raw_datas[id_scan].pos,\n",
    "                k=1,\n",
    "            )\n",
    "            self._full_preds[id_scan] = full_pred.argmax(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your dataset root directory, where the data was/will be downloaded\n",
    "DATA_ROOT = '/project/fsun/dvata'\n",
    "\n",
    "dataset_config = 'segmentation/multimodal/Feng/scannet-neucon-smallres-m2f-partial-subsampled'   \n",
    "models_config = 'segmentation/multimodal/Feng/mvfusion'    # model family\n",
    "model_name = 'MVFusion_3D_small'                       # specific model\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    f'data={dataset_config}',\n",
    "    f'models={models_config}',\n",
    "    f'model_name={model_name}',\n",
    "    f'data.dataroot={DATA_ROOT}',\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n",
    "cfg.data.load_m2f_masks = True   # load Mask2Former predicted masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "cfg.data.m2f_preds_dirname = 'm2f_masks'\n",
    "cfg.data.n_views = 9 #cfg.models[model_name].backbone.transformer.n_views\n",
    "print(cfg.data.n_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load predicted 2D semantic segmentation labels from directory  m2f_masks\n",
      "initialize train dataset\n",
      "initialize val dataset\n",
      "initialize test dataset\n",
      "line 720 scannet.py: split == 'test'\n",
      "Time = 1.4 sec.\n"
     ]
    }
   ],
   "source": [
    "# Dataset instantiation\n",
    "start = time()\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data = dataset.val_dataset[0]\n",
    "# print(mm_data.modalities['image'][0].m2f_pred_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: MVFusion_3D_small\n",
      "task:  segmentation.multimodal\n",
      "tested_model_name:  MVFusion_3D_small\n",
      "class_name:  MVFusionAPIModel\n",
      "model_module:  torch_points3d.models.segmentation.multimodal.Feng.mvfusion_3d\n",
      "name, cls of chosen model_cls:  MVFusionAPIModel <class 'torch_points3d.models.segmentation.multimodal.Feng.mvfusion_3d.MVFusionAPIModel'>\n",
      "x feature dim:  {'FEAT': 3}\n",
      "nc_in:  67\n",
      "nc_in:  64\n",
      "nc_in:  32\n",
      "nc_in:  64\n",
      "nc_in:  128\n",
      "nc_in:  256\n",
      "nc_in:  128\n",
      "nc_in:  128\n",
      "nc_in:  96\n",
      "nc_in:  96\n",
      "Model loaded\n",
      "input batch:  MMBatch(\n",
      "    data = Batch(batch=[97387], coords=[97387, 3], grid_size=[1], id_scan=[1], mapping_index=[97387], mvfusion_input=[70799, 9, 10], origin_id=[97387], pos=[97387, 3], ptr=[2], x=[97387, 3], y=[97387])\n",
      "    image = ImageBatch(num_settings=1, num_views=100, num_points=97387, device=cpu)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_points3d.models.model_factory import instantiate_model\n",
    "\n",
    "# Set your parameters\n",
    "checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/2022-12-07/12-07-34' # 3rd run\n",
    "\n",
    "# Create the model\n",
    "print(f\"Creating model: {cfg.model_name}\")\n",
    "model = instantiate_model(cfg, dataset)\n",
    "\n",
    "# Load the checkpoint and recover the model weights\n",
    "checkpoint = torch.load(f'{checkpoint_dir}/{model_name}.pt', map_location='cpu')\n",
    "model.load_state_dict_with_same_shape(checkpoint['models']['latest'], strict=False)\n",
    "\n",
    "# Prepare the model for inference\n",
    "model = model.eval().cuda()\n",
    "print('Model loaded')\n",
    "\n",
    "# Create a MMBatch and run inference\n",
    "batch = MMBatch.from_mm_data_list([mm_data])\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"input batch: \", batch)\n",
    "    model.set_input(batch, model.device)\n",
    "    model(batch)\n",
    "    \n",
    "# Recover the predicted labels for visualization\n",
    "mm_data.data.pred = model.output.detach().cpu().argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-view consistency metric\n",
    "## The analysis \n",
    "\n",
    "### Part 1: Compare baseline and MVFusion performance using only 2D-3D pixel-point correspondences.\n",
    "- How to make comparison fair in an unbalanced data setting? Number of pixels in 2D >> number of points in 3D, and each 3D point has varying number of matches.\n",
    "\n",
    "(a) just simply calculate mIoU of all projected pixels per 2D image, and compare with mIoU of projected 3D points to all images. In 2D mIoU! because that is our selling point\n",
    "\n",
    "(b) compare 2D mIoU of predicted masks, and 2D mIoU of Nearest Neighbor interpolated projected predictions from 3D.\n",
    " - then, also calculate a multi-view consistency metric \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mm_data.modalities['image'][0].mappings.values[2][-3:])\n",
    "\n",
    "# mm_data.modalities['image'][0].mappings.feature_map_indexing[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.utils.multimodal import lexargsort\n",
    "from torch_points3d.core.multimodal.csr import CSRData\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6937])\n"
     ]
    }
   ],
   "source": [
    "for x in mm_data.modalities['image'][0]:\n",
    "\n",
    "    print(x.mappings.feature_map_indexing[0].shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(174.0, 199.0, 232.0),\n",
       " (152.0, 223.0, 138.0),\n",
       " (31.0, 119.0, 180.0),\n",
       " (255.0, 187.0, 120.0),\n",
       " (188.0, 189.0, 34.0),\n",
       " (140.0, 86.0, 75.0),\n",
       " (255.0, 152.0, 150.0),\n",
       " (214.0, 39.0, 40.0),\n",
       " (197.0, 176.0, 213.0),\n",
       " (148.0, 103.0, 189.0),\n",
       " (196.0, 156.0, 148.0),\n",
       " (23.0, 190.0, 207.0),\n",
       " (247.0, 182.0, 210.0),\n",
       " (219.0, 219.0, 141.0),\n",
       " (255.0, 127.0, 14.0),\n",
       " (158.0, 218.0, 229.0),\n",
       " (44.0, 160.0, 44.0),\n",
       " (112.0, 128.0, 144.0),\n",
       " (227.0, 119.0, 194.0),\n",
       " (82.0, 84.0, 163.0),\n",
       " (0, 0, 0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker_mvfusion = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "tracker_mvfusion_2d = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "tracker_m2f = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "tracker_m2f_2d = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "\n",
    "CLASS_COLORS[0] = (174.0, 199.0, 232.0)\n",
    "CLASS_COLORS[-1] = (0, 0, 0)\n",
    "CLASS_COLORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_m2f.reset(stage='train')\n",
    "tracker_mvfusion.reset(stage='train')\n",
    "\n",
    "for sample_idx in range(3):\n",
    "    mm_data = dataset.val_dataset[sample_idx]\n",
    "    \n",
    "    # Create a MMBatch and run inference\n",
    "    batch = MMBatch.from_mm_data_list([mm_data])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"input batch: \", batch)\n",
    "        model.set_input(batch, model.device)\n",
    "        model(batch)\n",
    "\n",
    "    # Recover the predicted labels for visualization\n",
    "    mm_data.data.pred = model.output.detach().cpu().argmax(1)\n",
    "\n",
    "    mappings = mm_data.modalities['image'][0].mappings\n",
    "    point_ids = torch.arange(\n",
    "                    mappings.num_groups, device=mappings.device).repeat_interleave(\n",
    "                    mappings.pointers[1:] - mappings.pointers[:-1])\n",
    "    image_ids = mappings.images.repeat_interleave(\n",
    "                    mappings.values[1].pointers[1:] - mappings.values[1].pointers[:-1])    \n",
    "    pixels_full = mappings.pixels\n",
    "\n",
    "    # Sort point and image ids based on image_id\n",
    "    idx_sort = lexargsort(image_ids, point_ids)\n",
    "    image_ids = image_ids[idx_sort]\n",
    "    point_ids = point_ids[idx_sort]\n",
    "    pixels_full = pixels_full[idx_sort].long()\n",
    "\n",
    "    # Get pointers for easy indexing\n",
    "    pointers = CSRData._sorted_indices_to_pointers(image_ids)\n",
    "\n",
    "    # Loop over all N views\n",
    "    for i, x in enumerate(mm_data.modalities['image'][0]):\n",
    "\n",
    "        # Grab the 3D points corresponding to ith view\n",
    "        start, end = pointers[i], pointers[i+1]    \n",
    "        points = point_ids[start:end]\n",
    "        pixels = pixels_full[start:end]\n",
    "        # Image (x, y) pixel index\n",
    "        w, h = pixels[:, 0], pixels[:, 1]\n",
    "\n",
    "        # Grab set of points visible in current view\n",
    "        mm_data_of_view = mm_data[points]\n",
    "\n",
    "        # Get nearest neighbor interpolated projection image filled with 3D labels\n",
    "        pred_mask_2d = -1 * torch.ones((240, 320), dtype=torch.long, device=mm_data_of_view.device)    \n",
    "        pred_mask_2d[h, w] = mm_data_of_view.data.pred.squeeze()\n",
    "\n",
    "        nearest_neighbor = scipy.ndimage.morphology.distance_transform_edt(\n",
    "            pred_mask_2d==-1, return_distances=False, return_indices=True)    \n",
    "        pred_mask_2d = pred_mask_2d[nearest_neighbor]\n",
    "\n",
    "        # 2D mIoU calculation for M2F labels per view\n",
    "        # Get gt 2d image\n",
    "        gt_img_path = x.m2f_pred_mask_path[0].split(\"/\")\n",
    "        gt_img_path[-2] = 'label-filt-scannet20'\n",
    "        gt_img_path = \"/\".join(gt_img_path)\n",
    "        gt_img = Image.open(gt_img_path)\n",
    "        gt_img = np.asarray(gt_img.resize((320, 240), resample=0)).astype(int) - 1   # -1 label offset\n",
    "        gt_labels_2d = torch.LongTensor(gt_img[x.mappings.feature_map_indexing[2:]])\n",
    "\n",
    "        # M2F predicted mask for current view\n",
    "        m2f_labels_2d = x.get_mapped_m2f_features().squeeze()\n",
    "\n",
    "#         # Visualizations\n",
    "\n",
    "#         # color im\n",
    "#         im_path = x.m2f_pred_mask_path[0].split(\"/\")\n",
    "#         im_path[-2] = 'color_resized'\n",
    "#         im_path = \"/\".join(im_path)\n",
    "#         im = Image.open(im_path)\n",
    "#         im = np.asarray(im.resize((320, 240), resample=0))\n",
    "#         f, axarr = plt.subplots(1,4, figsize=(12, 8)) \n",
    "\n",
    "#         gt_img_rgb = np.array(list(map(lambda y: CLASS_COLORS[y], gt_img.flatten()))).reshape(240, 320, 3)\n",
    "#         pred_mask_2d_rgb = np.array(list(map(lambda y: CLASS_COLORS[y], pred_mask_2d.flatten()))).reshape(240, 320, 3)\n",
    "#         m2f_mask_2d_rgb = np.array(list(map(lambda y: CLASS_COLORS[y], x.m2f_pred_mask.flatten()))).reshape(240, 320, 3)\n",
    "        \n",
    "#         # use the created array to output your multiple images. In this case I have stacked 4 images vertically\n",
    "#         axarr[0].imshow(im.astype(int))\n",
    "#         axarr[1].imshow(gt_img_rgb.astype(int))\n",
    "#         axarr[2].imshow(m2f_mask_2d_rgb.astype(int))\n",
    "#         axarr[3].imshow(pred_mask_2d_rgb.astype(int))\n",
    "        \n",
    "#         axarr[0].set_title(\"image\")\n",
    "#         axarr[1].set_title(\"ground truth\")\n",
    "#         axarr[2].set_title(\"segmentation mask\")\n",
    "#         axarr[3].set_title(\"refined mask (projected from 3D)\")\n",
    "#         plt.show()\n",
    "\n",
    "        tracker_m2f.track(pred_labels=m2f_labels_2d, gt_labels=gt_labels_2d, model=None)\n",
    "        tracker_m2f_2d.track(pred_labels=x.m2f_pred_mask[0][0], gt_labels=torch.LongTensor(gt_img), model=None)\n",
    "        tracker_mvfusion.track(pred_labels=mm_data_of_view.data.pred, gt_labels=mm_data_of_view.data.y, model=None)\n",
    "        tracker_mvfusion_2d.track(pred_labels=pred_mask_2d, gt_labels=torch.LongTensor(gt_img), model=None)\n",
    "\n",
    "\n",
    "print(\"M2F mIoU of 3D projected points using 2D M2F pred label and 2D GT label: \\n\", tracker_m2f.get_metrics())\n",
    "print(\"MVFusion 3D mIoU: \\n\", tracker_mvfusion.get_metrics())\n",
    "\n",
    "print()\n",
    "print(\"M2F 2D mIoU using pred mask and GT img: \\n\", tracker_m2f_2d.get_metrics())\n",
    "print(\"MVFusion 2D projected nearest neighbor mIoU: \\n\", tracker_mvfusion_2d.get_metrics())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize_mm_data(mm_data, class_names=CLASS_NAMES, class_colors=CLASS_COLORS, error_color=(0, 0, 0), front='y', back='x', figsize=1000, pointsize=3, voxel=0.05, show_2d=True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-view consistency metric (Entropy)\n",
    "- For each class in a scene, aggregate all predicted labels of all points belonging to that class from all views in which it is seen.\n",
    "- In a scene, calculate the entropy of all predicted labels per class.\n",
    "- Aggregate these per class entropy scores across all validation scenes to get multi-view entropy scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMData(\n",
       "    data = Data(coords=[97387, 3], grid_size=[1], id_scan=[1], mapping_index=[97387], mvfusion_input=[72520, 9, 10], origin_id=[97387], pos=[97387, 3], pred=[97387], x=[97387, 3], y=[97387])\n",
       "    image = ImageData(num_settings=1, num_views=100, num_points=97387, device=cpu)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2f_mode_entropy_tracker = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "m2f_viewpred_entropy_tracker = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "mvfusion_entropy_tracker = ScannetSegmentationTracker(dataset=dataset, stage='train', wandb_log=False, use_tensorboard=False, ignore_label=IGNORE_LABEL)\n",
    "\n",
    "\n",
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1,  ..., 0, 0, 0])\n",
      "[1, 1, 6, 1, 1, 6, 1, 1, 19, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tensor([1, 1, 0,  ..., 0, 2, 0])\n",
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0])\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'train_acc': 89.24399681288179, 'train_macc': 83.44342992649281, 'train_miou': 49.209426409079654}\n",
      "{'train_acc': 94.7277149091515, 'train_macc': 88.77859864112395, 'train_miou': 66.5290880767561}\n",
      "{'train_acc': 87.37669839822357, 'train_macc': 79.92863987642941, 'train_miou': 44.872960735085535}\n"
     ]
    }
   ],
   "source": [
    "m2f_mode_entropy_tracker.reset(stage='train')\n",
    "mvfusion_entropy_tracker.reset(stage='train')\n",
    "m2f_viewpred_entropy_tracker.reset(stage='train')\n",
    "\n",
    "for sample_idx in range(3):\n",
    "\n",
    "    mm_data = dataset.val_dataset[sample_idx]\n",
    "\n",
    "    # Create a MMBatch and run inference\n",
    "    batch = MMBatch.from_mm_data_list([mm_data])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.set_input(batch, model.device)\n",
    "        model(batch)\n",
    "\n",
    "    # Recover the predicted labels for visualization\n",
    "    mm_data.data.pred = model.output.detach().cpu().argmax(1)\n",
    "\n",
    "    # This loop goes only once (actually not looping over all images, but processed everything at once)\n",
    "    for i, x in enumerate(mm_data.modalities['image']):\n",
    "\n",
    "        # M2F predicted mask for current view    \n",
    "        #### Mode preds\n",
    "        pixel_validity = mm_data.data.mvfusion_input[:, :, 0].bool()\n",
    "        mv_preds = mm_data.data.mvfusion_input[:, :, -1].long()\n",
    "\n",
    "        valid_m2f_feats = []\n",
    "        for i in range(len(mv_preds)):\n",
    "            valid_m2f_feats.append(mv_preds[i][pixel_validity[i]])\n",
    "\n",
    "        ### IMPROVE by removing this loop and merging with previous loop\n",
    "        mode_preds = []\n",
    "        for m2feats_of_seen_point in valid_m2f_feats:\n",
    "            mode_preds.append(torch.mode(m2feats_of_seen_point.squeeze(), dim=0)[0])\n",
    "        mode_preds = torch.stack(mode_preds, dim=0)\n",
    "\n",
    "\n",
    "        csr_idx = mm_data.modalities['image'][0].view_csr_indexing\n",
    "        n_seen = csr_idx[1:] - csr_idx[:-1]\n",
    "        seen_mask = ( n_seen > 0 )\n",
    "        \n",
    "        \n",
    "        m2f_mode_entropy_tracker.track(pred_labels=mode_preds, gt_labels=mm_data.data.y[seen_mask], model=None)\n",
    "        mvfusion_entropy_tracker.track(pred_labels=mm_data.data.pred[seen_mask], gt_labels=mm_data.data.y[seen_mask], model=None)\n",
    "\n",
    "        # Instead of mode preds, add all M2F preds within mvfusion_input. Duplicate gt labels to be able to evaluate.\n",
    "        seen_labels = mm_data.data.y[seen_mask]\n",
    "        duplicated_gt_labels = []\n",
    "        for i, m2f_feats_of_point in enumerate(valid_m2f_feats):\n",
    "            duplicated_gt_labels.extend([seen_labels[i].item()] * len(m2f_feats_of_point))\n",
    "            \n",
    "\n",
    "        valid_m2f_feats = torch.cat(valid_m2f_feats, dim=-1).tolist()\n",
    "        \n",
    "        m2f_viewpred_entropy_tracker.track(pred_labels=torch.LongTensor(valid_m2f_feats),\n",
    "                                           gt_labels=torch.LongTensor(duplicated_gt_labels), model=None)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "print(m2f_mode_entropy_tracker.get_metrics())\n",
    "print(mvfusion_entropy_tracker.get_metrics())\n",
    "print(m2f_viewpred_entropy_tracker.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28071956009032795\n"
     ]
    }
   ],
   "source": [
    "confusion_mat = m2f_viewpred_entropy_tracker._confusion_matrix.get_confusion_matrix()\n",
    "\n",
    "per_class_normalized_entropy = []\n",
    "for i in range(len(confusion_mat)):\n",
    "    nonzero_entries = confusion_mat[:, i][confusion_mat[:, i] > 0]\n",
    "#     print( nonzero_entries )\n",
    "    \n",
    "    # normalized entropy using log2 base\n",
    "    pk = nonzero_entries / nonzero_entries.sum()\n",
    "    \n",
    "    if len(pk) <= 1:\n",
    "        per_class_normalized_entropy.append(0.)\n",
    "    else:\n",
    "        normalized_entropy = -sum(pk * np.log2(pk)) / np.log2(len(pk))\n",
    "        per_class_normalized_entropy.append(normalized_entropy)\n",
    "        \n",
    "print(np.mean(np.array(per_class_normalized_entropy)))\n",
    "\n",
    "# MVFusion predicted 3d points: 0.1440264225076999\n",
    "# M2F mode pred 3d points: 0.17243167203189816\n",
    "# M2F view pred as 3d points (sampled more points): 0.28071956009032795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_multiview_inconsistency(masks=None, pp_mapping=None, pixel_validity=None, interpol_mask_labels=None, gt_mask_labels=None, gt_3d_labels=None):\n",
    "    # transpose for ease\n",
    "    interpol_mask_labels = interpol_mask_labels.T\n",
    "    gt_mask_labels = gt_mask_labels.T\n",
    "    \n",
    "    total_entropy = 0.\n",
    "    gt_total_entropy = 0.\n",
    "    n_multiview_points = 0\n",
    "    \n",
    "    # when x was the mode label, how many other views predicted x wrongly\n",
    "    barplot_n_mistakes_in_other_views = np.zeros(20)\n",
    "    gt_barplot_n_mistakes_in_other_views = np.zeros(20)\n",
    "    \n",
    "    for i in range(interpol_mask_labels.shape[0]):\n",
    "        # don't evaluate invalid gt class\n",
    "        if not gt_3d_labels[i] == 0:\n",
    "            labels, label_counts = torch.unique(interpol_mask_labels[i], return_counts=True, sorted=True)\n",
    "            gt_labels, gt_label_counts = torch.unique(gt_mask_labels[i], return_counts=True, sorted=True)\n",
    "\n",
    "            if labels[0] == 0.:\n",
    "                labels = labels[1:]\n",
    "                label_counts = label_counts[1:]\n",
    "                \n",
    "            if gt_labels[0] == 0.:\n",
    "                gt_labels = gt_labels[1:]\n",
    "                gt_label_counts = gt_label_counts[1:]\n",
    "\n",
    "            # evaluate inconsistency if point is seen in more than 1 view\n",
    "            if label_counts.sum() > 1:\n",
    "#                 p_tensor = label_counts\n",
    "#                 gt_p_tensor = gt_label_counts\n",
    "\n",
    "                # normalized entropy using log2 base\n",
    "                pk = label_counts / label_counts.sum()\n",
    "                normalized_entropy = -sum(pk * torch.log2(pk)) / torch.log2(torch.tensor(len(pk)))\n",
    "\n",
    "                gt_pk = gt_label_counts / gt_label_counts.sum()\n",
    "                gt_normalized_entropy = -sum(gt_pk * torch.log2(gt_pk)) / torch.log2(torch.tensor(len(gt_pk)))\n",
    "                \n",
    "#                 entropy = Categorical(probs = p_tensor).entropy()\n",
    "#                 gt_entropy = Categorical(probs = gt_p_tensor).entropy()\n",
    "                \n",
    "                barplot_n_mistakes_in_other_views[int(labels[0]-1)] += normalized_entropy\n",
    "                gt_barplot_n_mistakes_in_other_views[int(gt_labels[0]-1)] += gt_normalized_entropy\n",
    "                \n",
    "                n_multiview_points += 1\n",
    "                total_entropy += normalized_entropy\n",
    "                gt_total_entropy += gt_normalized_entropy\n",
    "                \n",
    "    return (total_entropy / n_multiview_points, n_multiview_points, barplot_n_mistakes_in_other_views / n_multiview_points), (gt_total_entropy / n_multiview_points, n_multiview_points, gt_barplot_n_mistakes_in_other_views / n_multiview_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "label_unique = set()\n",
    "for n in os.listdir(\"/project/fsun/data/scannet/scans/scene0000_00/swin_l_early\"):\n",
    "    a = Image.open(f\"/project/fsun/data/scannet/scans/scene0000_00/swin_l_early/{n}\")\n",
    "    label_unique.update(list(np.unique(np.array(a)))) \n",
    "print(label_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_unique = set()\n",
    "for n in os.listdir(\"/project/fsun/data/scannet/scans/scene0000_00/m2f_masks\"):\n",
    "    a = Image.open(f\"/project/fsun/data/scannet/scans/scene0000_00/m2f_masks/{n}\")\n",
    "    label_unique.update(list(np.unique(np.array(a)))) \n",
    "print(label_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data = dataset.train_dataset[0]\n",
    "mm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare mapping feature statistics between train/val/test proccessed_2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'val', 'test']:\n",
    "    data_dir = f\"/project/fsun/dvata/scannet-neucon-smallres-m2f/processed/processed_2d_{split}\"\n",
    "\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    for name in os.listdir(data_dir):\n",
    "        file = osp.join(data_dir, name)\n",
    "\n",
    "        data = torch.load(file)\n",
    "        data = data._mappings.values[2]\n",
    "\n",
    "        # note that estimation is biased\n",
    "        mean += data.mean(axis=0)\n",
    "        std += data.std(axis=0)\n",
    "        \n",
    "    mean /= len(os.listdir(data_dir))\n",
    "    std /= len(os.listdir(data_dir))\n",
    "\n",
    "\n",
    "    print(f\"{split} mapping feature statistics:\")\n",
    "    print(mean)\n",
    "    print(std)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your dataset root directory, where the data was/will be downloaded\n",
    "DATA_ROOT = '/project/fsun/dvata'\n",
    "\n",
    "dataset_config = 'segmentation/multimodal/Feng/scannet-neucon-smallres-m2f-partial-subsampled'   \n",
    "models_config = 'segmentation/multimodal/Feng/mvfusion'    # model family\n",
    "model_name = 'MVFusion_3D_small'                       # specific model\n",
    "\n",
    "# models_config = 'segmentation/multimodal/Feng/3d_only'    # model family\n",
    "# model_name = 'Res16UNet34'                       # specific model\n",
    "\n",
    "\n",
    "\n",
    "# dataset_config = 'segmentation/multimodal/scannet-sparse'\n",
    "# models_config = 'segmentation/multimodal/sparseconv3d'    # model family\n",
    "# model_name = 'Res16UNet34-L4-early'                       # specific model\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    f'data={dataset_config}',\n",
    "    f'models={models_config}',\n",
    "    f'model_name={model_name}',\n",
    "    f'data.dataroot={DATA_ROOT}',\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n",
    "# cfg.data.load_m2f_masks = True   # load Mask2Former predicted masks\n",
    "\n",
    "cfg.data.load_m2f_masks = True   # load Mask2Former predicted masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "cfg.data.m2f_preds_dirname = 'm2f_masks'\n",
    "cfg.data.n_views = 9 #cfg.models[model_name].backbone.transformer.n_views\n",
    "print(cfg.data.n_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load predicted 2D semantic segmentation labels from directory  m2f_masks\n",
      "initialize train dataset\n",
      "initialize val dataset\n",
      "initialize test dataset\n",
      "line 720 scannet.py: split == 'test'\n",
      "Time = 1.4 sec.\n"
     ]
    }
   ],
   "source": [
    "# Dataset instantiation\n",
    "start = time()\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)|\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference from pretrained weights and visualize predictions\n",
    "It is possible to visualize the pointwise predictions and errors from a model. \n",
    "\n",
    "To do so, we will use the pretrained weights made available with this project. See `README.md` to get the download links and manually place the `.pt` files locally. You will need to provide `checkpoint_dir` where you saved those files in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: MVFusion_3D_small\n",
      "task:  segmentation.multimodal\n",
      "tested_model_name:  MVFusion_3D_small\n",
      "class_name:  MVFusionAPIModel\n",
      "model_module:  torch_points3d.models.segmentation.multimodal.Feng.mvfusion_3d\n",
      "name, cls of chosen model_cls:  MVFusionAPIModel <class 'torch_points3d.models.segmentation.multimodal.Feng.mvfusion_3d.MVFusionAPIModel'>\n",
      "x feature dim:  {'FEAT': 3}\n",
      "nc_in:  67\n",
      "nc_in:  64\n",
      "nc_in:  32\n",
      "nc_in:  64\n",
      "nc_in:  128\n",
      "nc_in:  256\n",
      "nc_in:  128\n",
      "nc_in:  128\n",
      "nc_in:  96\n",
      "nc_in:  96\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from torch_points3d.models.model_factory import instantiate_model\n",
    "\n",
    "# Set your parameters\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/2022-12-04/15-22-16' # MVFusion_3D_small default m2f_masks\n",
    "checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/2022-12-07/12-07-34' # 3rd run\n",
    "\n",
    "        \n",
    "# # checkpoint_dir = '/home/fsun/DeepViewAgg/outputs/2022-12-04/15-48-56' # MVFusion_3D_small default swin_l_early\n",
    "\n",
    "\n",
    "# checkpoint_dir = '/project/fsun/DeepViewAgg/outputs/2022-11-04/15-51-33' # 3D Backbone, 68.04 miou\n",
    "# checkpoint_dir = '/home/fsun/DeepViewAgg/model_checkpoints' # DVA best model\n",
    "\n",
    "# Create the model\n",
    "print(f\"Creating model: {cfg.model_name}\")\n",
    "model = instantiate_model(cfg, dataset)\n",
    "# print(model)\n",
    "\n",
    "# Load the checkpoint and recover the 'best_miou' model weights\n",
    "checkpoint = torch.load(f'{checkpoint_dir}/{model_name}.pt', map_location='cpu')\n",
    "model.load_state_dict_with_same_shape(checkpoint['models']['latest'], strict=False)\n",
    "\n",
    "# Prepare the model for inference\n",
    "model = model.eval().cuda()\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have loaded the model, we need to run a forward pass on a sample. Howver, if we want to be able to visualize the predictions, we need to pay special attention to which type of 3D and 2D transforms we apply on the data if we do not want to break the mappings. To do so, we will manually apply some sensitive transforms to be able to both infer on the data and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_real_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25754/2988156686.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Pick a room in the Train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_real_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi_room\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_3d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude_3d_viz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude_2d_viz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Pick a room in the Val set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_real_data' is not defined"
     ]
    }
   ],
   "source": [
    "i_room = 0\n",
    "\n",
    "# Pick a room in the Train set\n",
    "mm_data = sample_real_data(dataset.train_dataset, idx=i_room, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
    "\n",
    "# Pick a room in the Val set\n",
    "# mm_data = sample_real_data(dataset.val_dataset, idx=i_room, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
    "\n",
    "# Pick a room in the Test set\n",
    "# mm_data = sample_real_data(dataset.test_dataset[0], idx=i_room, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
    "\n",
    "# Extract point cloud and images from MMData object\n",
    "data = mm_data.data.clone()\n",
    "images = mm_data.modalities['image'].clone()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell for validation sample with original validation transformations\n",
    "mm_data = dataset.val_dataset[0]\n",
    "\n",
    "# data = mm_data.data.clone()\n",
    "# images = mm_data.modalities['image'].clone()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For voxel-based 3D backbones such as SparseConv3d and MinkowskiNet, points need to be \n",
    "# preprocessed with Center and GridSampling3D. Unfortunately, Center breaks relative \n",
    "# positions between points and images. Besides, the combination of Center and GridSampling3D\n",
    "# may lead to some points being merged into the same voxels, so we must apply it to both the\n",
    "# inference and visualization data to make sure we have the same voxels. The workaround here \n",
    "# is to manually run these while keeping track of the centering offset\n",
    "center = data.pos.mean(dim=-2, keepdim=True)\n",
    "data = AddFeatsByKeys(list_add_to_x=[True, True, True], feat_names=['pos_x', 'pos_y', 'pos_z'], delete_feats=[True, True, True])(data)          # add z-height to the features\n",
    "data = Center()(data)                                                                                 # mean-center the data\n",
    "data = GridSampling3D(cfg.data.resolution_3d, quantize_coords=True, mode='last')(data)                # quantization for volumetric models\n",
    "\n",
    "# This last voxelization step with GridSampling3D might have removed some points, so we need\n",
    "# to update the mappings usign SelectMappingFromPointId. To control the size of the batch, we\n",
    "# use PickImagesFromMemoryCredit. Besides, 2D models expect normalized float images, which is\n",
    "# why we call ToFloatImage and Normalize\n",
    "data, images = SelectMappingFromPointId()(data, images)                                               # update mappings after GridSampling3D\n",
    "data, images = PickImagesFromMemoryCredit(\n",
    "    img_size=cfg.data.resolution_2d, \n",
    "    k_coverage=cfg.data.multimodal.settings.k_coverage, \n",
    "    n_img=cfg.data.multimodal.settings.test_pixel_credit)(data, images)                                      # select images to respect memory constraints\n",
    "data, images_infer = ToFloatImage()(data, images.clone())                                             # convert uint8 images to float\n",
    "data, images_infer = Normalize()(data, images_infer)                                                  # RGB normalization\n",
    "\n",
    "# Create a MMData for inference\n",
    "mm_data_infer = MMData(data, image=images_infer)\n",
    "print(mm_data_infer)\n",
    "\n",
    "# Create a MMBatch and run inference\n",
    "batch = MMBatch.from_mm_data_list([mm_data_infer])\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"input batch: \", batch)\n",
    "    model.set_input(batch, model.device)\n",
    "    model(batch)\n",
    "\n",
    "# Create a MMData for visualization\n",
    "data.pos += center\n",
    "mm_data = MMData(data, image=images)\n",
    "\n",
    "# Recover the predicted labels for visualization\n",
    "mm_data.data.pred = model.output.detach().cpu().argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch:  MMBatch(\n",
      "    data = Batch(batch=[97387], coords=[97387, 3], grid_size=[1], id_scan=[1], mapping_index=[97387], mvfusion_input=[70411, 9, 10], origin_id=[97387], pos=[97387, 3], ptr=[2], x=[97387, 3], y=[97387])\n",
      "    image = ImageBatch(num_settings=1, num_views=100, num_points=97387, device=cpu)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Run inference with augmentations\n",
    "# mm_data = dataset.train_dataset[0]\n",
    "mm_data = dataset.val_dataset[0]\n",
    "# mm_data = dataset.test_dataset[0][0]\n",
    "\n",
    "# Create a MMBatch and run inference\n",
    "batch = MMBatch.from_mm_data_list([mm_data])\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"input batch: \", batch)\n",
    "    model.set_input(batch, model.device)\n",
    "    model(batch)\n",
    "    \n",
    "# Recover the predicted labels for visualization\n",
    "mm_data.data.pred = model.output.detach().cpu().argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_COLORS[0] = (174.0, 199.0, 232.0)\n",
    "CLASS_COLORS[-1] = (0, 0, 0)\n",
    "# CLASS_COLORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(coords=[97387, 3], grid_size=[1], id_scan=[1], mapping_index=[97387], mvfusion_input=[70411, 9, 10], origin_id=[97387], pos=[97387, 3], pred=[97387], x=[97387, 3], y=[97387])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MMData(\n",
       "    data = Data(coords=[97387, 3], grid_size=[1], id_scan=[1], mapping_index=[97387], mvfusion_input=[70411, 9, 10], origin_id=[97387], pos=[97387, 3], pred=[97387], x=[97387, 3], y=[97387])\n",
       "    image = None\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mm_data.data)\n",
    "mm_data.modalities['image'] = None\n",
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = visualize_mm_data(mm_data, no_output=True, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='x', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select seen points\n",
    "csr_idx = mm_data.modalities['image'][0].view_csr_indexing\n",
    "dense_idx_list = torch.arange(mm_data.modalities['image'][0].num_points).repeat_interleave(csr_idx[1:] - csr_idx[:-1])\n",
    "# take subset of only seen points without re-indexing the same point\n",
    "mm_data = mm_data[dense_idx_list.unique()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mode_pred(data):\n",
    "    pixel_validity = data.data.mvfusion_input[:, :, 0].bool()\n",
    "    mv_preds = data.data.mvfusion_input[:, :, -1].long()\n",
    "            \n",
    "    valid_m2f_feats = []\n",
    "    for i in range(len(mv_preds)):\n",
    "        valid_m2f_feats.append(mv_preds[i][pixel_validity[i]])\n",
    "\n",
    "    mode_preds = []\n",
    "    for m2feats_of_seen_point in valid_m2f_feats:\n",
    "        mode_preds.append(torch.mode(m2feats_of_seen_point.squeeze(), dim=0)[0])\n",
    "    mode_preds = torch.stack(mode_preds, dim=0)\n",
    "        \n",
    "    return mode_preds\n",
    "\n",
    "mode_preds = get_mode_pred(mm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mode_preds.shape)\n",
    "print(mm_data.data.pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2f_mm_data = mm_data.clone()\n",
    "m2f_mm_data.data.x = None\n",
    "m2f_mm_data.data.pred = mode_preds\n",
    "# m2f_mm_data.data.pred = m2f_mm_data.data.pred[m2f_mm_data.data.y != -1]\n",
    "m2f_mm_data = m2f_mm_data[m2f_mm_data.data.y != -1]\n",
    "\n",
    "visualize_mm_data(m2f_mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_mask_pred', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data.data.x = None\n",
    "mm_data.data.pred = mm_data.data.pred[mm_data.data.y != -1]\n",
    "mm_data = mm_data[mm_data.data.y != -1]\n",
    "\n",
    "\n",
    "print(mm_data.data.pred.unique())\n",
    "mm_data.data.y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_pred_mask', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin_l_early masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2f_mm_data = mm_data.clone()\n",
    "m2f_mm_data.data.x = None\n",
    "m2f_mm_data.data.pred = mode_preds\n",
    "# m2f_mm_data.data.pred = m2f_mm_data.data.pred[m2f_mm_data.data.y != -1]\n",
    "m2f_mm_data = m2f_mm_data[m2f_mm_data.data.y != -1]\n",
    "\n",
    "visualize_mm_data(m2f_mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_mask_pred', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_pred_mask', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate number of correct predictions (accuracy)\n",
    "\n",
    "print(sum(mm_data.y == mm_data.pred) / len(mm_data.y))\n",
    "\n",
    "print(sum(m2f_mm_data.y == m2f_mm_data.pred) / len(m2f_mm_data.y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_room = 0\n",
    "\n",
    "# Pick a room in the Train set\n",
    "mm_data = sample_real_data(dataset.train_dataset, idx=i_room, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
    "\n",
    "# Pick a room in the Val set\n",
    "# mm_data = sample_real_data(dataset.val_dataset, idx=i_room, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
    "\n",
    "# Pick a room in the Test set\n",
    "# mm_data = sample_real_data(dataset.test_dataset[0], idx=i_room, exclude_3d=exclude_3d_viz, exclude_2d=exclude_2d_viz)\n",
    "\n",
    "# Extract point cloud and images from MMData object\n",
    "data = mm_data.data.clone()\n",
    "images = mm_data.modalities['image'].clone()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data = dataset.train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select seen points\n",
    "csr_idx = mm_data.modalities['image'][0].view_csr_indexing\n",
    "dense_idx_list = torch.arange(mm_data.modalities['image'][0].num_points).repeat_interleave(csr_idx[1:] - csr_idx[:-1])\n",
    "# take subset of only seen points without re-indexing the same point\n",
    "mm_data = mm_data[dense_idx_list.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_preds = get_mode_pred(mm_data)\n",
    "m2f_mm_data = mm_data.clone()\n",
    "m2f_mm_data.data.x = None\n",
    "m2f_mm_data.data.pred = mode_preds\n",
    "# m2f_mm_data.data.pred = m2f_mm_data.data.pred[m2f_mm_data.data.y != -1]\n",
    "m2f_mm_data = m2f_mm_data[m2f_mm_data.data.y != -1]\n",
    "\n",
    "visualize_mm_data(m2f_mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_mask_pred', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_preds = get_mode_pred(mm_data)\n",
    "m2f_mm_data = mm_data.clone()\n",
    "m2f_mm_data.data.x = None\n",
    "m2f_mm_data.data.pred = mode_preds\n",
    "# m2f_mm_data.data.pred = m2f_mm_data.data.pred[m2f_mm_data.data.y != -1]\n",
    "m2f_mm_data = m2f_mm_data[m2f_mm_data.data.y != -1]\n",
    "\n",
    "visualize_mm_data(m2f_mm_data, figsize=1000, pointsize=3, voxel=0.05, show_2d=False, back='m2f_mask_pred', front='y', class_names=CLASS_NAMES, class_colors=CLASS_COLORS, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
