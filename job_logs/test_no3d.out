[2022-10-13 11:27:15,211][torch_points3d.trainer][INFO] - DEVICE : cuda
self._cfg.training.checkpoint_dir:  
initialize train dataset
initialize val dataset
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
backbone.down_modules.0.image.conv
backbone
head
[2022-10-13 11:27:30,530][torch_points3d.core.schedulers.bn_schedulers][INFO] - Setting batchnorm momentum at 0.02
[2022-10-13 11:27:31,586][torch_points3d.trainer][WARNING] - The model will not be able to be used from pretrained weights without the corresponding dataset. Current properties are {'feature_dimension': 1, 'num_classes': 20}
[2022-10-13 11:27:31,586][torch_points3d.trainer][INFO] - No3DFeatureFusion(
  (backbone): No3DEncoder(
    (down_modules): ModuleList(
      (0): MultimodalBlockDown(
        (block_1): Identity()
        (block_2): Identity()
        (image): UnimodalBranch(
          drop_3d=None
          drop_mod=Dropout(p=0.1, inplace=True)
          keep_last_view=False
          checkpointing=
          (conv): ADE20KResNet18PPM(
            (encoder): ResnetDilated(
              (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn3): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
              (relu3): ReLU(inplace=True)
              (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
              (layer1): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (downsample): Sequential(
                    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    (1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  )
                )
                (1): BasicBlock(
                  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                )
              )
              (layer2): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                  (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (downsample): Sequential(
                    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                    (1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  )
                )
                (1): BasicBlock(
                  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                )
              )
              (layer3): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
                  (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (downsample): Sequential(
                    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    (1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  )
                )
                (1): BasicBlock(
                  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
                  (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
                  (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                )
              )
              (layer4): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
                  (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
                  (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (downsample): Sequential(
                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  )
                )
                (1): BasicBlock(
                  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
                  (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
                  (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                )
              )
            )
            (decoder): PPMFeatMap(
              (ppm): ModuleList(
                (0): Sequential(
                  (0): AdaptiveAvgPool2d(output_size=1)
                  (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (2): PrudentSynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (3): ReLU(inplace=True)
                )
                (1): Sequential(
                  (0): AdaptiveAvgPool2d(output_size=2)
                  (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (2): PrudentSynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (3): ReLU(inplace=True)
                )
                (2): Sequential(
                  (0): AdaptiveAvgPool2d(output_size=3)
                  (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (2): PrudentSynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (3): ReLU(inplace=True)
                )
                (3): Sequential(
                  (0): AdaptiveAvgPool2d(output_size=6)
                  (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (2): PrudentSynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                  (3): ReLU(inplace=True)
                )
              )
              (conv_last): Sequential(
                (0): Conv2d(2560, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
            )
          )
          (atomic_pool): BimodalCSRPool()
          (view_pool): GroupBimodalCSRPool(
            num_groups=4
            use_mod=False
            group_scaling=True
            save_last=False
            (E_map): DeepSetFeat(
              pool=['max']
              fusion=concatenation
              use_num=True
              (mlp_elt_1): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=8, out_features=32, bias=False)
                  (1): FastBatchNorm1d(
                    (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)
                  )
                  (2): LeakyReLU(negative_slope=0.2, inplace=True)
                )
                (1): Sequential(
                  (0): Linear(in_features=32, out_features=32, bias=False)
                  (1): FastBatchNorm1d(
                    (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)
                  )
                  (2): LeakyReLU(negative_slope=0.2, inplace=True)
                )
              )
              (mlp_set): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=33, out_features=32, bias=False)
                  (1): FastBatchNorm1d(
                    (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)
                  )
                  (2): LeakyReLU(negative_slope=0.2, inplace=True)
                )
                (1): Sequential(
                  (0): Linear(in_features=32, out_features=32, bias=False)
                  (1): FastBatchNorm1d(
                    (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)
                  )
                  (2): LeakyReLU(negative_slope=0.2, inplace=True)
                )
              )
              (mlp_elt_2): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=64, out_features=32, bias=False)
                  (1): FastBatchNorm1d(
                    (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)
                  )
                  (2): LeakyReLU(negative_slope=0.2, inplace=True)
                )
                (1): Sequential(
                  (0): Linear(in_features=32, out_features=32, bias=False)
                  (1): FastBatchNorm1d(
                    (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)
                  )
                  (2): LeakyReLU(negative_slope=0.2, inplace=True)
                )
              )
            )
            (E_mod): Sequential(
              (0): Sequential(
                (0): Linear(in_features=512, out_features=512, bias=False)
                (1): FastBatchNorm1d(
                  (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)
                )
                (2): LeakyReLU(negative_slope=0.2, inplace=True)
              )
              (1): Sequential(
                (0): Linear(in_features=512, out_features=512, bias=False)
                (1): FastBatchNorm1d(
                  (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)
                )
                (2): LeakyReLU(negative_slope=0.2, inplace=True)
              )
            )
            (E_score): Linear(in_features=32, out_features=4, bias=True)
            (G): Gating(num_groups=4, weight=True, bias=True)
          )
          (fusion): BimodalFusion(mode=residual)
          (drop_mod): Dropout(p=0.1, inplace=True)
        )
      )
    )
  )
  (head): Sequential(
    (0): Linear(in_features=512, out_features=20, bias=True)
  )
)
[2022-10-13 11:27:31,589][torch_points3d.utils.colors][INFO] - [0;32mOptimizer: SGD (
Parameter Group 0
    dampening: 0.1
    foreach: None
    initial_lr: 0.1
    lr: 0.1
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001

Parameter Group 1
    dampening: 0.1
    foreach: None
    initial_lr: 0.1
    lr: 0.1
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001

Parameter Group 2
    dampening: 0.1
    foreach: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001
)[0m
[2022-10-13 11:27:31,589][torch_points3d.utils.colors][INFO] - [0;32mLearning Rate Scheduler: ExponentialLR({'gamma': 0.9885}, update_scheduler_on=on_epoch)[0m
[2022-10-13 11:27:31,589][torch_points3d.utils.colors][INFO] - [0;32mBatchNorm Scheduler: BNMomentumScheduler(base_momentum: 0.02, update_scheduler_on=on_epoch)[0m
[2022-10-13 11:27:31,589][torch_points3d.utils.colors][INFO] - [0;32mAccumulated gradients: None[0m
[2022-10-13 11:27:31,589][torch_points3d.trainer][INFO] - Model size = 24718720
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
[2022-10-13 11:27:31,590][torch_points3d.trainer][INFO] - Dataset: ScannetDatasetMM 
[0;95mtrain_pre_batch_collate_transform [0m= None
[0;95mval_pre_batch_collate_transform [0m= None
[0;95mtest_pre_batch_collate_transform [0m= None
[0;95mpre_transform [0m= Compose([
    SaveOriginalPosId,
    PCAComputePointwise(num_neighbors=50, r=None, use_full_pos=False, use_cuda=False, use_faiss=False, ncells=None, nprobes=10, chunk_size=1000000),
    EigenFeatures(norm=True, linearity=True, planarity=True, scattering=True, temperature=None),
    RemoveAttributes(attr_names=['eigenvalues', 'eigenvectors'], strict=False),
])
[0;95mtest_transform [0m= Compose([
    GridSampling3D(grid_size=0.03, quantize_coords=True, mode=last),
    XYZFeature(axis=['z']),
    AddFeatsByKeys(pos_z=True, rgb=False, linearity=False, norm=False, planarity=False, scattering=False),
])
[0;95mtrain_transform [0m= Compose([
    ElasticDistortion(apply_distorsion=True, granularity=[0.2, 0.8], magnitude=[0.4, 1.6]),
    Random3AxisRotation(apply_rotation=True, rot_x=8, rot_y=8, rot_z=180),
    Random symmetry of axes: x=True, y=True, z=False,
    RandomScaleAnisotropic([0.9, 1.1]),
    GridSampling3D(grid_size=0.03, quantize_coords=True, mode=last),
    XYZFeature(axis=['z']),
    AddFeatsByKeys(pos_z=True, rgb=False, linearity=False, norm=False, planarity=False, scattering=False),
])
[0;95mval_transform [0m= Compose([
    GridSampling3D(grid_size=0.03, quantize_coords=True, mode=last),
    XYZFeature(axis=['z']),
    AddFeatsByKeys(pos_z=True, rgb=False, linearity=False, norm=False, planarity=False, scattering=False),
])
[0;95minference_transform [0m= Compose([
    SaveOriginalPosId,
    PCAComputePointwise(num_neighbors=50, r=None, use_full_pos=False, use_cuda=False, use_faiss=False, ncells=None, nprobes=10, chunk_size=1000000),
    EigenFeatures(norm=True, linearity=True, planarity=True, scattering=True, temperature=None),
    RemoveAttributes(attr_names=['eigenvalues', 'eigenvectors'], strict=False),
    GridSampling3D(grid_size=0.03, quantize_coords=True, mode=last),
    XYZFeature(axis=['z']),
    AddFeatsByKeys(pos_z=True, rgb=False, linearity=False, norm=False, planarity=False, scattering=False),
])
[0;95mpre_transform_image [0m= ComposeMultiModal([
    LoadImages(ref_size=[640, 480], crop_size=None, crop_offsets=None, downscale=None, show_progress=False),
    NonStaticMask(ref_size=(640, 480), proj_upscale=1, n_sample=5),
    MapImages(key=mapping_index, verbose=False, cylinder=False, ref_size=[640, 480], proj_upscale=1, method=SplattingVisibility, use_cuda=False, kwargs={'voxel': 0.03, 'r_max': 8, 'r_min': 0.05, 'exact': True, 'camera': 'scannet'}),
    NeighborhoodBasedMappingFeatures(k_list=[50], voxel=0.01, compute_density=True, compute_occlusion=True, use_faiss=False, use_cuda=False, ncells=None, nprobes=10, verbose=True),
])
[0;95mtest_transform_image [0m= ComposeMultiModal([
    SelectMappingFromPointId(key=mapping_index),
    ToImageData(),
    PickImagesFromMemoryCredit(credit=7680000, use_coverage=True, k_coverage=2),
    ToFloatImage(),
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
[0;95mtrain_transform_image [0m= ComposeMultiModal([
    SelectMappingFromPointId(key=mapping_index),
    ToImageData(),
    PickImagesFromMemoryCredit(credit=7680000, use_coverage=True, k_coverage=2),
    JitterMappingFeatures(sigma=0.02, clip=0.03),
    ColorJitter(brightness=[0.4, 1.6], contrast=[0.4, 1.6], saturation=[0.30000000000000004, 1.7], hue=None),
    RandomHorizontalFlip(p=0.5),
    ToFloatImage(),
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
[0;95mval_transform_image [0m= ComposeMultiModal([
    SelectMappingFromPointId(key=mapping_index),
    ToImageData(),
    PickImagesFromMemoryCredit(credit=7680000, use_coverage=True, k_coverage=2),
    ToFloatImage(),
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
[0;95minference_transform_image [0m= ComposeMultiModal([
    LoadImages(ref_size=[640, 480], crop_size=None, crop_offsets=None, downscale=None, show_progress=False),
    NonStaticMask(ref_size=(640, 480), proj_upscale=1, n_sample=5),
    MapImages(key=mapping_index, verbose=False, cylinder=False, ref_size=[640, 480], proj_upscale=1, method=SplattingVisibility, use_cuda=False, kwargs={'voxel': 0.03, 'r_max': 8, 'r_min': 0.05, 'exact': True, 'camera': 'scannet'}),
    NeighborhoodBasedMappingFeatures(k_list=[50], voxel=0.01, compute_density=True, compute_occlusion=True, use_faiss=False, use_cuda=False, ncells=None, nprobes=10, verbose=True),
    SelectMappingFromPointId(key=mapping_index),
    ToImageData(),
    PickImagesFromMemoryCredit(credit=7680000, use_coverage=True, k_coverage=2),
    ToFloatImage(),
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
Size of [0;95mtrain_dataset [0m= 1201
Size of [0;95mtest_dataset [0m= 0
Size of [0;95mval_dataset [0m= 312
[0;95mBatch size =[0m 4
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
[2022-10-13 11:27:34,811][torch_points3d.datasets.base_dataset][INFO] - Available stage selection datasets: [0;95m ['val'] [0m
[2022-10-13 11:27:34,811][torch_points3d.datasets.base_dataset][INFO] - The models will be selected using the metrics on following dataset: [0;95m val [0m
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
[2022-10-13 11:27:36,193][torch_points3d.trainer][INFO] - EPOCH 1 / 300
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
  0%|          | 0/301 [00:00<?, ?it/s]  0%|          | 0/301 [01:00<?, ?it/s] DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.
 DeepViewAgg/torch_points3d/datasets/segmentation/scannet.py,
        line 1118:
        (Feng) on my conda installation, this block causes a max 
        depth recursion error. Return super().indices() for now as it 
        results in no problems on my torch_geometric=1.7.2 and 
        pytorch=1.12.1 installation.

Error executing job with overrides: ['data=segmentation/multimodal/Feng/scannet-sparse', 'models=segmentation/multimodal/no3d', 'model_name=RGB_ResNet18PPM_g4-nomod_DeepSetFeat', 'task=segmentation', 'training=Feng/no3d', 'lr_scheduler=exponential', 'eval_frequency=10', 'data.dataroot=/project/fsun/dvata', 'training.cuda=0', 'training.batch_size=4', 'training.epochs=300', 'training.num_workers=4', 'training.optim.base_lr=0.1', 'training.wandb.log=False', 'training.wandb.name=Test_no3d', 'tracker_options.make_submission=False', 'training.checkpoint_dir=']
Traceback (most recent call last):
  File "train.py", line 23, in <module>
    main()
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/hydra/main.py", line 95, in decorated_main
    config_name=config_name,
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/hydra/_internal/utils.py", line 396, in _run_hydra
    overrides=overrides,
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/hydra/_internal/utils.py", line 453, in _run_app
    lambda: hydra.run(
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/hydra/_internal/utils.py", line 216, in run_and_report
    raise ex
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/hydra/_internal/utils.py", line 213, in run_and_report
    return func()
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/hydra/_internal/utils.py", line 456, in <lambda>
    overrides=overrides,
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "train.py", line 15, in main
    trainer.train()
  File "/home/fsun/DeepViewAgg/torch_points3d/trainer.py", line 150, in train
    self._train_epoch(epoch)
  File "/home/fsun/DeepViewAgg/torch_points3d/trainer.py", line 205, in _train_epoch
    self._model.optimize_parameters(epoch, self._dataset.batch_size)
  File "/home/fsun/DeepViewAgg/torch_points3d/models/base_model.py", line 245, in optimize_parameters
    self.forward(epoch=epoch)  # first call forward to calculate intermediate results
  File "/home/fsun/DeepViewAgg/torch_points3d/models/segmentation/multimodal/no3d.py", line 76, in forward
    data = self.backbone(self.input)
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fsun/DeepViewAgg/torch_points3d/applications/multimodal/no3d.py", line 105, in forward
    mm_data_dict = self.down_modules[i](mm_data_dict)
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fsun/DeepViewAgg/torch_points3d/modules/multimodal/modules.py", line 94, in forward
    mm_data_dict = mod_branch(mm_data_dict, m)
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fsun/DeepViewAgg/torch_points3d/modules/multimodal/modules.py", line 399, in forward
    mod_data = self.forward_conv(mod_data)
  File "/home/fsun/DeepViewAgg/torch_points3d/modules/multimodal/modules.py", line 468, in forward_conv
    mod_data[i].x = self.forward_conv(mod_data[i], i == 0).x
  File "/home/fsun/DeepViewAgg/torch_points3d/modules/multimodal/modules.py", line 479, in forward_conv
    mod_x = self.conv(mod_data.x, True)
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fsun/DeepViewAgg/torch_points3d/modules/multimodal/modalities/image.py", line 774, in forward
    pred = self.decoder(self.encoder(x, return_feature_maps=True),
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/mit_semseg/models/models.py", line 262, in forward
    x = self.layer2(x); conv_out.append(x);
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/mit_semseg/models/resnet.py", line 44, in forward
    out = self.conv2(out)
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 457, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/fsun/.conda/envs/pytorch3d/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 236.00 MiB (GPU 0; 23.65 GiB total capacity; 22.44 GiB already allocated; 25.44 MiB free; 22.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
