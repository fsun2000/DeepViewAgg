[2022-10-23 21:15:40,925][torch_points3d.trainer][INFO] - DEVICE : cuda
initialize train dataset
temporarily hard code N-views in get_view_dependent_features()
initialize val dataset
temporarily hard code N-views in get_view_dependent_features()
task:  segmentation.multimodal
tested_model_name:  MVFusion
Data(coords=[74664, 3], grid_size=[1], id_scan=[1], mapping_index=[74664], origin_id=[74664], pos=[74664, 3], x=[74664, 9, 10], x_seen_mask=[74664], y=[74664])
tensor(59982)
class_name:  MVFusion_model
model_module:  torch_points3d.models.segmentation.multimodal.Feng.mvfusion
opt:   {'class': 'Feng.mvfusion.MVFusion_model', 'down_conv': {'image': {'down_conv': {'module_name': 'ADE20KResNet18PPM', 'frozen': False}, 'atomic_pooling': {'module_name': 'BimodalCSRPool', 'mode': 'max'}, 'view_pooling': {'module_name': 'GroupBimodalCSRPool', 'in_map': 8, 'in_mod': 512, 'num_groups': 4, 'use_mod': False, 'gating': True, 'group_scaling': True, 'map_encoder': 'DeepSetFeat', 'use_num': True, 'pool': 'max', 'fusion': 'concatenation'}, 'fusion': {'module_name': 'BimodalFusion', 'mode': 'residual'}, 'drop_mod': 0.0, 'branching_index': 0}}, 'transformer': {'n_views': 9, 'in_map': 9, 'in_m2f': 20, 'embed_dim': 36, 'hidden_dim': 144, 'num_heads': 2, 'num_layers': 4, 'use_batch_norm': False, 'feat_downproj_dim': None, 'dropout': 0.0, 'mlp_dropout': 0.0, 'use_attn_mask': True, 'use_csr_mask': True, 'n_classes': 20}}
model:  MVFusion_model(
  (backbone): MVFusionEncoder(
    (down_modules): ModuleList(
      (0): MultimodalBlockDown(
        (block_1): Identity()
        (block_2): Identity()
        (image): UnimodalBranchOnlyAtomicPool(
          drop_3d=None
          drop_mod=None
          keep_last_view=False
          checkpointing=
          (atomic_pool): BimodalCSRPool()
        )
      )
    )
    (fusion): DVA_cls_5_fusion_7(
      (fusion): TransformerFusion(
        (input_layer): Linear(in_features=29, out_features=36, bias=True)
        (transformer_layers): ModuleList(
          (0): AttentionBlock(
            (norm_1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (norm_2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
            )
            (linear): Sequential(
              (0): Linear(in_features=36, out_features=144, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=144, out_features=36, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttentionBlock(
            (norm_1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (norm_2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
            )
            (linear): Sequential(
              (0): Linear(in_features=36, out_features=144, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=144, out_features=36, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttentionBlock(
            (norm_1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (norm_2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
            )
            (linear): Sequential(
              (0): Linear(in_features=36, out_features=144, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=144, out_features=36, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttentionBlock(
            (norm_1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (norm_2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
            )
            (linear): Sequential(
              (0): Linear(in_features=36, out_features=144, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=144, out_features=36, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (mlp_head): Sequential(
        (0): Dropout(p=0.0, inplace=False)
        (1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=36, out_features=80, bias=True)
        (3): Linear(in_features=80, out_features=20, bias=True)
      )
    )
  )
)
[2022-10-23 21:15:52,552][torch_points3d.core.schedulers.bn_schedulers][INFO] - Setting batchnorm momentum at 0.02
task:  segmentation.multimodal
tested_model_name:  MVFusion
[2022-10-23 21:15:52,729][torch_points3d.trainer][WARNING] - The model will not be able to be used from pretrained weights without the corresponding dataset. Current properties are {'feature_dimension': 9, 'num_classes': 20}
[2022-10-23 21:15:52,729][torch_points3d.trainer][INFO] - MVFusion_model(
  (backbone): MVFusionEncoder(
    (down_modules): ModuleList(
      (0): MultimodalBlockDown(
        (block_1): Identity()
        (block_2): Identity()
        (image): UnimodalBranchOnlyAtomicPool(
          drop_3d=None
          drop_mod=None
          keep_last_view=False
          checkpointing=
          (atomic_pool): BimodalCSRPool()
        )
      )
    )
    (fusion): DVA_cls_5_fusion_7(
      (fusion): TransformerFusion(
        (input_layer): Linear(in_features=29, out_features=36, bias=True)
        (transformer_layers): ModuleList(
          (0): AttentionBlock(
            (norm_1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (norm_2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
            )
            (linear): Sequential(
              (0): Linear(in_features=36, out_features=144, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=144, out_features=36, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttentionBlock(
            (norm_1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (norm_2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
            )
            (linear): Sequential(
              (0): Linear(in_features=36, out_features=144, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=144, out_features=36, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttentionBlock(
            (norm_1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (norm_2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
            )
            (linear): Sequential(
              (0): Linear(in_features=36, out_features=144, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=144, out_features=36, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttentionBlock(
            (norm_1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (norm_2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
            )
            (linear): Sequential(
              (0): Linear(in_features=36, out_features=144, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=144, out_features=36, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (mlp_head): Sequential(
        (0): Dropout(p=0.0, inplace=False)
        (1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=36, out_features=80, bias=True)
        (3): Linear(in_features=80, out_features=20, bias=True)
      )
    )
  )
)
[2022-10-23 21:15:52,730][torch_points3d.utils.colors][INFO] - [0;32mOptimizer: SGD (
Parameter Group 0
    dampening: 0.1
    foreach: None
    initial_lr: 0.1
    lr: 0.1
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001
)[0m
[2022-10-23 21:15:52,730][torch_points3d.utils.colors][INFO] - [0;32mLearning Rate Scheduler: ExponentialLR({'gamma': 0.9885}, update_scheduler_on=on_epoch)[0m
[2022-10-23 21:15:52,730][torch_points3d.utils.colors][INFO] - [0;32mBatchNorm Scheduler: BNMomentumScheduler(base_momentum: 0.02, update_scheduler_on=on_epoch)[0m
[2022-10-23 21:15:52,730][torch_points3d.utils.colors][INFO] - [0;32mAccumulated gradients: None[0m
[2022-10-23 21:15:52,730][torch_points3d.trainer][INFO] - Model size = 69848
[2022-10-23 21:15:52,731][torch_points3d.trainer][INFO] - Dataset: ScannetDatasetMM 
[0;95mtrain_pre_batch_collate_transform [0m= None
[0;95mval_pre_batch_collate_transform [0m= None
[0;95mtest_pre_batch_collate_transform [0m= None
[0;95mpre_transform [0m= Compose([
    SaveOriginalPosId,
    PCAComputePointwise(num_neighbors=50, r=None, use_full_pos=False, use_cuda=False, use_faiss=False, ncells=None, nprobes=10, chunk_size=1000000),
    EigenFeatures(norm=True, linearity=True, planarity=True, scattering=True, temperature=None),
    RemoveAttributes(attr_names=['eigenvalues', 'eigenvectors'], strict=False),
])
[0;95mtest_transform [0m= Compose([
    GridSampling3D(grid_size=0.03, quantize_coords=True, mode=last),
    XYZFeature(axis=['z']),
    AddFeatsByKeys(pos_z=True, rgb=False, linearity=False, norm=False, planarity=False, scattering=False),
])
[0;95mtrain_transform [0m= Compose([
    ElasticDistortion(apply_distorsion=True, granularity=[0.2, 0.8], magnitude=[0.4, 1.6]),
    Random3AxisRotation(apply_rotation=True, rot_x=8, rot_y=8, rot_z=180),
    Random symmetry of axes: x=True, y=True, z=False,
    RandomScaleAnisotropic([0.9, 1.1]),
    GridSampling3D(grid_size=0.03, quantize_coords=True, mode=last),
    XYZFeature(axis=['z']),
    AddFeatsByKeys(pos_z=True, rgb=False, linearity=False, norm=False, planarity=False, scattering=False),
])
[0;95mval_transform [0m= Compose([
    GridSampling3D(grid_size=0.03, quantize_coords=True, mode=last),
    XYZFeature(axis=['z']),
    AddFeatsByKeys(pos_z=True, rgb=False, linearity=False, norm=False, planarity=False, scattering=False),
])
[0;95minference_transform [0m= Compose([
    SaveOriginalPosId,
    PCAComputePointwise(num_neighbors=50, r=None, use_full_pos=False, use_cuda=False, use_faiss=False, ncells=None, nprobes=10, chunk_size=1000000),
    EigenFeatures(norm=True, linearity=True, planarity=True, scattering=True, temperature=None),
    RemoveAttributes(attr_names=['eigenvalues', 'eigenvectors'], strict=False),
    GridSampling3D(grid_size=0.03, quantize_coords=True, mode=last),
    XYZFeature(axis=['z']),
    AddFeatsByKeys(pos_z=True, rgb=False, linearity=False, norm=False, planarity=False, scattering=False),
])
[0;95mpre_transform_image [0m= ComposeMultiModal([
    LoadImages(ref_size=[320, 240], crop_size=None, crop_offsets=None, downscale=None, show_progress=False),
    NonStaticMask(ref_size=(320, 240), proj_upscale=1, n_sample=5),
    MapImages(key=mapping_index, verbose=False, cylinder=False, ref_size=[320, 240], proj_upscale=1, method=SplattingVisibility, use_cuda=False, kwargs={'voxel': 0.03, 'r_max': 8, 'r_min': 0.05, 'exact': True, 'camera': 'scannet'}),
    NeighborhoodBasedMappingFeatures(k_list=[50], voxel=0.01, compute_density=True, compute_occlusion=True, use_faiss=False, use_cuda=False, ncells=None, nprobes=10, verbose=True),
])
[0;95mtest_transform_image [0m= ComposeMultiModal([
    SelectMappingFromPointId(key=mapping_index),
    ToImageData(),
    PickImagesFromMemoryCredit(credit=7680000, use_coverage=True, k_coverage=2),
    ToFloatImage(),
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
[0;95mtrain_transform_image [0m= ComposeMultiModal([
    SelectMappingFromPointId(key=mapping_index),
    ToImageData(),
    PickImagesFromMemoryCredit(credit=7680000, use_coverage=True, k_coverage=2),
    JitterMappingFeatures(sigma=0.02, clip=0.03),
    ColorJitter(brightness=[0.4, 1.6], contrast=[0.4, 1.6], saturation=[0.30000000000000004, 1.7], hue=None),
    RandomHorizontalFlip(p=0.5),
    ToFloatImage(),
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
[0;95mval_transform_image [0m= ComposeMultiModal([
    SelectMappingFromPointId(key=mapping_index),
    ToImageData(),
    PickImagesFromMemoryCredit(credit=7680000, use_coverage=True, k_coverage=2),
    ToFloatImage(),
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
[0;95minference_transform_image [0m= ComposeMultiModal([
    LoadImages(ref_size=[320, 240], crop_size=None, crop_offsets=None, downscale=None, show_progress=False),
    NonStaticMask(ref_size=(320, 240), proj_upscale=1, n_sample=5),
    MapImages(key=mapping_index, verbose=False, cylinder=False, ref_size=[320, 240], proj_upscale=1, method=SplattingVisibility, use_cuda=False, kwargs={'voxel': 0.03, 'r_max': 8, 'r_min': 0.05, 'exact': True, 'camera': 'scannet'}),
    NeighborhoodBasedMappingFeatures(k_list=[50], voxel=0.01, compute_density=True, compute_occlusion=True, use_faiss=False, use_cuda=False, ncells=None, nprobes=10, verbose=True),
    SelectMappingFromPointId(key=mapping_index),
    ToImageData(),
    PickImagesFromMemoryCredit(credit=7680000, use_coverage=True, k_coverage=2),
    ToFloatImage(),
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
Size of [0;95mtrain_dataset [0m= 3
Size of [0;95mtest_dataset [0m= 0
Size of [0;95mval_dataset [0m= 3
[0;95mBatch size =[0m 3
Data(coords=[75762, 3], grid_size=[1], id_scan=[1], mapping_index=[75762], origin_id=[75762], pos=[75762, 3], x=[75762, 9, 10], x_seen_mask=[75762], y=[75762])
tensor(59978)
[2022-10-23 21:16:02,943][torch_points3d.datasets.base_dataset][INFO] - Available stage selection datasets: [0;95m ['val'] [0m
[2022-10-23 21:16:02,944][torch_points3d.datasets.base_dataset][INFO] - The models will be selected using the metrics on following dataset: [0;95m val [0m
[2022-10-23 21:16:05,150][torch_points3d.trainer][INFO] - EPOCH 1 / 100
  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[183832], coords=[183832, 3], grid_size=[3], id_scan=[3], mapping_index=[183832], origin_id=[183832], pos=[183832, 3], ptr=[4], x=[183832, 9, 10], x_seen_mask=[183832], y=[183832])
    image = ImageBatch(num_settings=1, num_views=299, num_points=183832, device=cpu)
)
x_seen_mask torch.Size([183832])
viewing_feats;  torch.Size([146910, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=183832, device=cpu), pos=[183832, 3], seen=[183832], x=[183832, 20])
seen_mask:  torch.Size([183832])
  0%|          | 0/1 [00:46<?, ?it/s, [0;92mdata_loading=45.29, iteration=1.535, train_acc=5.202, train_loss_seg=2.949, train_macc=8.110, train_miou=0.980[0m)]100%|##########| 1/1 [00:46<00:00, 46.83s/it, [0;92mdata_loading=45.29, iteration=1.535, train_acc=5.202, train_loss_seg=2.949, train_macc=8.110, train_miou=0.980[0m)]100%|##########| 1/1 [00:46<00:00, 46.83s/it, [0;92mdata_loading=45.29, iteration=1.535, train_acc=5.202, train_loss_seg=2.949, train_macc=8.110, train_miou=0.980[0m)][2022-10-23 21:16:51,999][torch_points3d.trainer][INFO] - Learning rate = 0.098850
[2022-10-23 21:16:52,000][torch_points3d.trainer][INFO] - EPOCH 2 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[193075], coords=[193075, 3], grid_size=[3], id_scan=[3], mapping_index=[193075], origin_id=[193075], pos=[193075, 3], ptr=[4], x=[193075, 9, 10], x_seen_mask=[193075], y=[193075])
    image = ImageBatch(num_settings=1, num_views=299, num_points=193075, device=cpu)
)
x_seen_mask torch.Size([193075])
viewing_feats;  torch.Size([153957, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=193075, device=cpu), pos=[193075, 3], seen=[193075], x=[193075, 20])
seen_mask:  torch.Size([193075])
  0%|          | 0/1 [00:44<?, ?it/s, [0;92mdata_loading=44.16, iteration=0.755, train_acc=38.82, train_loss_seg=2.176, train_macc=6.25 , train_miou=2.426[0m)]100%|##########| 1/1 [00:44<00:00, 44.92s/it, [0;92mdata_loading=44.16, iteration=0.755, train_acc=38.82, train_loss_seg=2.176, train_macc=6.25 , train_miou=2.426[0m)]100%|##########| 1/1 [00:44<00:00, 44.92s/it, [0;92mdata_loading=44.16, iteration=0.755, train_acc=38.82, train_loss_seg=2.176, train_macc=6.25 , train_miou=2.426[0m)][2022-10-23 21:17:36,942][torch_points3d.trainer][INFO] - Learning rate = 0.097713
[2022-10-23 21:17:36,942][torch_points3d.trainer][INFO] - EPOCH 3 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[194520], coords=[194520, 3], grid_size=[3], id_scan=[3], mapping_index=[194520], origin_id=[194520], pos=[194520, 3], ptr=[4], x=[194520, 9, 10], x_seen_mask=[194520], y=[194520])
    image = ImageBatch(num_settings=1, num_views=299, num_points=194520, device=cpu)
)
x_seen_mask torch.Size([194520])
viewing_feats;  torch.Size([154901, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=194520, device=cpu), pos=[194520, 3], seen=[194520], x=[194520, 20])
seen_mask:  torch.Size([194520])
  0%|          | 0/1 [00:45<?, ?it/s, [0;92mdata_loading=44.84, iteration=0.759, train_acc=40.30, train_loss_seg=2.052, train_macc=6.25 , train_miou=2.519[0m)]100%|##########| 1/1 [00:45<00:00, 45.60s/it, [0;92mdata_loading=44.84, iteration=0.759, train_acc=40.30, train_loss_seg=2.052, train_macc=6.25 , train_miou=2.519[0m)]100%|##########| 1/1 [00:45<00:00, 45.60s/it, [0;92mdata_loading=44.84, iteration=0.759, train_acc=40.30, train_loss_seg=2.052, train_macc=6.25 , train_miou=2.519[0m)][2022-10-23 21:18:22,564][torch_points3d.trainer][INFO] - Learning rate = 0.096590
[2022-10-23 21:18:22,564][torch_points3d.trainer][INFO] - EPOCH 4 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[192558], coords=[192558, 3], grid_size=[3], id_scan=[3], mapping_index=[192558], origin_id=[192558], pos=[192558, 3], ptr=[4], x=[192558, 9, 10], x_seen_mask=[192558], y=[192558])
    image = ImageBatch(num_settings=1, num_views=299, num_points=192558, device=cpu)
)
x_seen_mask torch.Size([192558])
viewing_feats;  torch.Size([149472, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=192558, device=cpu), pos=[192558, 3], seen=[192558], x=[192558, 20])
seen_mask:  torch.Size([192558])
  0%|          | 0/1 [00:45<?, ?it/s, [0;92mdata_loading=44.31, iteration=0.740, train_acc=54.58, train_loss_seg=2.083, train_macc=10.74, train_miou=6.733[0m)]100%|##########| 1/1 [00:45<00:00, 45.06s/it, [0;92mdata_loading=44.31, iteration=0.740, train_acc=54.58, train_loss_seg=2.083, train_macc=10.74, train_miou=6.733[0m)]100%|##########| 1/1 [00:45<00:00, 45.06s/it, [0;92mdata_loading=44.31, iteration=0.740, train_acc=54.58, train_loss_seg=2.083, train_macc=10.74, train_miou=6.733[0m)][2022-10-23 21:19:07,641][torch_points3d.trainer][INFO] - Learning rate = 0.095479
[2022-10-23 21:19:07,642][torch_points3d.trainer][INFO] - EPOCH 5 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[196092], coords=[196092, 3], grid_size=[3], id_scan=[3], mapping_index=[196092], origin_id=[196092], pos=[196092, 3], ptr=[4], x=[196092, 9, 10], x_seen_mask=[196092], y=[196092])
    image = ImageBatch(num_settings=1, num_views=299, num_points=196092, device=cpu)
)
x_seen_mask torch.Size([196092])
viewing_feats;  torch.Size([155249, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=196092, device=cpu), pos=[196092, 3], seen=[196092], x=[196092, 20])
seen_mask:  torch.Size([196092])
  0%|          | 0/1 [00:45<?, ?it/s, [0;92mdata_loading=44.72, iteration=0.758, train_acc=57.53, train_loss_seg=1.899, train_macc=11.20, train_miou=7.027[0m)]100%|##########| 1/1 [00:45<00:00, 45.48s/it, [0;92mdata_loading=44.72, iteration=0.758, train_acc=57.53, train_loss_seg=1.899, train_macc=11.20, train_miou=7.027[0m)]100%|##########| 1/1 [00:45<00:00, 45.48s/it, [0;92mdata_loading=44.72, iteration=0.758, train_acc=57.53, train_loss_seg=1.899, train_macc=11.20, train_miou=7.027[0m)][2022-10-23 21:19:53,143][torch_points3d.trainer][INFO] - Learning rate = 0.094381
[2022-10-23 21:19:53,143][torch_points3d.trainer][INFO] - EPOCH 6 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[197355], coords=[197355, 3], grid_size=[3], id_scan=[3], mapping_index=[197355], origin_id=[197355], pos=[197355, 3], ptr=[4], x=[197355, 9, 10], x_seen_mask=[197355], y=[197355])
    image = ImageBatch(num_settings=1, num_views=299, num_points=197355, device=cpu)
)
x_seen_mask torch.Size([197355])
viewing_feats;  torch.Size([155491, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=197355, device=cpu), pos=[197355, 3], seen=[197355], x=[197355, 20])
seen_mask:  torch.Size([197355])
  0%|          | 0/1 [00:45<?, ?it/s, [0;92mdata_loading=44.66, iteration=0.764, train_acc=39.80, train_loss_seg=1.940, train_macc=6.25 , train_miou=2.487[0m)]100%|##########| 1/1 [00:45<00:00, 45.43s/it, [0;92mdata_loading=44.66, iteration=0.764, train_acc=39.80, train_loss_seg=1.940, train_macc=6.25 , train_miou=2.487[0m)]100%|##########| 1/1 [00:45<00:00, 45.43s/it, [0;92mdata_loading=44.66, iteration=0.764, train_acc=39.80, train_loss_seg=1.940, train_macc=6.25 , train_miou=2.487[0m)][2022-10-23 21:20:38,592][torch_points3d.trainer][INFO] - Learning rate = 0.093295
[2022-10-23 21:20:38,593][torch_points3d.trainer][INFO] - EPOCH 7 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[192865], coords=[192865, 3], grid_size=[3], id_scan=[3], mapping_index=[192865], origin_id=[192865], pos=[192865, 3], ptr=[4], x=[192865, 9, 10], x_seen_mask=[192865], y=[192865])
    image = ImageBatch(num_settings=1, num_views=299, num_points=192865, device=cpu)
)
x_seen_mask torch.Size([192865])
viewing_feats;  torch.Size([152495, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=192865, device=cpu), pos=[192865, 3], seen=[192865], x=[192865, 20])
seen_mask:  torch.Size([192865])
  0%|          | 0/1 [00:45<?, ?it/s, [0;92mdata_loading=44.28, iteration=0.754, train_acc=39.81, train_loss_seg=1.856, train_macc=6.25 , train_miou=2.488[0m)]100%|##########| 1/1 [00:45<00:00, 45.04s/it, [0;92mdata_loading=44.28, iteration=0.754, train_acc=39.81, train_loss_seg=1.856, train_macc=6.25 , train_miou=2.488[0m)]100%|##########| 1/1 [00:45<00:00, 45.04s/it, [0;92mdata_loading=44.28, iteration=0.754, train_acc=39.81, train_loss_seg=1.856, train_macc=6.25 , train_miou=2.488[0m)][2022-10-23 21:21:23,648][torch_points3d.trainer][INFO] - Learning rate = 0.092222
[2022-10-23 21:21:23,648][torch_points3d.trainer][INFO] - EPOCH 8 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[190962], coords=[190962, 3], grid_size=[3], id_scan=[3], mapping_index=[190962], origin_id=[190962], pos=[190962, 3], ptr=[4], x=[190962, 9, 10], x_seen_mask=[190962], y=[190962])
    image = ImageBatch(num_settings=1, num_views=299, num_points=190962, device=cpu)
)
x_seen_mask torch.Size([190962])
viewing_feats;  torch.Size([151288, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=190962, device=cpu), pos=[190962, 3], seen=[190962], x=[190962, 20])
seen_mask:  torch.Size([190962])
  0%|          | 0/1 [00:42<?, ?it/s, [0;92mdata_loading=41.94, iteration=0.740, train_acc=60.56, train_loss_seg=1.564, train_macc=11.49, train_miou=7.148[0m)]100%|##########| 1/1 [00:42<00:00, 42.68s/it, [0;92mdata_loading=41.94, iteration=0.740, train_acc=60.56, train_loss_seg=1.564, train_macc=11.49, train_miou=7.148[0m)]100%|##########| 1/1 [00:42<00:00, 42.68s/it, [0;92mdata_loading=41.94, iteration=0.740, train_acc=60.56, train_loss_seg=1.564, train_macc=11.49, train_miou=7.148[0m)][2022-10-23 21:22:06,346][torch_points3d.trainer][INFO] - Learning rate = 0.091162
[2022-10-23 21:22:06,346][torch_points3d.trainer][INFO] - EPOCH 9 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[185735], coords=[185735, 3], grid_size=[3], id_scan=[3], mapping_index=[185735], origin_id=[185735], pos=[185735, 3], ptr=[4], x=[185735, 9, 10], x_seen_mask=[185735], y=[185735])
    image = ImageBatch(num_settings=1, num_views=299, num_points=185735, device=cpu)
)
x_seen_mask torch.Size([185735])
viewing_feats;  torch.Size([148396, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=185735, device=cpu), pos=[185735, 3], seen=[185735], x=[185735, 20])
seen_mask:  torch.Size([185735])
  0%|          | 0/1 [00:41<?, ?it/s, [0;92mdata_loading=40.40, iteration=0.714, train_acc=50.97, train_loss_seg=1.718, train_macc=10.25, train_miou=5.989[0m)]100%|##########| 1/1 [00:41<00:00, 41.12s/it, [0;92mdata_loading=40.40, iteration=0.714, train_acc=50.97, train_loss_seg=1.718, train_macc=10.25, train_miou=5.989[0m)]100%|##########| 1/1 [00:41<00:00, 41.12s/it, [0;92mdata_loading=40.40, iteration=0.714, train_acc=50.97, train_loss_seg=1.718, train_macc=10.25, train_miou=5.989[0m)][2022-10-23 21:22:47,479][torch_points3d.trainer][INFO] - Learning rate = 0.090114
[2022-10-23 21:22:47,480][torch_points3d.trainer][INFO] - EPOCH 10 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[192089], coords=[192089, 3], grid_size=[3], id_scan=[3], mapping_index=[192089], origin_id=[192089], pos=[192089, 3], ptr=[4], x=[192089, 9, 10], x_seen_mask=[192089], y=[192089])
    image = ImageBatch(num_settings=1, num_views=299, num_points=192089, device=cpu)
)
x_seen_mask torch.Size([192089])
viewing_feats;  torch.Size([150531, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=192089, device=cpu), pos=[192089, 3], seen=[192089], x=[192089, 20])
seen_mask:  torch.Size([192089])
  0%|          | 0/1 [00:40<?, ?it/s, [0;92mdata_loading=39.96, iteration=0.735, train_acc=58.43, train_loss_seg=2.131, train_macc=10.79, train_miou=6.702[0m)]100%|##########| 1/1 [00:40<00:00, 40.70s/it, [0;92mdata_loading=39.96, iteration=0.735, train_acc=58.43, train_loss_seg=2.131, train_macc=10.79, train_miou=6.702[0m)]100%|##########| 1/1 [00:40<00:00, 40.70s/it, [0;92mdata_loading=39.96, iteration=0.735, train_acc=58.43, train_loss_seg=2.131, train_macc=10.79, train_miou=6.702[0m)][2022-10-23 21:23:28,200][torch_points3d.trainer][INFO] - Learning rate = 0.089077

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[278603], coords=[278603, 3], grid_size=[3], id_scan=[3], mapping_index=[278603], origin_id=[278603], pos=[278603, 3], ptr=[4], x=[278603, 9, 10], x_seen_mask=[278603], y=[278603])
    image = ImageBatch(num_settings=1, num_views=300, num_points=278603, device=cpu)
)
x_seen_mask torch.Size([278603])
viewing_feats;  torch.Size([207083, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=300, num_points=278603, device=cpu), pos=[278603, 3], seen=[278603], x=[278603, 20])
seen_mask:  torch.Size([278603])
  0%|          | 0/1 [00:50<?, ?it/s, [0;93mval_acc=20.85, val_loss_seg=3.215, val_macc=9.090, val_miou=1.896[0m)]100%|##########| 1/1 [00:50<00:00, 50.82s/it, [0;93mval_acc=20.85, val_loss_seg=3.215, val_macc=9.090, val_miou=1.896[0m)]100%|##########| 1/1 [00:50<00:00, 50.82s/it, [0;93mval_acc=20.85, val_loss_seg=3.215, val_macc=9.090, val_miou=1.896[0m)][2022-10-23 21:24:19,051][torch_points3d.metrics.base_tracker][INFO] - ==================================================
[2022-10-23 21:24:19,051][torch_points3d.metrics.base_tracker][INFO] -     val_loss_seg = 3.2158164978027344
[2022-10-23 21:24:19,051][torch_points3d.metrics.base_tracker][INFO] -     val_acc = 20.858104572739894
[2022-10-23 21:24:19,051][torch_points3d.metrics.base_tracker][INFO] -     val_macc = 9.090909090909092
[2022-10-23 21:24:19,051][torch_points3d.metrics.base_tracker][INFO] -     val_miou = 1.8961923247944572
[2022-10-23 21:24:19,051][torch_points3d.metrics.base_tracker][INFO] -     val_miou_per_class = {0: '20.86', 1: '0.00', 2: '0.00', 3: '0.00', 4: '0.00', 5: '0.00', 6: '0.00', 7: '0.00', 8: '0.00', 9: '0.00', 10: '0.00', 11: '0.00', 12: '0.00', 13: '0.00', 14: '0.00', 15: '0.00', 16: '0.00', 17: '0.00', 18: '0.00', 19: '0.00'}
[2022-10-23 21:24:19,051][torch_points3d.metrics.base_tracker][INFO] - ==================================================
[2022-10-23 21:24:19,051][torch_points3d.trainer][INFO] - EPOCH 11 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[187190], coords=[187190, 3], grid_size=[3], id_scan=[3], mapping_index=[187190], origin_id=[187190], pos=[187190, 3], ptr=[4], x=[187190, 9, 10], x_seen_mask=[187190], y=[187190])
    image = ImageBatch(num_settings=1, num_views=299, num_points=187190, device=cpu)
)
x_seen_mask torch.Size([187190])
viewing_feats;  torch.Size([146906, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=187190, device=cpu), pos=[187190, 3], seen=[187190], x=[187190, 20])
seen_mask:  torch.Size([187190])
  0%|          | 0/1 [00:40<?, ?it/s, [0;92mdata_loading=40.11, iteration=0.714, train_acc=25.96, train_loss_seg=3.049, train_macc=6.25 , train_miou=1.622[0m)]100%|##########| 1/1 [00:40<00:00, 40.83s/it, [0;92mdata_loading=40.11, iteration=0.714, train_acc=25.96, train_loss_seg=3.049, train_macc=6.25 , train_miou=1.622[0m)]100%|##########| 1/1 [00:40<00:00, 40.83s/it, [0;92mdata_loading=40.11, iteration=0.714, train_acc=25.96, train_loss_seg=3.049, train_macc=6.25 , train_miou=1.622[0m)][2022-10-23 21:24:59,900][torch_points3d.trainer][INFO] - Learning rate = 0.088053
[2022-10-23 21:24:59,900][torch_points3d.trainer][INFO] - EPOCH 12 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[196018], coords=[196018, 3], grid_size=[3], id_scan=[3], mapping_index=[196018], origin_id=[196018], pos=[196018, 3], ptr=[4], x=[196018, 9, 10], x_seen_mask=[196018], y=[196018])
    image = ImageBatch(num_settings=1, num_views=299, num_points=196018, device=cpu)
)
x_seen_mask torch.Size([196018])
viewing_feats;  torch.Size([154952, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=196018, device=cpu), pos=[196018, 3], seen=[196018], x=[196018, 20])
seen_mask:  torch.Size([196018])
  0%|          | 0/1 [00:41<?, ?it/s, [0;92mdata_loading=40.73, iteration=0.761, train_acc=33.02, train_loss_seg=1.911, train_macc=7.410, train_miou=2.894[0m)]100%|##########| 1/1 [00:41<00:00, 41.49s/it, [0;92mdata_loading=40.73, iteration=0.761, train_acc=33.02, train_loss_seg=1.911, train_macc=7.410, train_miou=2.894[0m)]100%|##########| 1/1 [00:41<00:00, 41.49s/it, [0;92mdata_loading=40.73, iteration=0.761, train_acc=33.02, train_loss_seg=1.911, train_macc=7.410, train_miou=2.894[0m)][2022-10-23 21:25:41,415][torch_points3d.trainer][INFO] - Learning rate = 0.087040
[2022-10-23 21:25:41,415][torch_points3d.trainer][INFO] - EPOCH 13 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[195809], coords=[195809, 3], grid_size=[3], id_scan=[3], mapping_index=[195809], origin_id=[195809], pos=[195809, 3], ptr=[4], x=[195809, 9, 10], x_seen_mask=[195809], y=[195809])
    image = ImageBatch(num_settings=1, num_views=299, num_points=195809, device=cpu)
)
x_seen_mask torch.Size([195809])
viewing_feats;  torch.Size([152570, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=195809, device=cpu), pos=[195809, 3], seen=[195809], x=[195809, 20])
seen_mask:  torch.Size([195809])
  0%|          | 0/1 [00:41<?, ?it/s, [0;92mdata_loading=40.96, iteration=0.729, train_acc=40.48, train_loss_seg=2.728, train_macc=6.25 , train_miou=2.530[0m)]100%|##########| 1/1 [00:41<00:00, 41.69s/it, [0;92mdata_loading=40.96, iteration=0.729, train_acc=40.48, train_loss_seg=2.728, train_macc=6.25 , train_miou=2.530[0m)]100%|##########| 1/1 [00:41<00:00, 41.69s/it, [0;92mdata_loading=40.96, iteration=0.729, train_acc=40.48, train_loss_seg=2.728, train_macc=6.25 , train_miou=2.530[0m)][2022-10-23 21:26:23,132][torch_points3d.trainer][INFO] - Learning rate = 0.086039
[2022-10-23 21:26:23,132][torch_points3d.trainer][INFO] - EPOCH 14 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[191473], coords=[191473, 3], grid_size=[3], id_scan=[3], mapping_index=[191473], origin_id=[191473], pos=[191473, 3], ptr=[4], x=[191473, 9, 10], x_seen_mask=[191473], y=[191473])
    image = ImageBatch(num_settings=1, num_views=299, num_points=191473, device=cpu)
)
x_seen_mask torch.Size([191473])
viewing_feats;  torch.Size([151748, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=191473, device=cpu), pos=[191473, 3], seen=[191473], x=[191473, 20])
seen_mask:  torch.Size([191473])
  0%|          | 0/1 [00:40<?, ?it/s, [0;92mdata_loading=39.93, iteration=0.736, train_acc=39.35, train_loss_seg=2.086, train_macc=6.25 , train_miou=2.459[0m)]100%|##########| 1/1 [00:40<00:00, 40.68s/it, [0;92mdata_loading=39.93, iteration=0.736, train_acc=39.35, train_loss_seg=2.086, train_macc=6.25 , train_miou=2.459[0m)]100%|##########| 1/1 [00:40<00:00, 40.68s/it, [0;92mdata_loading=39.93, iteration=0.736, train_acc=39.35, train_loss_seg=2.086, train_macc=6.25 , train_miou=2.459[0m)][2022-10-23 21:27:03,831][torch_points3d.trainer][INFO] - Learning rate = 0.085050
[2022-10-23 21:27:03,831][torch_points3d.trainer][INFO] - EPOCH 15 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[191035], coords=[191035, 3], grid_size=[3], id_scan=[3], mapping_index=[191035], origin_id=[191035], pos=[191035, 3], ptr=[4], x=[191035, 9, 10], x_seen_mask=[191035], y=[191035])
    image = ImageBatch(num_settings=1, num_views=299, num_points=191035, device=cpu)
)
x_seen_mask torch.Size([191035])
viewing_feats;  torch.Size([149785, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=191035, device=cpu), pos=[191035, 3], seen=[191035], x=[191035, 20])
seen_mask:  torch.Size([191035])
  0%|          | 0/1 [00:41<?, ?it/s, [0;92mdata_loading=40.43, iteration=0.732, train_acc=25.57, train_loss_seg=2.086, train_macc=6.25 , train_miou=1.598[0m)]100%|##########| 1/1 [00:41<00:00, 41.16s/it, [0;92mdata_loading=40.43, iteration=0.732, train_acc=25.57, train_loss_seg=2.086, train_macc=6.25 , train_miou=1.598[0m)]100%|##########| 1/1 [00:41<00:00, 41.16s/it, [0;92mdata_loading=40.43, iteration=0.732, train_acc=25.57, train_loss_seg=2.086, train_macc=6.25 , train_miou=1.598[0m)][2022-10-23 21:27:45,021][torch_points3d.trainer][INFO] - Learning rate = 0.084072
[2022-10-23 21:27:45,021][torch_points3d.trainer][INFO] - EPOCH 16 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[205874], coords=[205874, 3], grid_size=[3], id_scan=[3], mapping_index=[205874], origin_id=[205874], pos=[205874, 3], ptr=[4], x=[205874, 9, 10], x_seen_mask=[205874], y=[205874])
    image = ImageBatch(num_settings=1, num_views=299, num_points=205874, device=cpu)
)
x_seen_mask torch.Size([205874])
viewing_feats;  torch.Size([163704, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=205874, device=cpu), pos=[205874, 3], seen=[205874], x=[205874, 20])
seen_mask:  torch.Size([205874])
  0%|          | 0/1 [00:42<?, ?it/s, [0;92mdata_loading=41.31, iteration=0.797, train_acc=26.30, train_loss_seg=2.377, train_macc=6.25 , train_miou=1.644[0m)]100%|##########| 1/1 [00:42<00:00, 42.12s/it, [0;92mdata_loading=41.31, iteration=0.797, train_acc=26.30, train_loss_seg=2.377, train_macc=6.25 , train_miou=1.644[0m)]100%|##########| 1/1 [00:42<00:00, 42.12s/it, [0;92mdata_loading=41.31, iteration=0.797, train_acc=26.30, train_loss_seg=2.377, train_macc=6.25 , train_miou=1.644[0m)][2022-10-23 21:28:27,159][torch_points3d.trainer][INFO] - Learning rate = 0.083105
[2022-10-23 21:28:27,160][torch_points3d.trainer][INFO] - EPOCH 17 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[205359], coords=[205359, 3], grid_size=[3], id_scan=[3], mapping_index=[205359], origin_id=[205359], pos=[205359, 3], ptr=[4], x=[205359, 9, 10], x_seen_mask=[205359], y=[205359])
    image = ImageBatch(num_settings=1, num_views=299, num_points=205359, device=cpu)
)
x_seen_mask torch.Size([205359])
viewing_feats;  torch.Size([160849, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=205359, device=cpu), pos=[205359, 3], seen=[205359], x=[205359, 20])
seen_mask:  torch.Size([205359])
  0%|          | 0/1 [00:42<?, ?it/s, [0;92mdata_loading=41.56, iteration=0.769, train_acc=24.73, train_loss_seg=2.365, train_macc=6.25 , train_miou=1.545[0m)]100%|##########| 1/1 [00:42<00:00, 42.33s/it, [0;92mdata_loading=41.56, iteration=0.769, train_acc=24.73, train_loss_seg=2.365, train_macc=6.25 , train_miou=1.545[0m)]100%|##########| 1/1 [00:42<00:00, 42.33s/it, [0;92mdata_loading=41.56, iteration=0.769, train_acc=24.73, train_loss_seg=2.365, train_macc=6.25 , train_miou=1.545[0m)][2022-10-23 21:29:09,512][torch_points3d.trainer][INFO] - Learning rate = 0.082149
[2022-10-23 21:29:09,513][torch_points3d.trainer][INFO] - EPOCH 18 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[189941], coords=[189941, 3], grid_size=[3], id_scan=[3], mapping_index=[189941], origin_id=[189941], pos=[189941, 3], ptr=[4], x=[189941, 9, 10], x_seen_mask=[189941], y=[189941])
    image = ImageBatch(num_settings=1, num_views=299, num_points=189941, device=cpu)
)
x_seen_mask torch.Size([189941])
viewing_feats;  torch.Size([149112, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=189941, device=cpu), pos=[189941, 3], seen=[189941], x=[189941, 20])
seen_mask:  torch.Size([189941])
  0%|          | 0/1 [00:40<?, ?it/s, [0;92mdata_loading=40.10, iteration=0.717, train_acc=25.53, train_loss_seg=2.143, train_macc=6.25 , train_miou=1.596[0m)]100%|##########| 1/1 [00:40<00:00, 40.82s/it, [0;92mdata_loading=40.10, iteration=0.717, train_acc=25.53, train_loss_seg=2.143, train_macc=6.25 , train_miou=1.596[0m)]100%|##########| 1/1 [00:40<00:00, 40.82s/it, [0;92mdata_loading=40.10, iteration=0.717, train_acc=25.53, train_loss_seg=2.143, train_macc=6.25 , train_miou=1.596[0m)][2022-10-23 21:29:50,358][torch_points3d.trainer][INFO] - Learning rate = 0.081205
[2022-10-23 21:29:50,358][torch_points3d.trainer][INFO] - EPOCH 19 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[191780], coords=[191780, 3], grid_size=[3], id_scan=[3], mapping_index=[191780], origin_id=[191780], pos=[191780, 3], ptr=[4], x=[191780, 9, 10], x_seen_mask=[191780], y=[191780])
    image = ImageBatch(num_settings=1, num_views=299, num_points=191780, device=cpu)
)
x_seen_mask torch.Size([191780])
viewing_feats;  torch.Size([151406, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=191780, device=cpu), pos=[191780, 3], seen=[191780], x=[191780, 20])
seen_mask:  torch.Size([191780])
  0%|          | 0/1 [00:41<?, ?it/s, [0;92mdata_loading=40.31, iteration=0.725, train_acc=25.82, train_loss_seg=1.951, train_macc=6.25 , train_miou=1.614[0m)]100%|##########| 1/1 [00:41<00:00, 41.04s/it, [0;92mdata_loading=40.31, iteration=0.725, train_acc=25.82, train_loss_seg=1.951, train_macc=6.25 , train_miou=1.614[0m)]100%|##########| 1/1 [00:41<00:00, 41.04s/it, [0;92mdata_loading=40.31, iteration=0.725, train_acc=25.82, train_loss_seg=1.951, train_macc=6.25 , train_miou=1.614[0m)][2022-10-23 21:30:31,421][torch_points3d.trainer][INFO] - Learning rate = 0.080271
[2022-10-23 21:30:31,422][torch_points3d.trainer][INFO] - EPOCH 20 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[204738], coords=[204738, 3], grid_size=[3], id_scan=[3], mapping_index=[204738], origin_id=[204738], pos=[204738, 3], ptr=[4], x=[204738, 9, 10], x_seen_mask=[204738], y=[204738])
    image = ImageBatch(num_settings=1, num_views=299, num_points=204738, device=cpu)
)
x_seen_mask torch.Size([204738])
viewing_feats;  torch.Size([161164, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=204738, device=cpu), pos=[204738, 3], seen=[204738], x=[204738, 20])
seen_mask:  torch.Size([204738])
  0%|          | 0/1 [00:41<?, ?it/s, [0;92mdata_loading=41.07, iteration=0.777, train_acc=45.24, train_loss_seg=1.864, train_macc=9.244, train_miou=4.958[0m)]100%|##########| 1/1 [00:41<00:00, 41.85s/it, [0;92mdata_loading=41.07, iteration=0.777, train_acc=45.24, train_loss_seg=1.864, train_macc=9.244, train_miou=4.958[0m)]100%|##########| 1/1 [00:41<00:00, 41.86s/it, [0;92mdata_loading=41.07, iteration=0.777, train_acc=45.24, train_loss_seg=1.864, train_macc=9.244, train_miou=4.958[0m)][2022-10-23 21:31:13,299][torch_points3d.trainer][INFO] - Learning rate = 0.079348

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[278603], coords=[278603, 3], grid_size=[3], id_scan=[3], mapping_index=[278603], origin_id=[278603], pos=[278603, 3], ptr=[4], x=[278603, 9, 10], x_seen_mask=[278603], y=[278603])
    image = ImageBatch(num_settings=1, num_views=300, num_points=278603, device=cpu)
)
x_seen_mask torch.Size([278603])
viewing_feats;  torch.Size([206170, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=300, num_points=278603, device=cpu), pos=[278603, 3], seen=[278603], x=[278603, 20])
seen_mask:  torch.Size([278603])
  0%|          | 0/1 [00:49<?, ?it/s, [0;93mval_acc=40.05, val_loss_seg=2.231, val_macc=10.70, val_miou=5.041[0m)]100%|##########| 1/1 [00:49<00:00, 49.99s/it, [0;93mval_acc=40.05, val_loss_seg=2.231, val_macc=10.70, val_miou=5.041[0m)]100%|##########| 1/1 [00:49<00:00, 49.99s/it, [0;93mval_acc=40.05, val_loss_seg=2.231, val_macc=10.70, val_miou=5.041[0m)][2022-10-23 21:32:03,296][torch_points3d.utils.colors][INFO] - [0;94macc: 20.858104572739894 -> 40.05437693099897, macc: 9.090909090909092 -> 10.703246317326256, miou: 1.8961923247944572 -> 5.0413698063502235[0m
[2022-10-23 21:32:03,314][torch_points3d.metrics.base_tracker][INFO] - ==================================================
[2022-10-23 21:32:03,314][torch_points3d.metrics.base_tracker][INFO] -     val_loss_seg = 2.2312726974487305
[2022-10-23 21:32:03,314][torch_points3d.metrics.base_tracker][INFO] -     val_acc = 40.05437693099897
[2022-10-23 21:32:03,314][torch_points3d.metrics.base_tracker][INFO] -     val_macc = 10.703246317326256
[2022-10-23 21:32:03,314][torch_points3d.metrics.base_tracker][INFO] -     val_miou = 5.0413698063502235
[2022-10-23 21:32:03,314][torch_points3d.metrics.base_tracker][INFO] -     val_miou_per_class = {0: '17.67', 1: '37.78', 2: '0.00', 3: '0.00', 4: '0.00', 5: '0.00', 6: '0.00', 7: '0.00', 8: '0.00', 9: '0.00', 10: '0.00', 11: '0.00', 12: '0.00', 13: '0.00', 14: '0.00', 15: '0.00', 16: '0.00', 17: '0.00', 18: '0.00', 19: '0.00'}
[2022-10-23 21:32:03,314][torch_points3d.metrics.base_tracker][INFO] - ==================================================
[2022-10-23 21:32:03,314][torch_points3d.trainer][INFO] - EPOCH 21 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[189636], coords=[189636, 3], grid_size=[3], id_scan=[3], mapping_index=[189636], origin_id=[189636], pos=[189636, 3], ptr=[4], x=[189636, 9, 10], x_seen_mask=[189636], y=[189636])
    image = ImageBatch(num_settings=1, num_views=299, num_points=189636, device=cpu)
)
x_seen_mask torch.Size([189636])
viewing_feats;  torch.Size([151308, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=189636, device=cpu), pos=[189636, 3], seen=[189636], x=[189636, 20])
seen_mask:  torch.Size([189636])
  0%|          | 0/1 [00:41<?, ?it/s, [0;92mdata_loading=40.33, iteration=0.743, train_acc=42.27, train_loss_seg=1.844, train_macc=6.881, train_miou=3.178[0m)]100%|##########| 1/1 [00:41<00:00, 41.08s/it, [0;92mdata_loading=40.33, iteration=0.743, train_acc=42.27, train_loss_seg=1.844, train_macc=6.881, train_miou=3.178[0m)]100%|##########| 1/1 [00:41<00:00, 41.08s/it, [0;92mdata_loading=40.33, iteration=0.743, train_acc=42.27, train_loss_seg=1.844, train_macc=6.881, train_miou=3.178[0m)][2022-10-23 21:32:44,411][torch_points3d.trainer][INFO] - Learning rate = 0.078435
[2022-10-23 21:32:44,412][torch_points3d.trainer][INFO] - EPOCH 22 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[193706], coords=[193706, 3], grid_size=[3], id_scan=[3], mapping_index=[193706], origin_id=[193706], pos=[193706, 3], ptr=[4], x=[193706, 9, 10], x_seen_mask=[193706], y=[193706])
    image = ImageBatch(num_settings=1, num_views=299, num_points=193706, device=cpu)
)
x_seen_mask torch.Size([193706])
viewing_feats;  torch.Size([150665, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=193706, device=cpu), pos=[193706, 3], seen=[193706], x=[193706, 20])
seen_mask:  torch.Size([193706])
  0%|          | 0/1 [00:41<?, ?it/s, [0;92mdata_loading=40.69, iteration=0.741, train_acc=39.18, train_loss_seg=1.846, train_macc=6.25 , train_miou=2.448[0m)]100%|##########| 1/1 [00:41<00:00, 41.43s/it, [0;92mdata_loading=40.69, iteration=0.741, train_acc=39.18, train_loss_seg=1.846, train_macc=6.25 , train_miou=2.448[0m)]100%|##########| 1/1 [00:41<00:00, 41.43s/it, [0;92mdata_loading=40.69, iteration=0.741, train_acc=39.18, train_loss_seg=1.846, train_macc=6.25 , train_miou=2.448[0m)][2022-10-23 21:33:25,867][torch_points3d.trainer][INFO] - Learning rate = 0.077533
[2022-10-23 21:33:25,867][torch_points3d.trainer][INFO] - EPOCH 23 / 100

  0%|          | 0/1 [00:00<?, ?it/s]data inside mvfusion application:  MMBatch(
    data = Batch(batch=[187425], coords=[187425, 3], grid_size=[3], id_scan=[3], mapping_index=[187425], origin_id=[187425], pos=[187425, 3], ptr=[4], x=[187425, 9, 10], x_seen_mask=[187425], y=[187425])
    image = ImageBatch(num_settings=1, num_views=299, num_points=187425, device=cpu)
)
x_seen_mask torch.Size([187425])
viewing_feats;  torch.Size([147924, 9, 9])
THE TWO SEEN MASKS ARE EQUAL:  tensor([True, True, True,  ..., True, True, True])
data in mvfusion model:  Batch(image=ImageBatch(num_settings=1, num_views=299, num_points=187425, device=cpu), pos=[187425, 3], seen=[187425], x=[187425, 20])
seen_mask:  torch.Size([187425])
  0%|          | 0/1 [00:40<?, ?it/s, [0;92mdata_loading=39.33, iteration=0.72 , train_acc=39.46, train_loss_seg=1.800, train_macc=6.25 , train_miou=2.466[0m)]100%|##########| 1/1 [00:40<00:00, 40.06s/it, [0;92mdata_loading=39.33, iteration=0.72 , train_acc=39.46, train_loss_seg=1.800, train_macc=6.25 , train_miou=2.466[0m)]100%|##########| 1/1 [00:40<00:00, 40.06s/it, [0;92mdata_loading=39.33, iteration=0.72 , train_acc=39.46, train_loss_seg=1.800, train_macc=6.25 , train_miou=2.466[0m)][2022-10-23 21:34:05,948][torch_points3d.trainer][INFO] - Learning rate = 0.076641
[2022-10-23 21:34:05,949][torch_points3d.trainer][INFO] - EPOCH 24 / 100

  0%|          | 0/1 [00:00<?, ?it/s]slurmstepd: error: *** JOB 10193665 ON r33n6 CANCELLED AT 2022-10-23T21:34:10 ***
