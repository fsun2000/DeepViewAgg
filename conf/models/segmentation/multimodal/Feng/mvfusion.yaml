# @package models
defaults:
  - segmentation/default

MVFusion_3D_small:
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 64  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 2*in_feat ],
                  [ 2*in_feat, in_feat ],               
                  [ in_feat, 2*in_feat ],              
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 2*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 9
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 64
            hidden_dim: 256
            num_heads: 2
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim:
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False          # False by default
            max_n_points: 200000      # max. size of transformer batched input. Increase if memory allows
            gating: False
            
MVFusion_3D_small_6views:
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 64  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 2*in_feat ],
                  [ 2*in_feat, in_feat ],               
                  [ in_feat, 2*in_feat ],              
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 2*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 6
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 64
            hidden_dim: 256
            num_heads: 2
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim:
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False          # False by default
            max_n_points: 200000      # max. size of transformer batched input. Increase if memory allows
            gating: False
            
MVFusion_3D_small_6views_2d_supervision:
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    use_cross_entropy: True
    use_2d_cross_entropy: True
    2d_loss_weight: 0.5
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 64  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 2*in_feat ],
                  [ 2*in_feat, in_feat ],               
                  [ in_feat, 2*in_feat ],              
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 2*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 6
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 64
            hidden_dim: 256
            num_heads: 2
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim:
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False          # False by default
            max_n_points: 200000      # max. size of transformer batched input. Increase if memory allows
            gating: False
            

MVFusion_3D_small_6views_no_attention_masking:
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 64  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 2*in_feat ],
                  [ 2*in_feat, in_feat ],               
                  [ in_feat, 2*in_feat ],              
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 2*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 6
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 64
            hidden_dim: 256
            num_heads: 2
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim:
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: False          # Transformer attention masking
            use_csr_mask: False           # Transformer pixel validity masking
            has_mlp_head: False          # False by default
            max_n_points: 200000      # max. size of transformer batched input. Increase if memory allows
            gating: False
            
            
# MVFusion_3D_small_6views_morepoints:
#     class: Feng.mvfusion_3d.MVFusionAPIModel
#     conv_type: "SPARSE"
#     backend: "torchsparse"
#     backbone: # backbone offset specific for Sparse conv application builder
#         define_constants:
#             in_feat: 32
#             in_feat_img: 4
#             in_feat_map: 8
#             in_feat_attention: 4
#             block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
#             out_mv_feat_dim: 64  # out dim of multi-view fused feature

#         down_conv:
#             module_name: ResNetDown
#             block: block
#             conv3d_after_fusion: False
#             N: [ 0, 2, 3, 4, 6 ]
#             kernel_size: [ 3, 2, 2, 2, 2 ]
#             stride: [ 1, 2, 2, 2, 2 ]
#             down_conv_nn:
#               [
#                   [ FEAT + out_mv_feat_dim, 2*in_feat ],
#                   [ 2*in_feat, in_feat ],               
#                   [ in_feat, 2*in_feat ],              
#                   [ 2*in_feat, 4*in_feat ],
#                   [ 4*in_feat, 8*in_feat ],
#               ]

#             image:
#                 down_conv:
#                     module_name: ADE20KResNet18TruncatedLayer4
#                     frozen: False
#                 atomic_pooling:
#                     module_name: BimodalCSRPool
#                     mode: max
#                 view_pooling:
#                     module_name: GroupBimodalCSRPool
#                     in_map: 8
#                     in_mod: out_mv_feat_dim
#                     num_groups: 4
#                     use_mod: False
#                     map_encoder: DeepSetFeat
#                     use_num: True
#                 fusion:
#                     module_name: BimodalFusion
#                     mode: concatenation
#                 branching_index: 0
#                 checkpointing:  c 
#                 interpolate: True

#         up_conv:
#             block: block
#             module_name: ResNetUp
#             N: [ 1, 1, 1, 1, 1 ]
#             kernel_size: [ 2, 2, 2, 2, 3 ]
#             stride: [ 2, 2, 2, 2, 1 ]
#             up_conv_nn:
#                 [
#                   [ 8*in_feat, 4*in_feat, 4*in_feat ],
#                   [ 4*in_feat, 2*in_feat, 4*in_feat ],
#                   [ 4*in_feat, in_feat, 3*in_feat ],
#                   [ 3*in_feat, 2*in_feat, 3*in_feat ],
#                   [ 3*in_feat, 0, 3*in_feat ],
#                 ]
#         transformer:   # config for multi-view fusion transformer
#             n_views: 6
#             in_map: 9                    # mapping feature size
#             in_m2f: 20                   # M2F one-hot encoded label size
#             embed_dim: 64
#             hidden_dim: 256
#             num_heads: 2
#             num_layers: 4
#             use_batch_norm: False        # layer norm by default
#             feat_downproj_dim:
#             remove_first_dropout: True   # remove first dropout layer before the Transformer block
#             dropout: 0.2
#             mlp_dropout: 0.0             # final layer dropout
#             use_attn_mask: True          # Transformer attention masking
#             use_csr_mask: True           # Transformer pixel validity masking
#             has_mlp_head: False          # False by default
#             max_n_points: 250000      # max. size of transformer batched input. Increase if memory allows
#             gating: False
            
MVFusion_3D_small_gating:
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 64  # out dim of multi-view fused feature
            transformer_num_heads: 2

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 2*in_feat ],
                  [ 2*in_feat, in_feat ],               
                  [ in_feat, 2*in_feat ],              
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: Feng_GroupGating
                    in_mod: out_mv_feat_dim
                    num_groups: transformer_num_heads
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 2*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 9
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 64
            hidden_dim: 256
            num_heads: transformer_num_heads
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim:
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False          # False by default
            max_n_points: 200000      # max. size of transformer batched input. Increase if memory allows
            gating: True
            
MVFusion_3D_medium:
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 128  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 2*in_feat ],
                  [ 2*in_feat, in_feat ],               
                  [ in_feat, 2*in_feat ],              
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 2*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 9
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 128
            hidden_dim: 256
            num_heads: 4
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim:
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False          # False by default
            max_n_points: 200000      # max. size of transformer batched input. Increase if memory allows
            gating: False
            

MVFusion_3D_medium_gating:
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 128  # out dim of multi-view fused feature
            transformer_num_heads: 4

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 2*in_feat ],
                  [ 2*in_feat, in_feat ],               
                  [ in_feat, 2*in_feat ],              
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: Feng_GroupGating
                    in_mod: out_mv_feat_dim
                    num_groups: transformer_num_heads
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 2*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 9
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 128
            hidden_dim: 256
            num_heads: transformer_num_heads
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim:
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False          # False by default
            max_n_points: 200000      # max. size of transformer batched input. Increase if memory allows
            gating: True

MVFusion_3D_medium_12views:
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 128  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 2*in_feat ],
                  [ 2*in_feat, in_feat ],               
                  [ in_feat, 2*in_feat ],              
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 2*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 12
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: out_mv_feat_dim
            hidden_dim: 256
            num_heads: 4
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim:
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False          # False by default
            max_n_points: 200000      # max. size of transformer batched input. Increase if memory allows
            gating: False

MVFusion_3D_large:
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 256  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 4*in_feat ], 
                  [ 4*in_feat, in_feat ],                 
                  [ in_feat, 2*in_feat ],                 
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 4*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 9
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 256
            hidden_dim: 512
            num_heads: 4
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim: # 16
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False
            max_n_points: 100000         # max. size of transformer batched input. Increase if memory allows
            gating: False

MVFusion_3D_large_6views:
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    use_cross_entropy: True
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 256  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 4*in_feat ],  # Perhaps lower hid_dim of 
                  [ 4*in_feat, in_feat ],                 # first two layers according to
                  [ in_feat, 2*in_feat ],                 # transformer output dim?
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 4*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 6
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 256
            hidden_dim: 512
            num_heads: 4
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim: # 16
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False
            max_n_points: 200000         # max. size of transformer batched input. Increase if memory allows
            gating: False


MVFusion_3D_large_6views_2d_supervision:
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    use_cross_entropy: True
    use_2d_cross_entropy: True
    2d_loss_weight: 0.5
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 256  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 4*in_feat ],  # Perhaps lower hid_dim of 
                  [ 4*in_feat, in_feat ],                 # first two layers according to
                  [ in_feat, 2*in_feat ],                 # transformer output dim?
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 4*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 6
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 256
            hidden_dim: 512
            num_heads: 4
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim: # 16
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False
            max_n_points: 200000         # max. size of transformer batched input. Increase if memory allows
            gating: False


MVFusion_3D: #Res16UNet34-L4-early-ade20k-interpolate:
    # 28.1 M params - 109.4 Mo on the GPU - ~2.7 ko/pixel at training time
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 256  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 4*in_feat ],  # Perhaps lower hid_dim of 
                  [ 4*in_feat, in_feat ],                 # first two layers according to
                  [ in_feat, 2*in_feat ],                 # transformer output dim?
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 4*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 9
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 256
            hidden_dim: 512
            num_heads: 4
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim: # 16
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False
            max_n_points: 999999999      # max. size of transformer batched input. Increase if memory allows #130000
            gating: False

MVFusion_3D_M1: #Res16UNet34-L4-early-ade20k-interpolate:
    # 28.1 M params - 109.4 Mo on the GPU - ~2.7 ko/pixel at training time
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 256  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 4*in_feat ],  # Perhaps lower hid_dim of 
                  [ 4*in_feat, in_feat ],                 # first two layers according to
                  [ in_feat, 2*in_feat ],                 # transformer output dim?
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 4*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 9
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 256
            hidden_dim: 512
            num_heads: 4
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim: # 16
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False
            max_n_points: 99999999      # max. size of transformer batched input. Increase if memory allows
            
MVFusion_3D_M2: #Res16UNet34-L4-early-ade20k-interpolate:
    # 28.1 M params - 109.4 Mo on the GPU - ~2.7 ko/pixel at training time
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 128  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 4*in_feat ],  # Perhaps lower hid_dim of 
                  [ 4*in_feat, in_feat ],                 # first two layers according to
                  [ in_feat, 2*in_feat ],                 # transformer output dim?
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 4*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 9
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 128
            hidden_dim: 512
            num_heads: 4
            num_layers: 8
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim: # 16
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False
            max_n_points: 130000      # max. size of transformer batched input. Increase if memory allows


MVFusion_3D_M3: #Res16UNet34-L4-early-ade20k-interpolate:
    # 28.1 M params - 109.4 Mo on the GPU - ~2.7 ko/pixel at training time
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 128  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 4*in_feat ],  # Perhaps lower hid_dim of 
                  [ 4*in_feat, in_feat ],                 # first two layers according to
                  [ in_feat, 2*in_feat ],                 # transformer output dim?
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 4*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 9
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 128
            hidden_dim: 256
            num_heads: 4
            num_layers: 8
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim: # 16
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False
            max_n_points: 130000      # max. size of transformer batched input. Increase if memory allows
            
MVFusion_3D_M4: #Res16UNet34-L4-early-ade20k-interpolate:
    # 28.1 M params - 109.4 Mo on the GPU - ~2.7 ko/pixel at training time
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 128  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 4*in_feat ],  # Perhaps lower hid_dim of 
                  [ 4*in_feat, in_feat ],                 # first two layers according to
                  [ in_feat, 2*in_feat ],                 # transformer output dim?
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 4*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 9
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 128
            hidden_dim: 256
            num_heads: 4
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim:
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False
            max_n_points: 130000      # max. size of transformer batched input. Increase if memory allows
            
MVFusion_3D_M5: #Res16UNet34-L4-early-ade20k-interpolate:
    # 28.1 M params - 109.4 Mo on the GPU - ~2.7 ko/pixel at training time
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 64  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 4*in_feat ],  # Perhaps lower hid_dim of 
                  [ 4*in_feat, in_feat ],                 # first two layers according to
                  [ in_feat, 2*in_feat ],                 # transformer output dim?
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 4*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 9
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 128
            hidden_dim: 256
            num_heads: 4
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim: 64
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False
            max_n_points: 130000      # max. size of transformer batched input. Increase if memory allows
            
MVFusion_3D_M6: #Res16UNet34-L4-early-ade20k-interpolate:
    # 28.1 M params - 109.4 Mo on the GPU - ~2.7 ko/pixel at training time
    class: Feng.mvfusion_3d.MVFusionAPIModel
    conv_type: "SPARSE"
    backend: "torchsparse"
    backbone: # backbone offset specific for Sparse conv application builder
        define_constants:
            in_feat: 32
            in_feat_img: 4
            in_feat_map: 8
            in_feat_attention: 4
            block: ResBlock # Can be any of the blocks in modules/MinkowskiEngine/api_modules.py
            out_mv_feat_dim: 64  # out dim of multi-view fused feature

        down_conv:
            module_name: ResNetDown
            block: block
            conv3d_after_fusion: False
            N: [ 0, 2, 3, 4, 6 ]
            kernel_size: [ 3, 2, 2, 2, 2 ]
            stride: [ 1, 2, 2, 2, 2 ]
            down_conv_nn:
              [
                  [ FEAT + out_mv_feat_dim, 4*in_feat ],  # Perhaps lower hid_dim of 
                  [ 4*in_feat, in_feat ],                 # first two layers according to
                  [ in_feat, 2*in_feat ],                 # transformer output dim?
                  [ 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, 8*in_feat ],
              ]

            image:
                down_conv:
                    module_name: ADE20KResNet18TruncatedLayer4
                    frozen: False
                atomic_pooling:
                    module_name: BimodalCSRPool
                    mode: max
                view_pooling:
                    module_name: GroupBimodalCSRPool
                    in_map: 8
                    in_mod: out_mv_feat_dim
                    num_groups: 4
                    use_mod: False
                    map_encoder: DeepSetFeat
                    use_num: True
                fusion:
                    module_name: BimodalFusion
                    mode: concatenation
                branching_index: 0
                checkpointing:  c 
                interpolate: True

        up_conv:
            block: block
            module_name: ResNetUp
            N: [ 1, 1, 1, 1, 1 ]
            kernel_size: [ 2, 2, 2, 2, 3 ]
            stride: [ 2, 2, 2, 2, 1 ]
            up_conv_nn:
                [
                  [ 8*in_feat, 4*in_feat, 4*in_feat ],
                  [ 4*in_feat, 2*in_feat, 4*in_feat ],
                  [ 4*in_feat, in_feat, 3*in_feat ],
                  [ 3*in_feat, 4*in_feat, 3*in_feat ],
                  [ 3*in_feat, 0, 3*in_feat ],
                ]
        transformer:   # config for multi-view fusion transformer
            n_views: 9
            in_map: 9                    # mapping feature size
            in_m2f: 20                   # M2F one-hot encoded label size
            embed_dim: 64
            hidden_dim: 256
            num_heads: 2
            num_layers: 4
            use_batch_norm: False        # layer norm by default
            feat_downproj_dim: 64
            remove_first_dropout: True   # remove first dropout layer before the Transformer block
            dropout: 0.2
            mlp_dropout: 0.0             # final layer dropout
            use_attn_mask: True          # Transformer attention masking
            use_csr_mask: True           # Transformer pixel validity masking
            has_mlp_head: False
            max_n_points: 130000      # max. size of transformer batched input. Increase if memory allows